[{"categories":["笔记","算法"],"content":"绪论 DFS 和 BFS 都是常用的图搜索算法，可以解决各种问题，比如：图遍历、路径搜索等 下面提到的int[] 可代表一个节点坐标 int[]{1,2} ","date":"2023-09-16","objectID":"/dfs_bfs/:0:1","tags":["算法","数据结构与算法","algorithm"],"title":"DFS_BFS","uri":"/dfs_bfs/"},{"categories":["笔记","算法"],"content":"DFS深度优先算法 工作原理：从起点开始，沿着一条分支（路径）尽可能的深入探索，直到终点或者无法继续为止，然后回溯探索其他分支（路径）。 使用场景：适合找所有的可能的路径问题（不一定是最短）；如：迷宫、拓扑排序；树的前序遍历 数据结构：递归或栈来实现 实现算法注意事项： 非递归使用栈时，先不用弹出节点，如果某个分支（路径）方向可以继续探索，将探索到的节点压入栈；注意，如果时通过for或while循环进入某个分支方向探索时，只需要能访问到下一个节点，就需要break；同时标记已经压入了新节点addNew= true；，若任何路径不可探索（自身也会被标记为已经访问）即addNew= false，则需要将该节点从栈中弹出从而进行回溯。 递归基是当前节点是目标节点，然后弹栈（终点到起点）或者全部搜索完成 递归实现时：1 进入了某个分支（方向）递归调用DFS的时候，需要return 2 使用额外的空间保存访问路径 （如：List\u003cint[]\u003e paths ; 或者List paths等） ","date":"2023-09-16","objectID":"/dfs_bfs/:0:2","tags":["算法","数据结构与算法","algorithm"],"title":"DFS_BFS","uri":"/dfs_bfs/"},{"categories":["笔记","算法"],"content":"BFS广度搜索算法 工作原理：从起点开始，探索所有直接相邻的节点，然后向外扩展。直到找到目标节点或全部搜索完。 使用场景：适合最短路径问题；树的层序遍历 数据结构：队列 实现算法注意事项： 使用队列 记录访问标识，使用辅助数据结构（如 Map\u003cint[],int[]\u003e ; k是当前找到的邻居节点，v是前驱节点即当前节点）跟踪前驱节点； 搜索到目标时，从队列取出的节点就是终点，根据终点int[] 从Map\u003cint[],int[]\u003e取出整个链路 ","date":"2023-09-16","objectID":"/dfs_bfs/:0:3","tags":["算法","数据结构与算法","algorithm"],"title":"DFS_BFS","uri":"/dfs_bfs/"},{"categories":["笔记","算法"],"content":"迷宫问题的DFS和BFS的代码实现如下： 二维数组值为1 表示是墙不可达；起点是 (0,0) ; 终点是起点对角线的终点;如 (n-1,m-1) ; 当然一下代码修改可指定起点和终点坐标位置 package com.example.demo; import java.util.ArrayList; import java.util.Collections; import java.util.HashMap; import java.util.LinkedList; import java.util.List; import java.util.Map; import java.util.Queue; import java.util.Scanner; import java.util.Stack; /** *nowcoder ： HJ43 宫问题 * https://www.nowcoder.com/profile/100401824/codeBookDetail?submissionId=426841626 * 迷宫问题： * 5 5 * 0 1 0 0 0 * 0 1 1 1 0 * 0 0 0 0 0 * 0 1 1 1 0 * 0 0 0 1 0 * * * 5 5 * 0 1 0 0 0 * 0 1 0 1 0 * 0 0 0 0 1 * 0 1 1 1 0 * 0 0 0 0 0 * */ // 注意类名必须为 Main, 不要有任何 package xxx 信息 public class Maze { static int[] up = {-1, 0}; static int[] down = {1, 0}; static int[] left = {0, -1}; static int[] right = {0, 1}; static int[][] directions = new int[][]{ right, down, left, up, }; // {0,0},{1,0},{2,0},{2,1},{2,2},{2,3} ,{2,4},{3,4},{4,4} static List\u003cint[]\u003e pathList = new ArrayList\u003c\u003e(); public static void main(String[] args) { Scanner in = new Scanner(System.in); // 注意 hasNext 和 hasNextLine 的区别 while (in.hasNextLine()) { // 注意 while 处理多个 case String nm = in.nextLine(); int n = Integer.parseInt(nm.split(\"\\\\s+\")[0]); int m = Integer.parseInt(nm.split(\"\\\\s+\")[1]); int[][] matix = new int[n][m]; // boolean[][] visited = new boolean[n][m]; for (int i = 0; i \u003c n; i++) { String line = in.nextLine(); String[] lines = line.split(\"\\\\s+\"); for (int j = 0; j \u003c lines.length; j++) { matix[i][j] = Integer.parseInt(lines[j]); } } System.out.println(\"DFS =====================================\"); pathList.clear(); dfs(matix, 0, 0); pathList.clear(); System.out.println(\"BFS=====================================\"); List\u003cint[]\u003e paths = bfs(matix); if (paths != null \u0026\u0026 paths.size() \u003e 0) { for (int[] p : paths) { System.out.println(\"(\" + p[0] + \",\" + p[1] + \") \"); } } System.out.println(\"dfsNotRecursion =====================================\"); List\u003cint[]\u003e dfsNotRecursion = dfsNotRecursion(matix); if (dfsNotRecursion != null \u0026\u0026 dfsNotRecursion.size() \u003e 0) { for (int[] p : dfsNotRecursion) { System.out.println(\"(\" + p[0] + \",\" + p[1] + \") \"); } } System.out.println(\" =====================================\"); } } public static void dfs(int[][] matix, int x, int y) { //行数 int rows = matix.length; // 列数 int cols = matix[0].length; // 访问标识 int[][] visited = new int[rows][cols]; visited[x][y] = 2 ; pathList.add(new int[]{x, y}); // 找到解了 if ((x == rows - 1 \u0026\u0026 y == cols - 1)) { for (int[] arr : pathList) { System.out.println(\"(\" + arr[0] + \",\" + arr[1] + \") \"); } return; } // 4个方向 for (int i = 0; i \u003c directions.length; i++) { int[] d = directions[i]; if (checkEdge(x + d[0], y + d[1], rows, cols) \u0026\u0026 matix[x + d[0]][y + d[1]] == 0 \u0026\u0026 visited[x + d[0]][y + d[1]] == 0 ) { dfs(matix, x + d[0], y + d[1]); // 重要 一定要有返回 return; } } // 回溯 pathList.remove(pathList.size() - 1); } //深度搜索需要维护前驱节点和访问标识 public static List\u003cint[]\u003e bfs(int[][] matix) { //队列，同时弹出节点的往多个方向搜索 Queue\u003cint[]\u003e queue = new LinkedList\u003c\u003e(); //跟踪记录前驱节点，用于回溯路径 Map\u003cint[], int[]\u003e traceMap = new HashMap\u003c\u003e(); // 记录访问的标识（后期优化可用于记录对应位置在路径上距离起点的距离） int rows = matix.length; int clos = matix[0].length; int[][] visited = new int[rows][clos]; // 0,0 节点 入栈，标记为访问，并跟踪 int[] startNode = new int[]{0, 0, 1}; queue.offer(startNode); traceMap.put(startNode, null); visited[startNode[0]][startNode[1]] = startNode[2]; while (!queue.isEmpty()) { int[] current = queue.poll(); // 已经到终点了 if (current[0] == rows - 1 \u0026\u0026 current[1] == clos - 1) { List\u003cint[]\u003e paths = new ArrayList\u003c\u003e(); while (current != null) { paths.add(current); current = traceMap.get(current); } // 修正为从起点开始输出 Collections.reverse(paths); // for(int[] v : visited){ // for(int p : v){ // System.out.print(p+ \" \"); // } // System.out.println(\"\"); // } return paths; } // 将当前节点的所有邻居节点压栈（本题不含斜线方向） for (int[] d : directions) { int[] nexts = new int[]{current[0] + d[0], current[1] + d[1], current[2] + 1}; if ( // 迷宫范围 checkEdge(nexts[0], nexts[1], rows, clos) // 未被访问过，防止死循环 \u0026\u0026 visited[nexts[0]][nexts[1]] == 0 // 不是墙 \u0026\u0026 matix","date":"2023-09-16","objectID":"/dfs_bfs/:0:4","tags":["算法","数据结构与算法","algorithm"],"title":"DFS_BFS","uri":"/dfs_bfs/"},{"categories":["interview"],"content":"1、可重入锁 ","date":"2023-09-11","objectID":"/yifeng_20230911/:1:0","tags":["interview"],"title":"Yifeng_20230911","uri":"/yifeng_20230911/"},{"categories":["interview"],"content":"1.1 理解 可重入锁，也叫做 递归锁，从名字上理解就是可以再进入的锁，重入性是指任意线程在获取到锁之后能够再次获取该锁而不会被锁阻塞 ","date":"2023-09-11","objectID":"/yifeng_20230911/:1:1","tags":["interview"],"title":"Yifeng_20230911","uri":"/yifeng_20230911/"},{"categories":["interview"],"content":"两个条件： 1 线程再次获取锁 ，2 可释放，线程重复n次获取了锁，随后在第n次释放该锁后，其它线程能够获取到该锁 ","date":"2023-09-11","objectID":"/yifeng_20230911/:1:2","tags":["interview"],"title":"Yifeng_20230911","uri":"/yifeng_20230911/"},{"categories":["interview"],"content":"JAVA中的实现 synchronized 关键字所使用的锁也是可重入的 ReentrantLock继承自 Lock接口 通过Sync 类，即自定义的同步组件实现，它是 ReentrantLock 里面的一个内部类，它继承自AQS(AbstractQueuedSynchronizer)，Sync 有两个子类：公平锁 FairSync 和 非公平锁 NonfairSync NonfairSync 的 lock() 方法： 1 CAS自旋操作state(0,1) 2 失败则 aqs.acquire(1),在acquire(1)中是一套锁抢占的模板,会先调tryAcquire,tryAcquire() 这个钩子方法去尝试获取锁，这个方法就是在 NonfairSync.tryAcquire()下的 nonfairTryAcquire().(先尝试CAS，state！=0就接着判断是否同一个线程所持有,如果是设置state+1) 3 如果nonfairTryAcquire的以上两种情况都不通过，则返回失败false，就会则进入 acquireQueued() 流程，也就是基于CLH队列的抢占模式； 进入的时候也会去执行一次获取锁的操作，如果还是获取不到，就调用LockSupport.park() 将当前线程挂起。那么当前线程什么时候会被唤醒呢？当持有锁的那个线程调用 unlock() 的时候，会将CLH队列的头节点的下一个节点上的线程唤醒，调用的是 LockSupport.unpark() 方法。 ","date":"2023-09-11","objectID":"/yifeng_20230911/:1:3","tags":["interview"],"title":"Yifeng_20230911","uri":"/yifeng_20230911/"},{"categories":["interview"],"content":"引申 AQS原理 AQS（AbstractQueuedSynchronizer）是Java中用于构建锁和同步器的底层框架。它提供了一个灵活的方式来实现各种同步机制，如ReentrantLock、Semaphore、CountDownLatch等，并且也可用于构建自定义的同步器。AQS的核心思想是基于队列的等待，通过管理一个等待队列来实现线程的排队和唤醒。 AQS底层使用了模板方法模式，自定义同步器时需要重写下面几个AQS提供的模板方法： isHeldExclusively()//该线程是否正在独占资源。只有用到condition才需要去实现它。 tryAcquire(int)//独占方式。尝试获取资源，成功则返回true，失败则返回false。 tryRelease(int)//独占方式。尝试释放资源，成功则返回true，失败则返回false。 tryAcquireShared(int)//共享方式。尝试获取资源。负数表示失败；0表示成功，但没有剩余可用资源；正数表示成功，且有剩余资源。 tryReleaseShared(int)//共享方式。尝试释放资源，成功则返回true，失败则返回false。 参考文章[JUC锁: 锁核心类AQS详解}(https://pdai.tech/md/java/thread/java-thread-x-lock-AbstractQueuedSynchronizer.html) ","date":"2023-09-11","objectID":"/yifeng_20230911/:1:4","tags":["interview"],"title":"Yifeng_20230911","uri":"/yifeng_20230911/"},{"categories":["interview"],"content":"2、synchronized锁升级过程 Java 详细对象结构: 对象头（Mark Word、Class Pointer、数组长度三个字段组成），对象体，对齐字节 ; Mark Word主要用来表示当前 Java 对象的线程锁状态以及 GC 的标志，可以表示 4 种不同的锁状态 无锁状态：初始状态，没有线程占用锁，线程可以无竞争地进入临界区。 偏向锁状态：当只有一个线程访问临界区时，该线程会偏向于锁，以提高性能。如果其他线程尝试进入临界区，偏向锁会升级为轻量级锁。 轻量级锁状态：多个线程竞争同一把锁，但还没有争用到达一定程度。这时，锁会升级为轻量级锁，使用CAS操作来尝试获取锁，避免了传统的重量级锁的开销。 重量级锁状态：多个线程争用锁的情况下，试图抢占的线程自旋达到阈值，就会停止自旋，那么此时锁就会膨胀成重量级锁，锁会升级为重量级锁，通过操作系统的互斥原语来实现线程的阻塞和唤醒。 锁的升级过程通常是自动的，根据竞争情况和线程行为，锁会在不同状态之间切换以优化性能。这个过程在JVM内部进行管理，开发者一般不需要显式干预。详细了解锁升级过程可以帮助优化多线程程序的性能。 ","date":"2023-09-11","objectID":"/yifeng_20230911/:2:0","tags":["interview"],"title":"Yifeng_20230911","uri":"/yifeng_20230911/"},{"categories":["interview"],"content":"3、countDownLauch工作原理 其底层是由AQS提供支持，所以其数据结构可以参考AQS的数据结构，而AQS的数据结构核心就是两个虚拟队列: 同步队列sync queue 和条件队列condition queue，不同的条件会有不同的条件队列。CountDownLatch典型的用法是将一个程序分为n个互相独立的可解决任务，并创建值为n的CountDownLatch。 核心函数 - await函数；此函数将会使当前线程在锁存器倒计数至零之前一直等待，除非线程被中断。对CountDownLatch对象的await的调用会转发为对Sync的acquireSharedInterruptibly(从AQS继承的方法)方法的调用。 ","date":"2023-09-11","objectID":"/yifeng_20230911/:3:0","tags":["interview"],"title":"Yifeng_20230911","uri":"/yifeng_20230911/"},{"categories":["interview"],"content":"4、说下Spring IOC，以及中间用了什么设计模式 使用对象时候由主动new对象转换成由外部提供对象,此过程中对象的创建权由程序转移到外部，这种思想叫做控制反转 Spring技术对此提供的实现 Spring提供了一个容器，称为IOC容器，用来充当IOC思想中的外部 IOC容器负责对象的创建、初始化等一系列工作，被创建或被管理的对象在IOC容器中统称为Bean。 ","date":"2023-09-11","objectID":"/yifeng_20230911/:4:0","tags":["interview"],"title":"Yifeng_20230911","uri":"/yifeng_20230911/"},{"categories":["interview"],"content":"1、工厂模式 Spring中在各种BeanFactory以及ApplicationContext创建中都用到了典型的工厂方法模式 ###　2、单例模式 在Spring中，所有的bean默认都是单例创建的。在创建bean的代码中我们经常看到Singleton这个单词。下面我们通过代码看看单例是怎么实现的。 AbstractBeanFactory.doGetBean() ","date":"2023-09-11","objectID":"/yifeng_20230911/:4:1","tags":["interview"],"title":"Yifeng_20230911","uri":"/yifeng_20230911/"},{"categories":["interview"],"content":"3、策略模式 在依赖注入的过程中，Spring会调用ApplicationContext 来获取Resource的实例。然而，Resource 接口封装了各种可能的资源类型，包括了：UrlResource，ClassPathResource，FileSystemResource等，Spring需要针对不同的资源采取不同的访问策略。在这里，Spring让ApplicationContext成为了资源访问策略的“决策者”。在资源访问策略的选择上，Spring采用了策略模式。当 Spring 应用需要进行资源访问时，它并不需要直接使用 Resource 实现类，而是调用 ApplicationContext 实例的 getResource() 方法来获得资源，ApplicationContext 将会负责选择 Resource 的实现类，也就是确定具体的资源访问策略，从而将应用程序和具体的资源访问策略分离开来。 ","date":"2023-09-11","objectID":"/yifeng_20230911/:4:2","tags":["interview"],"title":"Yifeng_20230911","uri":"/yifeng_20230911/"},{"categories":["interview"],"content":"4、装饰器模式 Spring中类中带有Wrapper的都是包装类 ","date":"2023-09-11","objectID":"/yifeng_20230911/:4:3","tags":["interview"],"title":"Yifeng_20230911","uri":"/yifeng_20230911/"},{"categories":["interview"],"content":"5 代理模式 AOP等等 ","date":"2023-09-11","objectID":"/yifeng_20230911/:4:4","tags":["interview"],"title":"Yifeng_20230911","uri":"/yifeng_20230911/"},{"categories":["interview"],"content":"6 责任链模式 Filter等 ","date":"2023-09-11","objectID":"/yifeng_20230911/:4:5","tags":["interview"],"title":"Yifeng_20230911","uri":"/yifeng_20230911/"},{"categories":["interview"],"content":"5、说下SpringBoot和Spring的区别? Spring和Spring Boot基于IOC AOP理念实现，Spring Boot集成了Spring。 对于我来说，Spring框架就是提供了IOC容器、控制反转、依赖注入以及一些模块，简化了大量的代码，便捷了程序的开发，节省了开发时间，提高了效率。 在我看来Spring Boot框架是对Spring框架的补充，它消除了Spring框架配置XML的麻烦事，完善了Spring框架的开发环境，使我们可以更加高效的完成编程，并且为我们提供了 spring-boot-starter-xxx 依赖，写大量的配置，现在用Spring Boot只需要导入相关依赖。 Spring 和 Spring Boot的最大的区别在于Spring Boot的自动装配原理 在我看来是SpringBoot（spring）将java推上了神座。 ","date":"2023-09-11","objectID":"/yifeng_20230911/:5:0","tags":["interview"],"title":"Yifeng_20230911","uri":"/yifeng_20230911/"},{"categories":["interview"],"content":"6、线程池核心参数、以及说下工作原理?工作中使用什么工作队列?以及用了什么拒绝策略? ","date":"2023-09-11","objectID":"/yifeng_20230911/:6:0","tags":["interview"],"title":"Yifeng_20230911","uri":"/yifeng_20230911/"},{"categories":["interview"],"content":"7、说说JVM垃圾搜集器? docker容器4G，你给JVM最大堆内存分配多少? ","date":"2023-09-11","objectID":"/yifeng_20230911/:7:0","tags":["interview"],"title":"Yifeng_20230911","uri":"/yifeng_20230911/"},{"categories":["interview"],"content":"8、说下哪些场景需要打破双亲委派机制? 当某个类加载器需要加载某个.class文件时，它首先把这个任务委托给他的上级类加载器，递归这个操作，如果上级的类加载器没有加载，自己才会去加载这个类。 BootstrapClassLoader 《== ExtClassLoader 《== AppClassLoader 《== CustomClassLoader（用户自定义） 1 防止加载同一个.class。通过委托去询问上级是否已经加载过该.class，如果加载过了，则不需要重新加载。保证了数据安全。 2 保证核心.class不被篡改。通过委托的方式，保证核心.class不被篡改，即使被篡改也不会被加载，即使被加载也不会是同一个class对象，因为不同的加载器加载同一个.class也不是同一个Class对象。这样则保证了Class的执行安全。 自定义类加载器： 如果不想打破双亲委派模型，就重写ClassLoader类中的findClass()方法即可，无法被父类加载器加载的类最终会通过这个方法被加载 而如果想打破双亲委派模型则需要重写ClassLoader类loadClass()方法（当然其中的坑也不会少）。典型的打破双亲委派模型的框架和中间件有tomcat与osgi ","date":"2023-09-11","objectID":"/yifeng_20230911/:8:0","tags":["interview"],"title":"Yifeng_20230911","uri":"/yifeng_20230911/"},{"categories":["interview"],"content":"9、CPU飙高、JVM内存泄漏如何解决? 可以使用jps查看java程序的资源占用，下面介绍使用top命令的 top 命令查找CPU和内存信息，根据排行可以取得对应进程id （ 如：top -p 452 查找指定进程的cpu内存信息） top -H -p 452 // 查看指定进程452的所有线程的CPU内存信息 记录CPU或内存占用高的线程ID，转化为对应的16进制，堆栈中搜索用( print命令（printf “%x \\n” 15）： 或其他工具） jstack -l 108032 | grep 1a601 -A 74 ； 打印线程108033（父进程是108032）的快照。 108033转成11a601 linux1@linuxonetest:~$ jstack -l 108032 | grep 1a601 -A 74 \"main\" #1 prio=5 os_prio=0 cpu=1447919.07ms elapsed=1448.76s tid=0x000003ff7c016c30 nid=0x1a601 runnable [0x000003ff823fd000] java.lang.Thread.State: RUNNABLE at main.main(main.java:11) at jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(java.base@17.0.8.1/Native Method) at jdk.internal.reflect.NativeMethodAccessorImpl.invoke(java.base@17.0.8.1/NativeMethodAccessorImpl.java:77) at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(java.base@17.0.8.1/DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(java.base@17.0.8.1/Method.java:568) at com.sun.tools.javac.launcher.Main.execute(jdk.compiler@17.0.8.1/Main.java:419) at com.sun.tools.javac.launcher.Main.run(jdk.compiler@17.0.8.1/Main.java:192) at com.sun.tools.javac.launcher.Main.main(jdk.compiler@17.0.8.1/Main.java:132) Locked ownable synchronizers: - None 可以看到是代码 11行： at main.main(main.java:11) ; 查看代码可以知道代码里面while循序一直在处理任务，没有让出CPU，导致CPU飙升 1 2 public class main{ 3 4 5 public static void main(String[] args){ 6 while(true){ 7 // System.out.println(\"Hello wordl\"); 8 // 9 int a = 10000 + 50000000; 10 a = a / 1000 ; 11 } 12 13 } 14 } 内存泄漏如何排查 # 外部触发 调用 循环次数最好通过外部控制 while(true) { System.out.println(\"一刻不停的处理任务\"); list.add(new String(new byte[1024 * 1024]) + \"处理任务分配一个1M的对象，序号为 \" + i); // sleep 100 ms } 首先通过jps查看进程PID是2785，然后通过top -p 2785发现内存升高到28.1%。 多次使用jstat -gc 2785查看GC日志;发现OU的内存，也就是老年代的使用内存在一直增加，有对象一直处于存活，并且一直有新对象产生。接下来就通过堆栈储文件查看内存的使用情况。 通过jmap -dump:format=b,file=height-cpu.bin 2785生产堆转储快照文件。使用Eclipse的内存分析器工具（MAT）打开height-cpu.bin文件进行堆内存分析 可以发现一个对象占了96%以上的内存。打开leak suspects页面，可以查看内存泄漏的原因 ; 点击详情可以查看具体的对象 … ","date":"2023-09-11","objectID":"/yifeng_20230911/:9:0","tags":["interview"],"title":"Yifeng_20230911","uri":"/yifeng_20230911/"},{"categories":["interview"],"content":"GC 统计 jstat -gc 452 1000 10 #每秒查询一次查询10次 (452进程id 1000ms) # 备注 ： s --\u003e Surive ; E --\u003e Eden , O --\u003e Old M --\u003e Metaspace 不是method C --\u003e Capacity(容量) U--\u003e Used Y--\u003e Young FGC--\u003e Full GC 第一行表示在应用程序启动后第一次采样时，各个内存区域和垃圾回收的情况。 例如，你可以看到： S0C是0.0，表示survivor space 0没有分配任何空间； S1C是4096.0，表示survivor space 1分配了4096 KB的空间； S0U是0.0，表示survivor space 0没有使用任何空间； S1U是4096.0，表示survivor space 1已经使用了全部空间； EC是309248.0，表示eden space分配了309248 KB的空间； EU是236544.0，表示eden space已经使用了236544 KB的空间； OC是183296.0，表示old space分配了183296 KB的空间； OU是125409.0，表示old space已经使用了125409 KB的空间； MC是140168.0，表示metaspace分配了140168 KB的空间； MU是135553.7，表示metaspace已经使用了135553.7 KB的空间； CCSC是15488.0，表示compressed class space分配了15488 KB的空间； CCSU是13814.7，表示compressed class space已经使用了13814.7 KB的空间； YGC是236，表示从应用程序启动到采样时发生了236次young generation垃圾回收； YGCT是3.545，表示从应用程序启动到采样时young generation垃圾回收花费了3.545秒； FGC是0，表示从应用程序启动到采样时没有发生full GC； FGCT是0.000，表示从应用程序启动到采样时full GC花费了0秒； CGC是12，表示从应用程序启动到采样时发生了12次concurrent GC； CGCT是0.188，表示从应用程序启动到采样时concurrent GC花费了0.188秒； GCT是3.733，表示从应用程序启动到采样时垃圾回收花费了总共3.733秒。 * The jstat Command - Oracle. https://docs.oracle.com/en/java/javase/14/docs/specs/man/jstat.html ","date":"2023-09-11","objectID":"/yifeng_20230911/:9:1","tags":["interview"],"title":"Yifeng_20230911","uri":"/yifeng_20230911/"},{"categories":["interview"],"content":"堆内存统计 jstat -gccapacity 452 NGCMN：新生代最小容量 NGCMX：新生代最大容量 NGC：当前新生代容量 S0C：第一个幸存区大小 S1C：第二个幸存区的大小 EC：伊甸园区的大小 OGCMN：老年代最小容量 OGCMX：老年代最大容量 OGC：当前老年代大小 OC:当前老年代大小 MCMN:最小元数据容量 MCMX：最大元数据容量 MC：当前元数据空间大小 CCSMN：最小压缩类空间大小 CCSMX：最大压缩类空间大小 CCSC：当前压缩类空间大小 YGC：年轻代gc次数 FGC：老年代GC次数 CGC: ","date":"2023-09-11","objectID":"/yifeng_20230911/:9:2","tags":["interview"],"title":"Yifeng_20230911","uri":"/yifeng_20230911/"},{"categories":["interview"],"content":"10、redis数据结构? string(字符串)、list(列表)、hash(字典)、set(集合) 、 zset(有序集合)和 Stream(流) zset(有序集合)的内部实现用的是一种叫做 「跳跃表」 的数据结构 typedef struct dictht { // 哈希表数组 dictEntry **table; // 哈希表大小 unsigned long size; // 哈希表大小掩码，用于计算索引值，总是等于 size - 1 unsigned long sizemask; // 该哈希表已有节点的数量 unsigned long used; } dictht; typedef struct dict { dictType *type; void *privdata; // 内部有两个 dictht 结构 dictht ht[2]; long rehashidx; /* rehashing not in progress if rehashidx == -1 */ unsigned long iterators; /* number of iterators currently running */ } dict; 可以从上面的源码中看到，实际上字典结构的内部包含两个 hashtable，通常情况下只有一个 hashtable 是有值的，但是在字典扩容缩容时，需要分配新的 hashtable，然后进行 渐进式搬迁; 大字典的扩容是比较耗时间的，需要重新申请新的数组，然后将旧字典所有链表中的元素重新挂接到新的数组下面，这是一个 O(n) 级别的操作，作为单线程的 Redis 很难承受这样耗时的过程，所以 Redis 使用 渐进式 rehash 小步搬迁 Stream 类型 Redis5.0带来了Stream类型。从字面上看是流类型，但其实从功能上看，应该是Redis对消息队列（MQ，Message Queue）的完善实现。用过Redis做消息队列的都了解，基于Reids的消息队列实现有很多种，例如： PUB/SUB，订阅/发布模式 基于List的 LPUSH+BRPOP 的实现 基于Sorted-Set的实现 Redis Stream的结构如上图所示，它有一个消息链表，将所有加入的消息都串起来，每个消息都有一个唯一的ID和对应的内容。消息是持久化的，Redis重启后，内容还在 用来实现典型的消息队列。该Stream类型的出现，几乎满足了消息队列具备的全部内容 参考 ： https://cloud.tencent.com/developer/article/1667574 ","date":"2023-09-11","objectID":"/yifeng_20230911/:10:0","tags":["interview"],"title":"Yifeng_20230911","uri":"/yifeng_20230911/"},{"categories":["interview"],"content":"11、说下mysqlinnodb索引数据结构? ","date":"2023-09-11","objectID":"/yifeng_20230911/:11:0","tags":["interview"],"title":"Yifeng_20230911","uri":"/yifeng_20230911/"},{"categories":["interview"],"content":"12、说下模板模式在Spring IoC的应用 ","date":"2023-09-11","objectID":"/yifeng_20230911/:12:0","tags":["interview"],"title":"Yifeng_20230911","uri":"/yifeng_20230911/"},{"categories":["interview"],"content":"13、mysql事务有哪些?默认事务是什么? ","date":"2023-09-11","objectID":"/yifeng_20230911/:13:0","tags":["interview"],"title":"Yifeng_20230911","uri":"/yifeng_20230911/"},{"categories":["interview"],"content":"14、说下kafka生产者、消费者整套流程?生产者批量提交配置，项目是如何配 ","date":"2023-09-11","objectID":"/yifeng_20230911/:14:0","tags":["interview"],"title":"Yifeng_20230911","uri":"/yifeng_20230911/"},{"categories":["interview"],"content":"16、zk的特性? zk实现注册中心的原理?服务提供者挂了，zk会怎么样? ","date":"2023-09-11","objectID":"/yifeng_20230911/:15:0","tags":["interview"],"title":"Yifeng_20230911","uri":"/yifeng_20230911/"},{"categories":["interview"],"content":"17、说下mysql都有哪些日志文件?每个文件的作用是什么? ","date":"2023-09-11","objectID":"/yifeng_20230911/:16:0","tags":["interview"],"title":"Yifeng_20230911","uri":"/yifeng_20230911/"},{"categories":["interview"],"content":"18、说下mysql 数据结构文件有哪些? ","date":"2023-09-11","objectID":"/yifeng_20230911/:17:0","tags":["interview"],"title":"Yifeng_20230911","uri":"/yifeng_20230911/"},{"categories":["interview"],"content":"19、说下kafka都有哪些核心文件? ","date":"2023-09-11","objectID":"/yifeng_20230911/:18:0","tags":["interview"],"title":"Yifeng_20230911","uri":"/yifeng_20230911/"},{"categories":["interview"],"content":"20、说下redission分布式锁的实现原理? ","date":"2023-09-11","objectID":"/yifeng_20230911/:19:0","tags":["interview"],"title":"Yifeng_20230911","uri":"/yifeng_20230911/"},{"categories":["interview"],"content":"21、说下lru算法的实现? ","date":"2023-09-11","objectID":"/yifeng_20230911/:20:0","tags":["interview"],"title":"Yifeng_20230911","uri":"/yifeng_20230911/"},{"categories":["interview"],"content":"1 利用LinkedHashMap实现LRU算法 /** * 利用LinkedHashMap实现的原理： * * get 能取到元素方法，会执行 ：afterNodeAccess(Node\u003cK,V\u003e e) 方法；move node to last； 前提 accessOrder 要设置为true * 默认是false； 所以LRUCache构造方法要调用 * public LinkedHashMap(int initialCapacity,float loadFactor,boolean accessOrder) { * super(initialCapacity, loadFactor); * this.accessOrder = accessOrder; * } * put方法添加新元素成功后调用 void afterNodeInsertion(boolean evict)： // possibly remove eldest: * afterNodeInsertion会 possibly remove eldest （从头部移除；前提：removeEldestEntry 返回true） * 实际是 在HashMap中实现 ： 最后一个参数evict为true会执行移除最久为使用的 * public V put(K key, V value) { * return putVal(hash(key), key, value, false, true); * } * @param \u003cK\u003e * @param \u003cV\u003e */ class LRUCache\u003cK,V\u003e extends LinkedHashMap\u003cK,V\u003e { private int capacity ; /** * LinkedHashMap 的构造方法中， accessOrder = false; 代表访问序 * @param capacity * @param loadFactor */ public LRUCache( int capacity,float loadFactor) { super(capacity, loadFactor,true); this.capacity = capacity; } public LRUCache(int capacity) { super(capacity, 0.75F,true); this.capacity = capacity; } /** * 容量不够时移除； LinkedHashMap是直接返回false，不移除 * @return */ @Override protected boolean removeEldestEntry(Map.Entry\u003cK, V\u003e eldest) { return super.size() \u003e this.capacity; } } ","date":"2023-09-11","objectID":"/yifeng_20230911/:20:1","tags":["interview"],"title":"Yifeng_20230911","uri":"/yifeng_20230911/"},{"categories":["interview"],"content":"2 哈希表+双向链表实现 定义 size，capacity，cache（hashmap 哈希表实现，Map\u003cK,Node\u003e cache ;） package com.example.demo; import java.util.HashMap; import java.util.Map; public class LRUCache\u003cK, V\u003e { private int size; private int capacity; /** * 存储数据 */ private Map\u003cK, Node\u003e cache; /** * 标识作用，虚拟节点，辅助用，减少空判断,不存储具体的值 */ private Node header; /** * 标识作用，不存储具体的值 */ private Node tail; public LRUCache() { this(16); } public LRUCache(int capacity) { this.capacity = capacity; this.cache = new HashMap\u003c\u003e(capacity); this.header = new Node(); this.tail = new Node(); this.header.next = this.tail; this.tail.prev = this.header; } /** * 能访问到元素需要将原来的元素移动到末尾，标识最近访问 * * @param k * @return */ public V get(K k) { Node node = cache.get(k); if (node == null) { return null; } node.moveToLast(node); return node.v; } /** * \u003cp\u003e1 加入的元素放到末尾，放之前先判断是否存在同样的元素，存在则修改前后指针。\u003c/p\u003e * \u003cp\u003e 2 不存在即就是新加入的元素，判断超过容量限制来决定删除头部元素 \u003c/p\u003e * * @param k * @param v */ public void put(K k, V v) { Node node = cache.get(k); if (node == null) { node = new Node(k,v); cache.put(k,node); size++; node.addToLast(node); if(size \u003e capacity){ //删除头部 Node first = node.removeFirst(); cache.remove(first.k); size -- ; } }else { node.v = v ; node.moveToLast(node); } } /** * 必须是非静态的类才能使用外部类的泛型和使用外部类的变量tail和header */ class Node { K k; V v; /** * 前驱节点 */ Node prev; Node next; public Node() { } public Node(K k, V v) { this.k = k; this.v = v; } private void addToLast(Node node) { node.prev = tail.prev; node.next = tail; node.prev.next = node; tail.prev = node; } private void addToFirst(Node node) { node.next = header.next; node.prev = header; node.next.prev = node; header.next = node; } private void moveToLast(Node node) { remove(node); addToLast(node); } private void moveToFirst(Node node) { remove(node); addToFirst(node); } private Node remove(Node node) { if (node == null) return null; if (node.prev == null || node.next == null) { return node; } node.prev.next = node.next; node.next.prev = node.prev; node.next = null; node.prev = null; return node; } private Node removeFirst() { Node h = header.next; return remove(h); } private Node removeLast() { Node t = tail.prev; return remove(t); } } public static void main(String[] args) { LRUCache\u003cInteger, Integer\u003e lruCache = new LRUCache\u003c\u003e(10); for (int i = 0; i \u003c 20; i++) { // 1 会一直在 lruCache.put(1,1); lruCache.put(i,i); } for (int i = 18; i \u003e 12; i--) { lruCache.put(i,i); } for (int i = 0 ; i\u003c 20; i++){ System.out.print(lruCache.get(i) + \" \"); } } } ","date":"2023-09-11","objectID":"/yifeng_20230911/:20:2","tags":["interview"],"title":"Yifeng_20230911","uri":"/yifeng_20230911/"},{"categories":[],"content":"如果在SpringBoot项目中生成的Excel文件在打包成JAR或WAR文件后无法打开，可能是因为文件路径或资源访问的问题。 在默认情况下，当你将Excel文件放置在SpringBoot项目中的src/main/resources目录下时，它会被打包到生成的JAR或WAR文件中。 原因 * 推荐 springboot的resource目录下的文件是默认自动压缩的，但是我这里怎么是变大了….反正解决了 解决 * 在同级目录下的POM文件的下添加以下插件配置(注:必须重新mavenclean否则不生效) \u003cplugin\u003e \u003cgroupId\u003eorg.apache.maven.plugins\u003c/groupId\u003e \u003cartifactId\u003emaven-resources-plugin\u003c/artifactid\u003e \u003cconfiguration\u003e \u003cencoding\u003eUTF-8\u003c/encoding\u003e \u003c!-- 避免被自动压缩,导致文件受损打开报错 --\u003e \u003cnonFilteredFileExtensions\u003e \u003cnonFilteredFileExtension\u003exlsx\u003c/nonFilteredFileExtension\u003e \u003cnonFilteredFileExtension\u003exls\u003c/nonFilteredFileExtension\u003e \u003c/nonFilteredFileExtensions\u003e \u003c/configuration\u003e \u003c/plugin\u003e ","date":"2023-08-30","objectID":"/after-package-xlsx-cannot-open/:0:0","tags":["java","maven"],"title":" After maven package Xlsx Cannot Open","uri":"/after-package-xlsx-cannot-open/"},{"categories":[],"content":"Hugo -server -D 404问题记录 after exec hugo server -D commond ,but return 404 because outside, use the termux on android , but not work. caused by git submodule ,the themes dir is empty. solution:git submodule update --init --recursive. ","date":"2023-08-30","objectID":"/huho-blog/:1:0","tags":[],"title":"Hugo搭建博客部分问题记录","uri":"/huho-blog/"},{"categories":["笔记"],"content":"使用Docker快速搭建ZooKeeper集群 ","date":"2023-08-15","objectID":"/zookeeper-cluster-demo/:0:0","tags":["docker","docker compose","容器编排"],"title":"使用Docker快速搭建ZooKeeper集群","uri":"/zookeeper-cluster-demo/"},{"categories":["笔记"],"content":"背景 使用Docker 安装zookeeper只是安装单机的。现在，我们要用 docker-compose 来编排集群容器。因为一个一个地启动 ZK 太麻烦了. ","date":"2023-08-15","objectID":"/zookeeper-cluster-demo/:1:0","tags":["docker","docker compose","容器编排"],"title":"使用Docker快速搭建ZooKeeper集群","uri":"/zookeeper-cluster-demo/"},{"categories":["笔记"],"content":"1. docker-compose.yml 文件 首先创建一个名为 docker-compose.yml 的文件, 其内容如下: version: '3' services: zoo1: image: zookeeper restart: always container_name: zoo1 ports: - \"2181:2181\" environment: ZOO_MY_ID: 1 ZOO_SERVERS: server.1=zoo1:2888:3888;2181 server.2=zoo2:2888:3888;2182 server.3=zoo3:2888:3888;2183 server.4=zoo4:2888:3888;2184 zoo2: image: zookeeper restart: always container_name: zoo2 ports: - \"2182:2181\" environment: ZOO_MY_ID: 2 ZOO_SERVERS: server.1=zoo1:2888:3888;2181 server.2=zoo2:2888:3888;2182 server.3=zoo3:2888:3888;2183 server.4=zoo4:2888:3888;2184 zoo3: image: zookeeper restart: always container_name: zoo3 ports: - \"2183:2181\" environment: ZOO_MY_ID: 3 ZOO_SERVERS: server.1=zoo1:2888:3888;2181 server.2=zoo2:2888:3888;2182 server.3=zoo3:2888:3888;2183 server.4=zoo4:2888:3888;2184 zoo4: image: zookeeper restart: always container_name: zoo4 ports: - \"2184:2181\" environment: ZOO_MY_ID: 4 ZOO_SERVERS: server.1=zoo1:2888:3888;2181 server.2=zoo2:2888:3888;2182 server.3=zoo3:2888:3888;2183 server.4=zoo4:2888:3888;2184 ","date":"2023-08-15","objectID":"/zookeeper-cluster-demo/:2:0","tags":["docker","docker compose","容器编排"],"title":"使用Docker快速搭建ZooKeeper集群","uri":"/zookeeper-cluster-demo/"},{"categories":["笔记"],"content":"编排配置说明 1、version: 版本号，一般来说，和你的 Docker Engine 的版本相匹配；这里选择 3； 2、image: 镜像版本：默认使用的是latest版本，可缺省； 3、ZOO_MY_ID（环境变量），该id在集群中必须是唯一的，并且其值应介于1和255之间； 请注意，如果使用已包含 myid 文件的 /data 目录启动容器，则此变量将不会产生任何影响。相当于你在zoo.cfg中指定的dataDir目录中创建文件myid，并且文件的内容就是范围为1到255的整数； 4、ZOO_SERVERS（环境变量）: 此变量允许您指定Zookeeper集群的计算机列表； 每个条目都应该这样指定：server.id=::[:role];[:] 条目间空格分隔 id 是一个数字，表示集群中的服务器ID； address1表示这个服务器的ip地址； 集群通信端⼝: port1 表示这个服务器与集群中的 Leader 服务器交换信息的端口； 集群选举端⼝: port2 表示万一集群中的 Leader 服务器挂了，需要一个端口来重新进行选举，选出一个新的 Leader，而这个端口就是用来执行选举时服务器相互通信的端口； role: 默认是 participant,即参与过半机制的⻆⾊，选举，事务请求过半提交，还有⼀个是observer, 观察者，不参与选举以及过半机制。 client port address 是可选的，如果未指定，则默认为 “0.0.0.0” ； client port 位于分号的右侧。从3.5.0开始，zoo.cfg中不再使用clientPort和clientPortAddress配置参数。作为替代，client port 用来表示客户端连接 Zookeeper 服务器的端口，Zookeeper 会监听这个端口，接受客户端的访问请求。 请注意，如果使用已包含zoo.cfg文件的/conf目录启动容器，则此变量不会产生任何影响。换句话说，zoo.cfg文件中包含集群的计算机列表，此时该环境变量不生效。 ","date":"2023-08-15","objectID":"/zookeeper-cluster-demo/:2:1","tags":["docker","docker compose","容器编排"],"title":"使用Docker快速搭建ZooKeeper集群","uri":"/zookeeper-cluster-demo/"},{"categories":["笔记"],"content":"2. 启动集群 打开命令行提示符，进入 docker-compose.yml 所在的目录，接着执行 docker-compose -f docker-compose.yml up 最后检查一下运行情况：容器 zoo1,zoo2,zoo3,zoo4 都在运行中。 ","date":"2023-08-15","objectID":"/zookeeper-cluster-demo/:3:0","tags":["docker","docker compose","容器编排"],"title":"使用Docker快速搭建ZooKeeper集群","uri":"/zookeeper-cluster-demo/"},{"categories":["笔记"],"content":"3. 检测集群状态 检测集群状态的命令是 zkServer.sh status /conf/zoo.cfg。使用该命令的前提是要先登入容器中，可以使用 docker exec -it zoo1 bash 登入 zoo1 中。 he@LAPTOP-0PME2UF0:~$ docker exec -it zoo1 bash root@aaa4ead811f0:/apache-zookeeper-3.8.2-bin# zkServer.sh status /conf/zoo.cfg ZooKeeper JMX enabled by default Using config: /conf/zoo.cfg Client port found: 2181. Client address: localhost. Client SSL: false. Mode: follower he@LAPTOP-0PME2UF0:~$ docker exec -it zoo2 bash root@14fc28ab859e:/apache-zookeeper-3.8.2-bin# zkServer.sh status /conf/zoo.cfg ZooKeeper JMX enabled by default Using config: /conf/zoo.cfg Client port found: 2182. Client address: localhost. Client SSL: false. Mode: follower he@LAPTOP-0PME2UF0:~$ docker exec -it zoo3 bash root@7d5c0792e1d5:/apache-zookeeper-3.8.2-bin# zkServer.sh status /conf/zoo.cfg ZooKeeper JMX enabled by default Using config: /conf/zoo.cfg Client port found: 2183. Client address: localhost. Client SSL: false. Mode: follower he@LAPTOP-0PME2UF0:~$ docker exec -it zoo4 bash root@30e423d9997b:/apache-zookeeper-3.8.2-bin# zkServer.sh status /conf/zoo.cfg ZooKeeper JMX enabled by default Using config: /conf/zoo.cfg Client port found: 2184. Client address: localhost. Client SSL: false. Mode: leader ","date":"2023-08-15","objectID":"/zookeeper-cluster-demo/:4:0","tags":["docker","docker compose","容器编排"],"title":"使用Docker快速搭建ZooKeeper集群","uri":"/zookeeper-cluster-demo/"},{"categories":["笔记"],"content":"4. ZooKeeper 集群角色介绍 Zookeeper 集群模式⼀共有三种类型的⻆⾊： Leader : 处理所有的事务请求（写请求），可以处理读请求，集群中只能有⼀个Leader Follower : 只能处理读请求，同时作为 Leader的候选节点，即如果Leader宕机，Follower节点要参与到新的Leader选举中，有可能成为新的Leader节点。| Observer : 只能处理读请求。不能参与选举| ","date":"2023-08-15","objectID":"/zookeeper-cluster-demo/:5:0","tags":["docker","docker compose","容器编排"],"title":"使用Docker快速搭建ZooKeeper集群","uri":"/zookeeper-cluster-demo/"},{"categories":["笔记"],"content":"5. 不宕机动态扩容和缩容 ZooKeeper 支持通过修改 zoo.cfg 并重启来改变ZooKeeper集群中的服务器，但是这种方式，不是很方便，上线一台服务器或者下线一台服务器需要对所有的服务器进行重启。 ZooKeeper 3.5.0 提供了⽀持动态扩容/缩容的新特性。但是通过客户端 API可以变更服务端集群状态是件很危险的事情，所以在ZooKeeper 3.5.3 版本要⽤动态配置，需要开启超级管理员身份验证模式 ACLs。 关于ZooKeeper的ACL权限控制， Step1: 我为 super:qwer1234 生成了 digest 为 super:YjJhp1/a1jnzeGTDN7nAUxFcep8=； Step2: docker-compose -f docker-compose.yml down 停掉并删除原来的容器； Step3: 修改 docker-compose.yml 的内容： version: '3' services: zoo1: image: zookeeper restart: always container_name: zoo1 ports: - \"2181:2181\" environment: ZOO_MY_ID: 1 ZOO_SERVERS: server.1=zoo1:2888:3888;2181 server.2=zoo2:2888:3888;2181 server.3=zoo3:2888:3888;2181 server.4=zoo4:2888:3888;2181 ZOO_CFG_EXTRA: \"reconfigEnabled=true\" JVMFLAGS: \"-Dzookeeper.DigestAuthenticationProvider.superDigest=super:YjJhp1/a1jnzeGTDN7nAUxFcep8=\" zoo2: image: zookeeper restart: always container_name: zoo2 ports: - \"2182:2181\" environment: ZOO_MY_ID: 2 ZOO_SERVERS: server.1=zoo1:2888:3888;2181 server.2=zoo2:2888:3888;2181 server.3=zoo3:2888:3888;2181 server.4=zoo4:2888:3888;2181 ZOO_CFG_EXTRA: \"reconfigEnabled=true\" JVMFLAGS: \"-Dzookeeper.DigestAuthenticationProvider.superDigest=super:YjJhp1/a1jnzeGTDN7nAUxFcep8=\" zoo3: image: zookeeper restart: always container_name: zoo3 ports: - \"2183:2181\" environment: ZOO_MY_ID: 3 ZOO_SERVERS: server.1=zoo1:2888:3888;2181 server.2=zoo2:2888:3888;2181 server.3=zoo3:2888:3888;2181 server.4=zoo4:2888:3888;2181 ZOO_CFG_EXTRA: \"reconfigEnabled=true\" JVMFLAGS: \"-Dzookeeper.DigestAuthenticationProvider.superDigest=super:YjJhp1/a1jnzeGTDN7nAUxFcep8=\" zoo4: image: zookeeper restart: always container_name: zoo4 ports: - \"2184:2181\" environment: ZOO_MY_ID: 4 ZOO_SERVERS: server.1=zoo1:2888:3888;2181 server.2=zoo2:2888:3888;2181 server.3=zoo3:2888:3888;2181 server.4=zoo4:2888:3888;2181 ZOO_CFG_EXTRA: \"reconfigEnabled=true\" JVMFLAGS: \"-Dzookeeper.DigestAuthenticationProvider.superDigest=super:YjJhp1/a1jnzeGTDN7nAUxFcep8=\" ZOO_CFG_EXTRA: “reconfigEnabled=true” 开启了默认关闭的客户端 reconfig API JVMFLAGS: “-Dzookeeper.DigestAuthenticationProvider.superDigest=super:YjJhp1/a1jnzeGTDN7nAUxFcep8=” 设置了超级管理员账号为 super，明文密码为 qwer1234 Step4: 重启集群 在 docker-compose.yml 所在目录执行以下命令： docker-compose -f docker-compose.yml up Step5: 登录客户端 ,使用如下命令在容器内连接该节点的 ZooKeeper 服务 zkCli.sh -server localhost:2181 接下来就是使用ZooKeeper客户端的reconfig命令了。 5.1 查看当前集群信息 get /zookeeper/config 执行结果如下： [zk: localhost:2181(CONNECTED) 0] get /zookeeper/config server.1=zoo1:2888:3888:participant;0.0.0.0:2181 server.2=zoo2:2888:3888:participant;0.0.0.0:2181 server.3=zoo3:2888:3888:participant;0.0.0.0:2181 server.4=zoo4:2888:3888:participant;0.0.0.0:2181 version=100000000 5.2 扩缩容前的认证超级管理员身份 ,客户端命令行执行addauth digest super:qwer1234 5.3 移除集群中的一台服务器 reconfig -remove \u003cid\u003e比如说，从集群中移除ID为3的ZooKeeper服务： [zk: localhost:2181(CONNECTED) 6] reconfig -remove 3 Committed new configuration: server.1=zoo1:2888:3888:participant;0.0.0.0:2181 server.2=zoo2:2888:3888:participant;0.0.0.0:2181 server.4=zoo4:2888:3888:participant;0.0.0.0:2181 version=300000001 5.4 为集群新增一台服务器 reconfig -add server.id=\u003caddress1\u003e:\u003cport1\u003e:\u003cport2\u003e[:role];[\u003cclient port address\u003e:]\u003cclient port\u003e 比如说，把刚才移除的服务器再添加回来： [zk: localhost:2181(CONNECTED) 7] reconfig -add server.3=zoo3:2888:3888;2181 Committed new configuration: server.1=zoo1:2888:3888:participant;0.0.0.0:2181 server.2=zoo2:2888:3888:participant;0.0.0.0:2181 server.3=zoo3:2888:3888:participant;0.0.0.0:2181 server.4=zoo4:2888:3888:participant;0.0.0.0:2181 version=400000001 ","date":"2023-08-15","objectID":"/zookeeper-cluster-demo/:6:0","tags":["docker","docker compose","容器编排"],"title":"使用Docker快速搭建ZooKeeper集群","uri":"/zookeeper-cluster-demo/"},{"categories":["笔记"],"content":"6. 过半机制 participant ⻆⾊能够形成集群（过半机制），如果集群内的 participant 有一半及以上都宕机了，此时客户端将不再可用。 6.1 半数服务器宕机 比如说，停掉了 zoo3 和 zoo4：此时，没有停掉的服务，也启动不了客户端： 6.2 半数以上服务器可用,然后重启了 zoo3： 稍等一会之后，集群再次变得可用了。 ","date":"2023-08-15","objectID":"/zookeeper-cluster-demo/:7:0","tags":["docker","docker compose","容器编排"],"title":"使用Docker快速搭建ZooKeeper集群","uri":"/zookeeper-cluster-demo/"},{"categories":["笔记"],"content":"问题记录 ","date":"2023-08-15","objectID":"/zookeeper-cluster-demo/:8:0","tags":["docker","docker compose","容器编排"],"title":"使用Docker快速搭建ZooKeeper集群","uri":"/zookeeper-cluster-demo/"},{"categories":["笔记"],"content":"1 : Client port not found in server configs 参考官网 ZOO_SERVERS 的配置，以zoo1为例: server.1=zoo1:2888:3888 server.2=zoo2:2888:3888 server.3=zoo3:2888:3888 server.4=zoo4:2888:3888 没有加上端口号，实践后遇上了问题。 解决方案：立即停止和删除容器、网络、卷： docker-compose -f docker-compose.yml down 然后，改成上面给出的 docker-compose.yml 文件内容，然后用 docker-compose -f docker-compose.yml up 重启集群服务器。 ","date":"2023-08-15","objectID":"/zookeeper-cluster-demo/:8:1","tags":["docker","docker compose","容器编排"],"title":"使用Docker快速搭建ZooKeeper集群","uri":"/zookeeper-cluster-demo/"},{"categories":["笔记"],"content":"2 :KeeperErrorCode = Reconfig is disabled 从3.5.0开始到3.5.3之前，没有办法禁用动态重新配置功能。我们希望提供禁用重新配置功能的选项，因为启用重新配置后，我们存在一个安全问题，即恶意参与者可以对ZooKeeper集合的配置进行任意更改，包括向集合中添加受损服务器。我们倾向于让用户自行决定是否启用它，并确保适当的安全措施到位。因此，在3.5.3中引入了reconfigEnabled configuration选项，以便可以完全禁用重新配置功能，并且通过reconfig API重新配置集群的任何尝试（无论是否具有身份验证）在默认情况下都将失败，除非reconfigEnabled设置为true。要将选项设置为true，配置文件（zoo.cfg）应包含： reconfigEnabled=true ","date":"2023-08-15","objectID":"/zookeeper-cluster-demo/:8:2","tags":["docker","docker compose","容器编排"],"title":"使用Docker快速搭建ZooKeeper集群","uri":"/zookeeper-cluster-demo/"},{"categories":["笔记"],"content":"3: Insufficient permission 解决方案：addauth digest super:qwer1234 赋予超级管理员权限 [zk: localhost:2181(CONNECTED) 4] reconfig -remove 3 Insufficient permission : [zk: localhost:2181(CONNECTED) 5] addauth digest super:qwer1234 [zk: localhost:2181(CONNECTED) 6] reconfig -remove 3 Committed new configuration: server.1=zoo1:2888:3888:participant;0.0.0.0:2181 server.2=zoo2:2888:3888:participant;0.0.0.0:2181 server.4=zoo4:2888:3888:participant;0.0.0.0:2181 version=300000001 ","date":"2023-08-15","objectID":"/zookeeper-cluster-demo/:8:3","tags":["docker","docker compose","容器编排"],"title":"使用Docker快速搭建ZooKeeper集群","uri":"/zookeeper-cluster-demo/"},{"categories":["笔记"],"content":"demo-common ","date":"2023-08-09","objectID":"/demo-project-devops-record/:1:0","tags":["devops","jvm","docker"],"title":"演示项目-运维部署记录（demo project devops record）","uri":"/demo-project-devops-record/"},{"categories":["笔记"],"content":"Dockerfile FROM it-registry.itlaohuo.top/libary/centos7-jdk1.8:1.0.22-arms-agent-0510 WORKDIR /home COPY ./democommon-service/target/democommon-service-*.jar /home COPY ./docker-file/demo-ccommon/tools /home/tools/ COPY ./docker-file/demo-ccommon/conf /home/conf/ COPY ./docker-file/demo-ccommon/bin/entrypoint / RUN chmod +x /entrypoint.sh ENTRYPOINT [\"/entrypoint.sh\"] ","date":"2023-08-09","objectID":"/demo-project-devops-record/:1:1","tags":["devops","jvm","docker"],"title":"演示项目-运维部署记录（demo project devops record）","uri":"/demo-project-devops-record/"},{"categories":["笔记"],"content":"tools 目录文件 arthas_conn.sh #!/bin/bash JAVA_TOOL_OPTIONS= APP_NAME=\"vowrk-common\" ARTHAS_PATH = \"/home/webapps/ArmsAgent2/arthas/arthas-boot.jar\" JAVA_PID=$(ps -ef |grep ${APP_NAME} |grep -v grep |awk '{print $2}') TELNET_PORT=$(netstat -lnp |grep 127.0.0.1 |awk '{print $4}' |awk -F':' '{print $2}') if [[ -n ${TELNET_PORT}]]; then java - jar ${ARTHAS_PATH} ${JAVA_PID} --telnet-port ${TELNET_PORT} else java -jar ${ARTHAS_PATH} ${JAVA_PID} fi check_health.sh #!/bin/bash http_code=(\"200\", \"503\") if [[ -n \"${http_code[@]}\" = ~ `curl --connect-timeout 8 -m 8 -sL -w '%{http_code}' http://127.0.0.1:8080/actuator/health -o /dev/null` ]]; then echo \"server health\" exit 0 else echo \"check health result\" : `curl --connect-timeout 8 -m 8 -sL http://127.0.0.1:8080/actuator/health ` echo \"server not health\" exit 1 fi check_ready.sh #!/bin/bash http_code=(\"200\", \"503\") if [[ -n \"${http_code[@]}\" = ~ `curl --connect-timeout 8 -m 8 -sL -w '%{http_code}' http://127.0.0.1:8080/actuator/health -o /dev/null` ]]; then echo \"server health\" exit 0 else echo \"check health result\" : `curl --connect-timeout 8 -m 8 -sL http://127.0.0.1:8080/actuator/health ` echo \"server not health\" exit 1 fi exit_pod.sh #!/bin/bash curl --connect-timeout 8 -m 8 -sL http://127.0.0.1:22222/offline sleep 20 ps -ef | grep java | awk '{print $2}' | awk 'NR==1' | xargs kill -15 http_code=(\"200\", \"503\") for i in {1..60} do if [[ -n \"${http_code[@]}\" = ~ `curl --connect-timeout 8 -m 8 -sL -w '%{http_code}' http://127.0.0.1:8080/actuator/health -o /dev/null` ]]; then echo \"server running\" exit 0 else echo \"server stop success\" exit 0 fi sleep 3 done fullgc_dump.sh #!/bin/bash export JAVA_TOOL_OPTIONS= FGC_NUMD=0 FGC_NUM=0 tmp_file=\"/tmp/fullgc\" health_file=\"/tmp/tools/check_health.sh\" ready_file=\"/tmp/tools/ready_health.sh\" while true;do sleep 10 [[ -f ${tmp_file} ]] \u0026\u0026 FGC_NUMD=$(cat ${tmp_file} |tail -1) FGC_NUM=$(jstat -gc 1 |awk '{print $15}' |tail -1) if [ ${FGC_NUM} -gt ${FGC_NUMD} ]; then mv ${health_file} ${health_file}.bak echo 1 \u003e ${health_file} mv ${ready_file} ${ready_file}.bak echo 0 \u003e ${ready_file} jmap -F -dump:live,format=b,file=./dump-${date +%F-%M-%S}.hprof 1 echo ${FGC_NUM} \u003e\u003e ${tmp_file} rm -f ${health_file} ${ready_file} mv ${health_file}.bak ${health_file} mv ${ready_file}.bak ${ready_file} fi done ","date":"2023-08-09","objectID":"/demo-project-devops-record/:1:2","tags":["devops","jvm","docker"],"title":"演示项目-运维部署记录（demo project devops record）","uri":"/demo-project-devops-record/"},{"categories":["笔记"],"content":"conf 目录 cn-dev.sh JAVA_OPTION=\"-DsetaEnv=dev -Dserver.port=8080 \\ -Dapp.id=demo-cn-common -Dapollo.meta=http://it-apollo-dev.itlaohuo.top/ \\ -Denv=dev -DserverAddr=it-seata-nacos.itlaohuo.top \\ -Xmx8000M -Xms8000M -Xss512K -XX:MaxMetaspaceSize=1024M -XX:MetaspaceSize=1024M -XX:+UseG1GC \\ -XX:MaxGCPauseMillis=200 -XX:G1HeapRegionSize=16M \\ -Dit.log.switch=true \\ -Dit.app.name=demo-common \\ -Dit.app.env=dev \\ -D.server.tomcat.max-thread=500 \\ -Dnamespace=2506f038-7c4b-4af3-8fa1-c4915ac6694f -Dencrptionkey=23asdwee2343afwe \\ -Duser.home=/data01/demo-common/logs\" cn-pro.sh JAVA_OPTION=\"-Dserver.port=8080 \\ -Dapp.id=demo-cn-common \\ -Dapollo.meta=http://it-apollo-pro.itlaohuo.top/ \\ -Dapollo.bootstarp.namespaces=application,v2 \\ -Dapollo.clientAuthKeyName=d6e25b4a0cf4b60 -Denv=pro -Dnamespace=759623-xxxxxxx\\ -DserverAddr=it-seata-nacos.itlaohuo.top -DsetaEnv=pro -Dencrptionkey=07cxxxxxxxx \\ -Duser.home=/data01/democommon \\ -Xmx21000M -Xms21000M -XX:MaxMetaspaceSize=1024M -XX:MetaspaceSize=1024M -XX:+UseG1GC \\ -XX:MaxGCPauseMillis=200 -XX:+ParallelRefProcEnabled \\ -XX:ErrorFile=/home/logs/gc/hs_err_pid%p.log -Xloggc:/home/gc/gc.log \\ -XX:HeadDumpPath=/home/logs/gc/commondump \\ -XX:+HeadDumpOnOutOfMemoryError \\ -XX:+PrintGCDetails -XX:+PrintGCDateStamps \\ -XX:+PrintGCApplicationConcurrentTime \\ -XX:+PrintGCApplicationStoppedTime \\ -XX:+PrintHeadAtGC \\ -Dit.log.switch=true \\ -Dit.app.name=demo-common \\ -Dit.app.env=pro \\ -D.rocket.mq.gray.switch=true \\ -Ddubbo.labels=stag=v2 \\ -Dxxl.job.executor.gray=false\" cn-pro-grey.sh JAVA_OPTION=\"-Dserver.port=8080 \\ -Dapp.id=demo-cn-common \\ -Dapollo.meta=http://it-apollo-pro.itlaohuo.top/ \\ -Dapollo.bootstarp.namespaces=application,v1 \\ -Dapollo.clientAuthKeyName=d6e25b4a0cf4b60 -Denv=pro -Dnamespace=759623-xxxxxxx\\ -DserverAddr=it-seata-nacos.itlaohuo.top -DsetaEnv=pro -Dencrptionkey=07cxxxxxxxx \\ -Duser.home=/data01/democommon \\ -Xmx22000M -Xms22000M -XX:MaxMetaspaceSize=1024M -XX:MetaspaceSize=1024M -XX:+UseG1GC \\ -XX:MaxGCPauseMillis=200 -XX:+ParallelRefProcEnabled \\ -XX:ErrorFile=/home/logs/gc/hs_err_pid%p.log -Xloggc:/home/gc/gc.log \\ -XX:HeadDumpPath=/home/logs/gc/commondump \\ -XX:+HeadDumpOnOutOfMemoryError \\ -XX:+PrintGCDetails -XX:+PrintGCDateStamps \\ -XX:+PrintGCApplicationConcurrentTime \\ -XX:+PrintGCApplicationStoppedTime \\ -XX:+PrintHeadAtGC \\ -Dit.log.switch=true \\ -Dit.app.name=demo-common-grey \\ -Dit.app.env=pro \\ -D.rocket.mq.gray.switch=true \\ -Ddubbo.labels=stag=v1 \\ -Dxxl.job.executor.gray=true\" ","date":"2023-08-09","objectID":"/demo-project-devops-record/:1:3","tags":["devops","jvm","docker"],"title":"演示项目-运维部署记录（demo project devops record）","uri":"/demo-project-devops-record/"},{"categories":["笔记"],"content":"bin目录 entrypoint.sh #!/bin/bash JAVA_DUBBO_GROUP=\"\" if [ $DUBBO_GROUP ];then JAVA_DUBBO_GROUP=\"-Ddubbo.provider.group=${DUBBO_GROUP} -Dbubbo.consumer.group=${DUBBO_GROUP}\" fi fi [[$ENV ==in* ]] ;then ln -sf /usr/share/zoneinfo/Asia/Calcutta /etc/licaltime elif [[$ENV ==eur-pro* ]] ;then ln -sf /usr/share/zoneinfo/Europe/Paris /etc/licaltime fi mv /home/conf/${ENV}.sh /tmp/${ENV}.sh rm -rf /home/conf/*.sh mv /tmp/${ENV}.sh /home/conf/${ENV}.sh source /home/conf/${ENV}.sh JAVA_OPTION = \"${JAVA_DUBBO_GROUP}\" \\ ${JAVA_OPTION} JAR_FILE= `find /home/*.jar | awk \"{print $1}\"` exec java jar -server $JAVA_OPTION -jar $JAR_FILE ","date":"2023-08-09","objectID":"/demo-project-devops-record/:1:4","tags":["devops","jvm","docker"],"title":"演示项目-运维部署记录（demo project devops record）","uri":"/demo-project-devops-record/"},{"categories":["笔记"],"content":"devops 应用流水线 输入源 代码源 git仓库地址 默认分支 构建应用 应用类型：JAVA 构建类型：Maven JAVA版本 工具版本 构建命令： docker run --rm -u 5001 -v `pwd`:/home/code it-registry.itlaohuo.top/libray/centos7-jdk1.8:1,0.16-arms-docker-build /bin/bash -c \"export LC_ALL=en_US.UTF-8 \u0026\u0026 mvn -B clean deploy -U -Dmaven.test.skip=true -Dautoconfig.skip -Djob.skip -f ./pom.xml\" 代码扫描 sonar 构建镜像 Dockerfile 输入方式： 勾选 外部输入源（其他选项：本地输入源，自定义Dockerfile） Dockerfile仓库：http://it-gitlab.itlaohuo.top/v-work/docker-file.git 分支名称：master Dockerfile路径：./demo-common/Dockerfile 推送镜像 部署 制品路径：使用上一步推送的镜像 部署逻辑集群： 命名空间： 启动参数： 环境变量：ENV=cn-dev 停止前命令：sh /home/tools/exit_pod.sh 规格：CPU-Request = 1.0 核 CPU-Limited = 1.0核心 内存—Request = 2.0GB 内存-Limited = 2.0GB ； 副本数= 2 缩放/升级策略： 健康检查：命令探针；sh /home/tools/check_health.sh ; 设定频率，超时时间，失败阈值 ","date":"2023-08-09","objectID":"/demo-project-devops-record/:1:5","tags":["devops","jvm","docker"],"title":"演示项目-运维部署记录（demo project devops record）","uri":"/demo-project-devops-record/"},{"categories":["笔记"],"content":"1. Kubernetes介绍 (资料来源于B站“冰糖没糖”的分享) ","date":"2023-08-04","objectID":"/kubernetes/:0:0","tags":["devops","kubernetes"],"title":"Kubernetes使用手册 (资料来源于B站“冰糖没糖”的分享)","uri":"/kubernetes/"},{"categories":["笔记"],"content":"1.1 应用部署方式演变 在部署应用程序的方式上，主要经历了三个时代： 传统部署：互联网早期，会直接将应用程序部署在物理机上 优点：简单，不需要其它技术的参与 缺点：不能为应用程序定义资源使用边界，很难合理地分配计算资源，而且程序之间容易产生影响 虚拟化部署：可以在一台物理机上运行多个虚拟机，每个虚拟机都是独立的一个环境 优点：程序环境不会相互产生影响，提供了一定程度的安全性 缺点：增加了操作系统，浪费了部分资源 容器化部署：与虚拟化类似，但是共享了操作系统 优点： 可以保证每个容器拥有自己的文件系统、CPU、内存、进程空间等 运行应用程序所需要的资源都被容器包装，并和底层基础架构解耦 容器化的应用程序可以跨云服务商、跨Linux操作系统发行版进行部署 容器化部署方式给带来很多的便利，但是也会出现一些问题，比如说： 一个容器故障停机了，怎么样让另外一个容器立刻启动去替补停机的容器 当并发访问量变大的时候，怎么样做到横向扩展容器数量 这些容器管理的问题统称为容器编排问题，为了解决这些容器编排问题，就产生了一些容器编排的软件： Swarm：Docker自己的容器编排工具 Mesos：Apache的一个资源统一管控的工具，需要和Marathon结合使用 Kubernetes：Google开源的的容器编排工具 ","date":"2023-08-04","objectID":"/kubernetes/:1:0","tags":["devops","kubernetes"],"title":"Kubernetes使用手册 (资料来源于B站“冰糖没糖”的分享)","uri":"/kubernetes/"},{"categories":["笔记"],"content":"1.2 kubernetes简介 kubernetes，是一个全新的基于容器技术的分布式架构领先方案，是谷歌严格保密十几年的秘密武器—-Borg系统的一个开源版本，于2014年9月发布第一个版本，2015年7月发布第一个正式版本。 kubernetes的本质是一组服务器集群，它可以在集群的每个节点上运行特定的程序，来对节点中的容器进行管理。目的是实现资源管理的自动化，主要提供了如下的主要功能： 自我修复：一旦某一个容器崩溃，能够在1秒中左右迅速启动新的容器 弹性伸缩：可以根据需要，自动对集群中正在运行的容器数量进行调整 服务发现：服务可以通过自动发现的形式找到它所依赖的服务 负载均衡：如果一个服务起动了多个容器，能够自动实现请求的负载均衡 版本回退：如果发现新发布的程序版本有问题，可以立即回退到原来的版本 存储编排：可以根据容器自身的需求自动创建存储卷 ","date":"2023-08-04","objectID":"/kubernetes/:2:0","tags":["devops","kubernetes"],"title":"Kubernetes使用手册 (资料来源于B站“冰糖没糖”的分享)","uri":"/kubernetes/"},{"categories":["笔记"],"content":"1.3 kubernetes组件 一个kubernetes集群主要是由控制节点(master)、**工作节点(node)**构成，每个节点上都会安装不同的组件。 master：集群的控制平面，负责集群的决策 ( 管理 ) ApiServer : 资源操作的唯一入口，接收用户输入的命令，提供认证、授权、API注册和发现等机制 Scheduler : 负责集群资源调度，按照预定的调度策略将Pod调度到相应的node节点上 ControllerManager : 负责维护集群的状态，比如程序部署安排、故障检测、自动扩展、滚动更新等 Etcd ：负责存储集群中各种资源对象的信息 node：集群的数据平面，负责为容器提供运行环境 ( 干活 ) Kubelet : 负责维护容器的生命周期，即通过控制docker，来创建、更新、销毁容器 KubeProxy : 负责提供集群内部的服务发现和负载均衡 Docker : 负责节点上容器的各种操作 下面，以部署一个nginx服务来说明kubernetes系统各个组件调用关系： 首先要明确，一旦kubernetes环境启动之后，master和node都会将自身的信息存储到etcd数据库中 一个nginx服务的安装请求会首先被发送到master节点的apiServer组件 apiServer组件会调用scheduler组件来决定到底应该把这个服务安装到哪个node节点上 在此时，它会从etcd中读取各个node节点的信息，然后按照一定的算法进行选择，并将结果告知apiServer apiServer调用controller-manager去调度Node节点安装nginx服务 kubelet接收到指令后，会通知docker，然后由docker来启动一个nginx的pod pod是kubernetes的最小操作单元，容器必须跑在pod中至此， 一个nginx服务就运行了，如果需要访问nginx，就需要通过kube-proxy来对pod产生访问的代理 这样，外界用户就可以访问集群中的nginx服务了 ","date":"2023-08-04","objectID":"/kubernetes/:3:0","tags":["devops","kubernetes"],"title":"Kubernetes使用手册 (资料来源于B站“冰糖没糖”的分享)","uri":"/kubernetes/"},{"categories":["笔记"],"content":"1.4 kubernetes概念 Master：集群控制节点，每个集群需要至少一个master节点负责集群的管控 Node：工作负载节点，由master分配容器到这些node工作节点上，然后node节点上的docker负责容器的运行 Pod：kubernetes的最小控制单元，容器都是运行在pod中的，一个pod中可以有1个或者多个容器 Controller：控制器，通过它来实现对pod的管理，比如启动pod、停止pod、伸缩pod的数量等等 Service：pod对外服务的统一入口，下面可以维护者同一类的多个pod Label：标签，用于对pod进行分类，同一类pod会拥有相同的标签 NameSpace：命名空间，用来隔离pod的运行环境 2. kubernetes集群环境搭建 ","date":"2023-08-04","objectID":"/kubernetes/:4:0","tags":["devops","kubernetes"],"title":"Kubernetes使用手册 (资料来源于B站“冰糖没糖”的分享)","uri":"/kubernetes/"},{"categories":["笔记"],"content":"2.1 前置知识点 目前生产部署Kubernetes 集群主要有两种方式： kubeadm Kubeadm 是一个K8s 部署工具，提供kubeadm init 和kubeadm join，用于快速部署Kubernetes 集群。 官方地址：https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm/ 二进制包 从github 下载发行版的二进制包，手动部署每个组件，组成Kubernetes 集群。 Kubeadm 降低部署门槛，但屏蔽了很多细节，遇到问题很难排查。如果想更容易可控，推荐使用二进制包部署Kubernetes 集群，虽然手动部署麻烦点，期间可以学习很多工作原理，也利于后期维护。 ","date":"2023-08-04","objectID":"/kubernetes/:5:0","tags":["devops","kubernetes"],"title":"Kubernetes使用手册 (资料来源于B站“冰糖没糖”的分享)","uri":"/kubernetes/"},{"categories":["笔记"],"content":"2.2 kubeadm 部署方式介绍 kubeadm 是官方社区推出的一个用于快速部署kubernetes 集群的工具，这个工具能通过两条指令完成一个kubernetes 集群的部署： 创建一个Master 节点kubeadm init 将Node 节点加入到当前集群中$ kubeadm join \u003cMaster 节点的IP 和端口\u003e ","date":"2023-08-04","objectID":"/kubernetes/:6:0","tags":["devops","kubernetes"],"title":"Kubernetes使用手册 (资料来源于B站“冰糖没糖”的分享)","uri":"/kubernetes/"},{"categories":["笔记"],"content":"2.3 安装要求 在开始之前，部署Kubernetes 集群机器需要满足以下几个条件： 一台或多台机器，操作系统CentOS7.x-86_x64 硬件配置：2GB 或更多RAM，2 个CPU 或更多CPU，硬盘30GB 或更多 集群中所有机器之间网络互通 可以访问外网，需要拉取镜像 禁止swap 分区 ","date":"2023-08-04","objectID":"/kubernetes/:7:0","tags":["devops","kubernetes"],"title":"Kubernetes使用手册 (资料来源于B站“冰糖没糖”的分享)","uri":"/kubernetes/"},{"categories":["笔记"],"content":"2.4 最终目标 在所有节点上安装Docker 和kubeadm 部署Kubernetes Master 部署容器网络插件 部署Kubernetes Node，将节点加入Kubernetes 集群中 部署Dashboard Web 页面，可视化查看Kubernetes 资源 ","date":"2023-08-04","objectID":"/kubernetes/:8:0","tags":["devops","kubernetes"],"title":"Kubernetes使用手册 (资料来源于B站“冰糖没糖”的分享)","uri":"/kubernetes/"},{"categories":["笔记"],"content":"2.5 准备环境 角色 IP地址 组件 k8s-master01 192.168.5.3 docker，kubectl，kubeadm，kubelet k8s-node01 192.168.5.4 docker，kubectl，kubeadm，kubelet k8s-node02 192.168.5.5 docker，kubectl，kubeadm，kubelet ","date":"2023-08-04","objectID":"/kubernetes/:9:0","tags":["devops","kubernetes"],"title":"Kubernetes使用手册 (资料来源于B站“冰糖没糖”的分享)","uri":"/kubernetes/"},{"categories":["笔记"],"content":"2.6 系统初始化 ","date":"2023-08-04","objectID":"/kubernetes/:10:0","tags":["devops","kubernetes"],"title":"Kubernetes使用手册 (资料来源于B站“冰糖没糖”的分享)","uri":"/kubernetes/"},{"categories":["笔记"],"content":"2.6.1 设置系统主机名以及 Host 文件的相互解析 hostnamectl set-hostname k8s-master01 \u0026\u0026 bash hostnamectl set-hostname k8s-node01 \u0026\u0026 bash hostnamectl set-hostname k8s-node02 \u0026\u0026 bash cat \u003c\u003cEOF\u003e\u003e /etc/hosts 192.168.5.3 k8s-master01 192.168.5.4 k8s-node01 192.168.5.5 k8s-node02 EOF scp /etc/hosts root@192.168.5.4:/etc/hosts scp /etc/hosts root@192.168.5.5:/etc/hosts ","date":"2023-08-04","objectID":"/kubernetes/:10:1","tags":["devops","kubernetes"],"title":"Kubernetes使用手册 (资料来源于B站“冰糖没糖”的分享)","uri":"/kubernetes/"},{"categories":["笔记"],"content":"2.6.2 安装依赖文件（所有节点都要操作） yum install -y conntrack ntpdate ntp ipvsadm ipset jq iptables curl sysstat libseccomp wget vim net-tools git ","date":"2023-08-04","objectID":"/kubernetes/:10:2","tags":["devops","kubernetes"],"title":"Kubernetes使用手册 (资料来源于B站“冰糖没糖”的分享)","uri":"/kubernetes/"},{"categories":["笔记"],"content":"2.6.3 设置防火墙为 Iptables 并设置空规则（所有节点都要操作） systemctl stop firewalld \u0026\u0026 systemctl disable firewalld yum -y install iptables-services \u0026\u0026 systemctl start iptables \u0026\u0026 systemctl enable iptables \u0026\u0026 iptables -F \u0026\u0026 service iptables save ","date":"2023-08-04","objectID":"/kubernetes/:10:3","tags":["devops","kubernetes"],"title":"Kubernetes使用手册 (资料来源于B站“冰糖没糖”的分享)","uri":"/kubernetes/"},{"categories":["笔记"],"content":"2.6.4 关闭 SELINUX（所有节点都要操作） swapoff -a \u0026\u0026 sed -i '/ swap / s/^\\(.*\\)$/#\\1/g' /etc/fstab setenforce 0 \u0026\u0026 sed -i 's/^SELINUX=.*/SELINUX=disabled/' /etc/selinux/config ","date":"2023-08-04","objectID":"/kubernetes/:10:4","tags":["devops","kubernetes"],"title":"Kubernetes使用手册 (资料来源于B站“冰糖没糖”的分享)","uri":"/kubernetes/"},{"categories":["笔记"],"content":"2.6.5 调整内核参数，对于 K8S（所有节点都要操作） modprobe br_netfilter cat \u003c\u003cEOF\u003e kubernetes.conf net.bridge.bridge-nf-call-iptables=1 net.bridge.bridge-nf-call-ip6tables=1 net.ipv4.ip_forward=1 net.ipv4.tcp_tw_recycle=0 vm.swappiness=0 # 禁止使用 swap 空间，只有当系统 OOM 时才允许使用它 vm.overcommit_memory=1 # 不检查物理内存是否够用 vm.panic_on_oom=0 # 开启 OOM fs.inotify.max_user_instances=8192 fs.inotify.max_user_watches=1048576 fs.file-max=52706963 fs.nr_open=52706963 net.ipv6.conf.all.disable_ipv6=1 net.netfilter.nf_conntrack_max=2310720 EOF cp kubernetes.conf /etc/sysctl.d/kubernetes.conf sysctl -p /etc/sysctl.d/kubernetes.conf ","date":"2023-08-04","objectID":"/kubernetes/:10:5","tags":["devops","kubernetes"],"title":"Kubernetes使用手册 (资料来源于B站“冰糖没糖”的分享)","uri":"/kubernetes/"},{"categories":["笔记"],"content":"2.6.6 调整系统时区（所有节点都要操作） # 设置系统时区为 中国/上海 timedatectl set-timezone Asia/Shanghai # 将当前的 UTC 时间写入硬件时钟 timedatectl set-local-rtc 0 # 重启依赖于系统时间的服务 systemctl restart rsyslog systemctl restart crond ","date":"2023-08-04","objectID":"/kubernetes/:10:6","tags":["devops","kubernetes"],"title":"Kubernetes使用手册 (资料来源于B站“冰糖没糖”的分享)","uri":"/kubernetes/"},{"categories":["笔记"],"content":"2.6.7 设置 rsyslogd 和 systemd journald（所有节点都要操作） # 持久化保存日志的目录 mkdir /var/log/journal mkdir /etc/systemd/journald.conf.d cat \u003e /etc/systemd/journald.conf.d/99-prophet.conf \u003c\u003cEOF [Journal] # 持久化保存到磁盘 Storage=persistent # 压缩历史日志 Compress=yes SyncIntervalSec=5m RateLimitInterval=30s RateLimitBurst=1000 # 最大占用空间 10G SystemMaxUse=10G # 单日志文件最大 200M SystemMaxFileSize=200M # 日志保存时间 2 周 MaxRetentionSec=2week # 不将日志转发到 syslog ForwardToSyslog=no EOF systemctl restart systemd-journald ","date":"2023-08-04","objectID":"/kubernetes/:10:7","tags":["devops","kubernetes"],"title":"Kubernetes使用手册 (资料来源于B站“冰糖没糖”的分享)","uri":"/kubernetes/"},{"categories":["笔记"],"content":"2.6.8 kube-proxy开启ipvs的前置条件（所有节点都要操作） cat \u003c\u003cEOF\u003e /etc/sysconfig/modules/ipvs.modules #!/bin/bash modprobe -- ip_vs modprobe -- ip_vs_rr modprobe -- ip_vs_wrr modprobe -- ip_vs_sh modprobe -- nf_conntrack_ipv4 EOF chmod 755 /etc/sysconfig/modules/ipvs.modules \u0026\u0026 bash /etc/sysconfig/modules/ipvs.modules \u0026\u0026 lsmod | grep -e ip_vs -e nf_conntrack_ipv4 ","date":"2023-08-04","objectID":"/kubernetes/:10:8","tags":["devops","kubernetes"],"title":"Kubernetes使用手册 (资料来源于B站“冰糖没糖”的分享)","uri":"/kubernetes/"},{"categories":["笔记"],"content":"2.6.9 安装 Docker 软件（所有节点都要操作） yum install -y yum-utils device-mapper-persistent-data lvm2 yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo yum install -y docker-ce ## 创建 /etc/docker 目录 mkdir /etc/docker cat \u003e /etc/docker/daemon.json \u003c\u003cEOF { \"exec-opts\": [\"native.cgroupdriver=systemd\"], \"log-driver\": \"json-file\", \"log-opts\": { \"max-size\": \"100m\" } } EOF mkdir -p /etc/systemd/system/docker.service.d # 重启docker服务 systemctl daemon-reload \u0026\u0026 systemctl restart docker \u0026\u0026 systemctl enable docker 上传文件到 /etc/yum.repos.d/目录下，也可以 代替 yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo 命令 docker-ce.repo [docker-ce-stable] name=Docker CE Stable - $basearch baseurl=https://mirrors.aliyun.com/docker-ce/linux/centos/$releasever/$basearch/stable enabled=1 gpgcheck=1 gpgkey=https://mirrors.aliyun.com/docker-ce/linux/centos/gpg [docker-ce-stable-debuginfo] name=Docker CE Stable - Debuginfo $basearch baseurl=https://mirrors.aliyun.com/docker-ce/linux/centos/$releasever/debug-$basearch/stable enabled=0 gpgcheck=1 gpgkey=https://mirrors.aliyun.com/docker-ce/linux/centos/gpg [docker-ce-stable-source] name=Docker CE Stable - Sources baseurl=https://mirrors.aliyun.com/docker-ce/linux/centos/$releasever/source/stable enabled=0 gpgcheck=1 gpgkey=https://mirrors.aliyun.com/docker-ce/linux/centos/gpg [docker-ce-test] name=Docker CE Test - $basearch baseurl=https://mirrors.aliyun.com/docker-ce/linux/centos/$releasever/$basearch/test enabled=0 gpgcheck=1 gpgkey=https://mirrors.aliyun.com/docker-ce/linux/centos/gpg [docker-ce-test-debuginfo] name=Docker CE Test - Debuginfo $basearch baseurl=https://mirrors.aliyun.com/docker-ce/linux/centos/$releasever/debug-$basearch/test enabled=0 gpgcheck=1 gpgkey=https://mirrors.aliyun.com/docker-ce/linux/centos/gpg [docker-ce-test-source] name=Docker CE Test - Sources baseurl=https://mirrors.aliyun.com/docker-ce/linux/centos/$releasever/source/test enabled=0 gpgcheck=1 gpgkey=https://mirrors.aliyun.com/docker-ce/linux/centos/gpg [docker-ce-nightly] name=Docker CE Nightly - $basearch baseurl=https://mirrors.aliyun.com/docker-ce/linux/centos/$releasever/$basearch/nightly enabled=0 gpgcheck=1 gpgkey=https://mirrors.aliyun.com/docker-ce/linux/centos/gpg [docker-ce-nightly-debuginfo] name=Docker CE Nightly - Debuginfo $basearch baseurl=https://mirrors.aliyun.com/docker-ce/linux/centos/$releasever/debug-$basearch/nightly enabled=0 gpgcheck=1 gpgkey=https://mirrors.aliyun.com/docker-ce/linux/centos/gpg [docker-ce-nightly-source] name=Docker CE Nightly - Sources baseurl=https://mirrors.aliyun.com/docker-ce/linux/centos/$releasever/source/nightly enabled=0 gpgcheck=1 gpgkey=https://mirrors.aliyun.com/docker-ce/linux/centos/gpg ","date":"2023-08-04","objectID":"/kubernetes/:10:9","tags":["devops","kubernetes"],"title":"Kubernetes使用手册 (资料来源于B站“冰糖没糖”的分享)","uri":"/kubernetes/"},{"categories":["笔记"],"content":"2.6.10 安装 Kubeadm （所有节点都要操作） cat \u003c\u003cEOF \u003e /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64 enabled=1 gpgcheck=0 repo_gpgcheck=0 gpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg http://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg EOF yum install -y kubelet kubeadm kubectl \u0026\u0026 systemctl enable kubelet ","date":"2023-08-04","objectID":"/kubernetes/:10:10","tags":["devops","kubernetes"],"title":"Kubernetes使用手册 (资料来源于B站“冰糖没糖”的分享)","uri":"/kubernetes/"},{"categories":["笔记"],"content":"2.7 部署Kubernetes Master ","date":"2023-08-04","objectID":"/kubernetes/:11:0","tags":["devops","kubernetes"],"title":"Kubernetes使用手册 (资料来源于B站“冰糖没糖”的分享)","uri":"/kubernetes/"},{"categories":["笔记"],"content":"2.7.1 初始化主节点（主节点操作） kubeadm init --apiserver-advertise-address=192.168.5.3 --image-repository registry.aliyuncs.com/google_containers --kubernetes-version v1.21.1 --service-cidr=10.96.0.0/12 --pod-network-cidr=10.244.0.0/16 mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config ","date":"2023-08-04","objectID":"/kubernetes/:11:1","tags":["devops","kubernetes"],"title":"Kubernetes使用手册 (资料来源于B站“冰糖没糖”的分享)","uri":"/kubernetes/"},{"categories":["笔记"],"content":"2.7.2 加入主节点以及其余工作节点 kubeadm join 192.168.5.3:6443 --token h0uelc.l46qp29nxscke7f7 \\ --discovery-token-ca-cert-hash sha256:abc807778e24bff73362ceeb783cc7f6feec96f20b4fd707c3f8e8312294e28f ","date":"2023-08-04","objectID":"/kubernetes/:11:2","tags":["devops","kubernetes"],"title":"Kubernetes使用手册 (资料来源于B站“冰糖没糖”的分享)","uri":"/kubernetes/"},{"categories":["笔记"],"content":"2.7.3 部署网络 kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml 下边是文件 --- apiVersion: policy/v1beta1 kind: PodSecurityPolicy metadata: name: psp.flannel.unprivileged annotations: seccomp.security.alpha.kubernetes.io/allowedProfileNames: docker/default seccomp.security.alpha.kubernetes.io/defaultProfileName: docker/default apparmor.security.beta.kubernetes.io/allowedProfileNames: runtime/default apparmor.security.beta.kubernetes.io/defaultProfileName: runtime/default spec: privileged: false volumes: - configMap - secret - emptyDir - hostPath allowedHostPaths: - pathPrefix: \"/etc/cni/net.d\" - pathPrefix: \"/etc/kube-flannel\" - pathPrefix: \"/run/flannel\" readOnlyRootFilesystem: false # Users and groups runAsUser: rule: RunAsAny supplementalGroups: rule: RunAsAny fsGroup: rule: RunAsAny # Privilege Escalation allowPrivilegeEscalation: false defaultAllowPrivilegeEscalation: false # Capabilities allowedCapabilities: ['NET_ADMIN', 'NET_RAW'] defaultAddCapabilities: [] requiredDropCapabilities: [] # Host namespaces hostPID: false hostIPC: false hostNetwork: true hostPorts: - min: 0 max: 65535 # SELinux seLinux: # SELinux is unused in CaaSP rule: 'RunAsAny' --- kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: name: flannel rules: - apiGroups: ['extensions'] resources: ['podsecuritypolicies'] verbs: ['use'] resourceNames: ['psp.flannel.unprivileged'] - apiGroups: - \"\" resources: - pods verbs: - get - apiGroups: - \"\" resources: - nodes verbs: - list - watch - apiGroups: - \"\" resources: - nodes/status verbs: - patch --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: flannel roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: flannel subjects: - kind: ServiceAccount name: flannel namespace: kube-system --- apiVersion: v1 kind: ServiceAccount metadata: name: flannel namespace: kube-system --- kind: ConfigMap apiVersion: v1 metadata: name: kube-flannel-cfg namespace: kube-system labels: tier: node app: flannel data: cni-conf.json: | { \"name\": \"cbr0\", \"cniVersion\": \"0.3.1\", \"plugins\": [ { \"type\": \"flannel\", \"delegate\": { \"hairpinMode\": true, \"isDefaultGateway\": true } }, { \"type\": \"portmap\", \"capabilities\": { \"portMappings\": true } } ] } net-conf.json: | { \"Network\": \"10.244.0.0/16\", \"Backend\": { \"Type\": \"vxlan\" } } --- apiVersion: apps/v1 kind: DaemonSet metadata: name: kube-flannel-ds namespace: kube-system labels: tier: node app: flannel spec: selector: matchLabels: app: flannel template: metadata: labels: tier: node app: flannel spec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/os operator: In values: - linux hostNetwork: true priorityClassName: system-node-critical tolerations: - operator: Exists effect: NoSchedule serviceAccountName: flannel initContainers: - name: install-cni image: quay.io/coreos/flannel:v0.14.0 command: - cp args: - -f - /etc/kube-flannel/cni-conf.json - /etc/cni/net.d/10-flannel.conflist volumeMounts: - name: cni mountPath: /etc/cni/net.d - name: flannel-cfg mountPath: /etc/kube-flannel/ containers: - name: kube-flannel image: quay.io/coreos/flannel:v0.14.0 command: - /opt/bin/flanneld args: - --ip-masq - --kube-subnet-mgr resources: requests: cpu: \"100m\" memory: \"50Mi\" limits: cpu: \"100m\" memory: \"50Mi\" securityContext: privileged: false capabilities: add: [\"NET_ADMIN\", \"NET_RAW\"] env: - name: POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace volumeMounts: - name: run mountPath: /run/flannel - name: flannel-cfg mountPath: /etc/kube-flannel/ volumes: - name: run hostPath: path: /run/flannel - name: cni hostPath: path: /etc/cni/net.d - name: flannel-cfg configMap: name: kube-flannel-cfg ","date":"2023-08-04","objectID":"/kubernetes/:11:3","tags":["devops","kubernetes"],"title":"Kubernetes使用手册 (资料来源于B站“冰糖没糖”的分享)","uri":"/kubernetes/"},{"categories":["笔记"],"content":"2.8 测试kubernetes 集群 ","date":"2023-08-04","objectID":"/kubernetes/:12:0","tags":["devops","kubernetes"],"title":"Kubernetes使用手册 (资料来源于B站“冰糖没糖”的分享)","uri":"/kubernetes/"},{"categories":["笔记"],"content":"2.8.1 部署nginx 测试 kubectl create deployment nginx --image=nginx kubectl expose deployment nginx --port=80 --type=NodePort kubectl get pod,svc 3. 资源管理 ","date":"2023-08-04","objectID":"/kubernetes/:12:1","tags":["devops","kubernetes"],"title":"Kubernetes使用手册 (资料来源于B站“冰糖没糖”的分享)","uri":"/kubernetes/"},{"categories":["笔记"],"content":"3.1 资源管理介绍 在kubernetes中，所有的内容都抽象为资源，用户需要通过操作资源来管理kubernetes。 kubernetes的本质上就是一个集群系统，用户可以在集群中部署各种服务，所谓的部署服务，其实就是在kubernetes集群中运行一个个的容器，并将指定的程序跑在容器中。 kubernetes的最小管理单元是pod而不是容器，所以只能将容器放在 Pod中，而kubernetes一般也不会直接管理Pod，而是通过 Pod控制器来管理Pod的。 Pod可以提供服务之后，就要考虑如何访问Pod中服务，kubernetes提供了 Service资源实现这个功能。 当然，如果Pod中程序的数据需要持久化，kubernetes还提供了各种 存储系统。 学习kubernetes的核心，就是学习如何对集群上的 Pod、Pod控制器、Service、存储等各种资源进行操作 ","date":"2023-08-04","objectID":"/kubernetes/:13:0","tags":["devops","kubernetes"],"title":"Kubernetes使用手册 (资料来源于B站“冰糖没糖”的分享)","uri":"/kubernetes/"},{"categories":["笔记"],"content":"3.2 YAML语言介绍 YAML是一个类似 XML、JSON 的标记性语言。它强调以数据为中心，并不是以标识语言为重点。因而YAML本身的定义比较简单，号称\"一种人性化的数据格式语言\"。 \u003cheima\u003e \u003cage\u003e15\u003c/age\u003e \u003caddress\u003eBeijing\u003c/address\u003e \u003c/heima\u003e heima: age: 15 address: Beijing YAML的语法比较简单，主要有下面几个： 大小写敏感 使用缩进表示层级关系 缩进不允许使用tab，只允许空格( 低版本限制 ) 缩进的空格数不重要，只要相同层级的元素左对齐即可 ‘#‘表示注释 YAML支持以下几种数据类型： 纯量：单个的、不可再分的值 对象：键值对的集合，又称为映射（mapping）/ 哈希（hash） / 字典（dictionary） 数组：一组按次序排列的值，又称为序列（sequence） / 列表（list） # 纯量, 就是指的一个简单的值，字符串、布尔值、整数、浮点数、Null、时间、日期 # 1 布尔类型 c1: true (或者True) # 2 整型 c2: 234 # 3 浮点型 c3: 3.14 # 4 null类型 c4: ~ # 使用~表示null # 5 日期类型 c5: 2018-02-17 # 日期必须使用ISO 8601格式，即yyyy-MM-dd # 6 时间类型 c6: 2018-02-17T15:02:31+08:00 # 时间使用ISO 8601格式，时间和日期之间使用T连接，最后使用+代表时区 # 7 字符串类型 c7: heima # 简单写法，直接写值 , 如果字符串中间有特殊字符，必须使用双引号或者单引号包裹 c8: line1 line2 # 字符串过多的情况可以拆成多行，每一行会被转化成一个空格 # 对象 # 形式一(推荐): heima: age: 15 address: Beijing # 形式二(了解): heima: {age: 15,address: Beijing} # 数组 # 形式一(推荐): address: - 顺义 - 昌平 # 形式二(了解): address: [顺义,昌平] 小提示： 1 书写yaml切记 : 后面要加一个空格 2 如果需要将多段yaml配置放在一个文件中，中间要使用 ---分隔 3 下面是一个yaml转json的网站，可以通过它验证yaml是否书写正确 https://www.json2yaml.com/convert-yaml-to-json ","date":"2023-08-04","objectID":"/kubernetes/:14:0","tags":["devops","kubernetes"],"title":"Kubernetes使用手册 (资料来源于B站“冰糖没糖”的分享)","uri":"/kubernetes/"},{"categories":["笔记"],"content":"3.3 资源管理方式 命令式对象管理：直接使用命令去操作kubernetes资源 kubectl run nginx-pod --image=nginx:1.17.1 --port=80 命令式对象配置：通过命令配置和配置文件去操作kubernetes资源 kubectl create/patch -f nginx-pod.yaml 声明式对象配置：通过apply命令和配置文件去操作kubernetes资源 kubectl apply -f nginx-pod.yaml 类型 操作对象 适用环境 优点 缺点 命令式对象管理 对象 测试 简单 只能操作活动对象，无法审计、跟踪 命令式对象配置 文件 开发 可以审计、跟踪 项目大时，配置文件多，操作麻烦 声明式对象配置 目录 开发 支持目录操作 意外情况下难以调试 ","date":"2023-08-04","objectID":"/kubernetes/:15:0","tags":["devops","kubernetes"],"title":"Kubernetes使用手册 (资料来源于B站“冰糖没糖”的分享)","uri":"/kubernetes/"},{"categories":["笔记"],"content":"3.3.1 命令式对象管理 kubectl命令 kubectl是kubernetes集群的命令行工具，通过它能够对集群本身进行管理，并能够在集群上进行容器化应用的安装部署。kubectl命令的语法如下： kubectl [command] [type] [name] [flags] comand：指定要对资源执行的操作，例如create、get、delete type：指定资源类型，比如deployment、pod、service name：指定资源的名称，名称大小写敏感 flags：指定额外的可选参数 # 查看所有pod kubectl get pod # 查看某个pod kubectl get pod pod_name # 查看某个pod,以yaml格式展示结果 kubectl get pod pod_name -o yaml 资源类型 kubernetes中所有的内容都抽象为资源，可以通过下面的命令进行查看: kubectl api-resources 经常使用的资源有下面这些： 资源分类 资源名称 缩写 资源作用 集群级别资源 nodes no 集群组成部分 namespaces ns 隔离Pod pod资源 pods po 装载容器 pod资源控制器 replicationcontrollers rc 控制pod资源 replicasets rs 控制pod资源 deployments deploy 控制pod资源 daemonsets ds 控制pod资源 jobs 控制pod资源 cronjobs cj 控制pod资源 horizontalpodautoscalers hpa 控制pod资源 statefulsets sts 控制pod资源 服务发现资源 services svc 统一pod对外接口 ingress ing 统一pod对外接口 存储资源 volumeattachments 存储 persistentvolumes pv 存储 persistentvolumeclaims pvc 存储 配置资源 configmaps cm 配置 secrets 配置 操作 kubernetes允许对资源进行多种操作，可以通过–help查看详细的操作命令 kubectl --help 经常使用的操作有下面这些： 命令分类 命令 翻译 命令作用 基本命令 create 创建 创建一个资源 edit 编辑 编辑一个资源 get 获取 获取一个资源 patch 更新 更新一个资源 delete 删除 删除一个资源 explain 解释 展示资源文档 运行和调试 run 运行 在集群中运行一个指定的镜像 expose 暴露 暴露资源为Service describe 描述 显示资源内部信息 logs 日志输出容器在 pod 中的日志 输出容器在 pod 中的日志 attach 缠绕进入运行中的容器 进入运行中的容器 exec 执行容器中的一个命令 执行容器中的一个命令 cp 复制 在Pod内外复制文件 rollout 首次展示 管理资源的发布 scale 规模 扩(缩)容Pod的数量 autoscale 自动调整 自动调整Pod的数量 高级命令 apply rc 通过文件对资源进行配置 label 标签 更新资源上的标签 其他命令 cluster-info 集群信息 显示集群信息 version 版本 显示当前Server和Client的版本 下面以一个namespace / pod的创建和删除简单演示下命令的使用： # 创建一个namespace [root@master ~]# kubectl create namespace dev namespace/dev created # 获取namespace [root@master ~]# kubectl get ns NAME STATUS AGE default Active 21h dev Active 21s kube-node-lease Active 21h kube-public Active 21h kube-system Active 21h # 在此namespace下创建并运行一个nginx的Pod [root@master ~]# kubectl run pod --image=nginx:latest -n dev kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead. deployment.apps/pod created # 查看新创建的pod [root@master ~]# kubectl get pod -n dev NAME READY STATUS RESTARTS AGE pod 1/1 Running 0 21s # 删除指定的pod [root@master ~]# kubectl delete pod pod-864f9875b9-pcw7x pod \"pod\" deleted # 删除指定的namespace [root@master ~]# kubectl delete ns dev namespace \"dev\" deleted ","date":"2023-08-04","objectID":"/kubernetes/:15:1","tags":["devops","kubernetes"],"title":"Kubernetes使用手册 (资料来源于B站“冰糖没糖”的分享)","uri":"/kubernetes/"},{"categories":["笔记"],"content":"3.3.2 命令式对象配置 命令式对象配置就是使用命令配合配置文件一起来操作kubernetes资源。 1） 创建一个nginxpod.yaml，内容如下： apiVersion: v1 kind: Namespace metadata: name: dev --- apiVersion: v1 kind: Pod metadata: name: nginxpod namespace: dev spec: containers: - name: nginx-containers image: nginx:latest 2）执行create命令，创建资源： [root@master ~]# kubectl create -f nginxpod.yaml namespace/dev created pod/nginxpod created 此时发现创建了两个资源对象，分别是namespace和pod 3）执行get命令，查看资源： [root@master ~]# kubectl get -f nginxpod.yaml NAME STATUS AGE namespace/dev Active 18s NAME READY STATUS RESTARTS AGE pod/nginxpod 1/1 Running 0 17s 这样就显示了两个资源对象的信息 4）执行delete命令，删除资源： [root@master ~]# kubectl delete -f nginxpod.yaml namespace \"dev\" deleted pod \"nginxpod\" deleted 此时发现两个资源对象被删除了 总结: 命令式对象配置的方式操作资源，可以简单的认为：命令 + yaml配置文件（里面是命令需要的各种参数） ","date":"2023-08-04","objectID":"/kubernetes/:15:2","tags":["devops","kubernetes"],"title":"Kubernetes使用手册 (资料来源于B站“冰糖没糖”的分享)","uri":"/kubernetes/"},{"categories":["笔记"],"content":"3.3.3 声明式对象配置 声明式对象配置跟命令式对象配置很相似，但是它只有一个命令apply。 # 首先执行一次kubectl apply -f yaml文件，发现创建了资源 [root@master ~]# kubectl apply -f nginxpod.yaml namespace/dev created pod/nginxpod created # 再次执行一次kubectl apply -f yaml文件，发现说资源没有变动 [root@master ~]# kubectl apply -f nginxpod.yaml namespace/dev unchanged pod/nginxpod unchanged 总结: 其实声明式对象配置就是使用apply描述一个资源最终的状态（在yaml中定义状态） 使用apply操作资源： 如果资源不存在，就创建，相当于 kubectl create 如果资源已存在，就更新，相当于 kubectl patch 扩展：kubectl可以在node节点上运行吗 ? kubectl的运行是需要进行配置的，它的配置文件是$HOME/.kube，如果想要在node节点运行此命令，需要将master上的.kube文件复制到node节点上，即在master节点上执行下面操作： scp -r HOME/.kube node1: HOME/ 使用推荐: 三种方式应该怎么用 ? 创建/更新资源 使用声明式对象配置 kubectl apply -f XXX.yaml 删除资源 使用命令式对象配置 kubectl delete -f XXX.yaml 查询资源 使用命令式对象管理 kubectl get(describe) 资源名称 4. 实战入门 本章节将介绍如何在kubernetes集群中部署一个nginx服务，并且能够对其进行访问。 ","date":"2023-08-04","objectID":"/kubernetes/:15:3","tags":["devops","kubernetes"],"title":"Kubernetes使用手册 (资料来源于B站“冰糖没糖”的分享)","uri":"/kubernetes/"},{"categories":["笔记"],"content":"4.1 Namespace Namespace是kubernetes系统中的一种非常重要资源，它的主要作用是用来实现多套环境的资源隔离或者多租户的资源隔离。 默认情况下，kubernetes集群中的所有的Pod都是可以相互访问的。但是在实际中，可能不想让两个Pod之间进行互相的访问，那此时就可以将两个Pod划分到不同的namespace下。kubernetes通过将集群内部的资源分配到不同的Namespace中，可以形成逻辑上的\"组\"，以方便不同的组的资源进行隔离使用和管理。 可以通过kubernetes的授权机制，将不同的namespace交给不同租户进行管理，这样就实现了多租户的资源隔离。此时还能结合kubernetes的资源配额机制，限定不同租户能占用的资源，例如CPU使用量、内存使用量等等，来实现租户可用资源的管理。 kubernetes在集群启动之后，会默认创建几个namespace [root@master ~]# kubectl get namespace NAME STATUS AGE default Active 45h # 所有未指定Namespace的对象都会被分配在default命名空间 kube-node-lease Active 45h # 集群节点之间的心跳维护，v1.13开始引入 kube-public Active 45h # 此命名空间下的资源可以被所有人访问（包括未认证用户） kube-system Active 45h # 所有由Kubernetes系统创建的资源都处于这个命名空间 下面来看namespace资源的具体操作： 查看 # 1 查看所有的ns 命令：kubectl get ns [root@master ~]# kubectl get ns NAME STATUS AGE default Active 45h kube-node-lease Active 45h kube-public Active 45h kube-system Active 45h # 2 查看指定的ns 命令：kubectl get ns ns名称 [root@master ~]# kubectl get ns default NAME STATUS AGE default Active 45h # 3 指定输出格式 命令：kubectl get ns ns名称 -o 格式参数 # kubernetes支持的格式有很多，比较常见的是wide、json、yaml [root@master ~]# kubectl get ns default -o yaml apiVersion: v1 kind: Namespace metadata: creationTimestamp: \"2021-05-08T04:44:16Z\" name: default resourceVersion: \"151\" selfLink: /api/v1/namespaces/default uid: 7405f73a-e486-43d4-9db6-145f1409f090 spec: finalizers: - kubernetes status: phase: Active # 4 查看ns详情 命令：kubectl describe ns ns名称 [root@master ~]# kubectl describe ns default Name: default Labels: \u003cnone\u003e Annotations: \u003cnone\u003e Status: Active # Active 命名空间正在使用中 Terminating 正在删除命名空间 # ResourceQuota 针对namespace做的资源限制 # LimitRange针对namespace中的每个组件做的资源限制 No resource quota. No LimitRange resource. 创建 # 创建namespace [root@master ~]# kubectl create ns dev namespace/dev created 删除 # 删除namespace [root@master ~]# kubectl delete ns dev namespace \"dev\" deleted 配置方式 首先准备一个yaml文件：ns-dev.yaml apiVersion: v1 kind: Namespace metadata: name: dev 然后就可以执行对应的创建和删除命令了： 创建：kubectl create -f ns-dev.yaml 删除：kubectl delete -f ns-dev.yaml ","date":"2023-08-04","objectID":"/kubernetes/:16:0","tags":["devops","kubernetes"],"title":"Kubernetes使用手册 (资料来源于B站“冰糖没糖”的分享)","uri":"/kubernetes/"},{"categories":["笔记"],"content":"4.2 Pod Pod是kubernetes集群进行管理的最小单元，程序要运行必须部署在容器中，而容器必须存在于Pod中。 Pod可以认为是容器的封装，一个Pod中可以存在一个或者多个容器。 kubernetes在集群启动之后，集群中的各个组件也都是以Pod方式运行的。可以通过下面命令查看： [root@master ~]# kubectl get pod -n kube-system NAMESPACE NAME READY STATUS RESTARTS AGE kube-system coredns-6955765f44-68g6v 1/1 Running 0 2d1h kube-system coredns-6955765f44-cs5r8 1/1 Running 0 2d1h kube-system etcd-master 1/1 Running 0 2d1h kube-system kube-apiserver-master 1/1 Running 0 2d1h kube-system kube-controller-manager-master 1/1 Running 0 2d1h kube-system kube-flannel-ds-amd64-47r25 1/1 Running 0 2d1h kube-system kube-flannel-ds-amd64-ls5lh 1/1 Running 0 2d1h kube-system kube-proxy-685tk 1/1 Running 0 2d1h kube-system kube-proxy-87spt 1/1 Running 0 2d1h kube-system kube-scheduler-master 1/1 Running 0 2d1h 创建并运行 kubernetes没有提供单独运行Pod的命令，都是通过Pod控制器来实现的 # 命令格式： kubectl run (pod控制器名称) [参数] # --image 指定Pod的镜像 # --port 指定端口 # --namespace 指定namespace [root@master ~]# kubectl run nginx --image=nginx:latest --port=80 --namespace dev deployment.apps/nginx created 查看pod信息 # 查看Pod基本信息 [root@master ~]# kubectl get pods -n dev NAME READY STATUS RESTARTS AGE nginx 1/1 Running 0 43s # 查看Pod的详细信息 [root@master ~]# kubectl describe pod nginx -n dev Name: nginx Namespace: dev Priority: 0 Node: node1/192.168.5.4 Start Time: Wed, 08 May 2021 09:29:24 +0800 Labels: pod-template-hash=5ff7956ff6 run=nginx Annotations: \u003cnone\u003e Status: Running IP: 10.244.1.23 IPs: IP: 10.244.1.23 Controlled By: ReplicaSet/nginx Containers: nginx: Container ID: docker://4c62b8c0648d2512380f4ffa5da2c99d16e05634979973449c98e9b829f6253c Image: nginx:latest Image ID: docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 Port: 80/TCP Host Port: 0/TCP State: Running Started: Wed, 08 May 2021 09:30:01 +0800 Ready: True Restart Count: 0 Environment: \u003cnone\u003e Mounts: /var/run/secrets/kubernetes.io/serviceaccount from default-token-hwvvw (ro) Conditions: Type Status Initialized True Ready True ContainersReady True PodScheduled True Volumes: default-token-hwvvw: Type: Secret (a volume populated by a Secret) SecretName: default-token-hwvvw Optional: false QoS Class: BestEffort Node-Selectors: \u003cnone\u003e Tolerations: node.kubernetes.io/not-ready:NoExecute for 300s node.kubernetes.io/unreachable:NoExecute for 300s Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled \u003cunknown\u003e default-scheduler Successfully assigned dev/nginx-5ff7956ff6-fg2db to node1 Normal Pulling 4m11s kubelet, node1 Pulling image \"nginx:latest\" Normal Pulled 3m36s kubelet, node1 Successfully pulled image \"nginx:latest\" Normal Created 3m36s kubelet, node1 Created container nginx Normal Started 3m36s kubelet, node1 Started container nginx 访问Pod # 获取podIP [root@master ~]# kubectl get pods -n dev -o wide NAME READY STATUS RESTARTS AGE IP NODE ... nginx 1/1 Running 0 190s 10.244.1.23 node1 ... #访问POD [root@master ~]# curl http://10.244.1.23:80 \u003c!DOCTYPE html\u003e \u003chtml\u003e \u003chead\u003e \u003ctitle\u003eWelcome to nginx!\u003c/title\u003e \u003c/head\u003e \u003cbody\u003e \u003cp\u003e\u003cem\u003eThank you for using nginx.\u003c/em\u003e\u003c/p\u003e \u003c/body\u003e \u003c/html\u003e 删除指定Pod # 删除指定Pod [root@master ~]# kubectl delete pod nginx -n dev pod \"nginx\" deleted # 此时，显示删除Pod成功，但是再查询，发现又新产生了一个 [root@master ~]# kubectl get pods -n dev NAME READY STATUS RESTARTS AGE nginx 1/1 Running 0 21s # 这是因为当前Pod是由Pod控制器创建的，控制器会监控Pod状况，一旦发现Pod死亡，会立即重建 # 此时要想删除Pod，必须删除Pod控制器 # 先来查询一下当前namespace下的Pod控制器 [root@master ~]# kubectl get deploy -n dev NAME READY UP-TO-DATE AVAILABLE AGE nginx 1/1 1 1 9m7s # 接下来，删除此PodPod控制器 [root@master ~]# kubectl delete deploy nginx -n dev deployment.apps \"nginx\" deleted # 稍等片刻，再查询Pod，发现Pod被删除了 [root@master ~]# kubectl get pods -n dev No resources found in dev namespace. 配置操作 创建一个pod-nginx.yaml，内容如下： apiVersion: v1 kind: Pod metadata: name: nginx namespace: dev spec: containers: - image: nginx:latest name: pod ports: - name: nginx-port containerPort: 80 protocol: TCP 然后就可以执行对应的创建和删除命令了： 创建：kubectl create -f pod-nginx.yaml 删除：kubectl delete -f pod-nginx.yaml ","date":"2023-08-04","objectID":"/kubernetes/:17:0","tags":["devops","kubernetes"],"title":"Kubernetes使用手册 (资料来源于B站“冰糖没糖”的分享)","uri":"/kubernetes/"},{"categories":["笔记"],"content":"4.3 Label Label是kubernetes系统中的一个重要概念。它的作用就是在资源上添加标识，用来对它们进行区分和选择。 Label的特点： 一个Label会以key/value键值对的形式附加到各种对象上，如Node、Pod、Service等等 一个资源对象可以定义任意数量的Label ，同一个Label也可以被添加到任意数量的资源对象上去 Label通常在资源对象定义时确定，当然也可以在对象创建后动态添加或者删除 可以通过Label实现资源的多维度分组，以便灵活、方便地进行资源分配、调度、配置、部署等管理工作。 一些常用的Label 示例如下： 版本标签：“version”:“release”, “version”:“stable”…… 环境标签：“environment”:“dev”，“environment”:“test”，“environment”:“pro” 架构标签：“tier”:“frontend”，“tier”:“backend” 标签定义完毕之后，还要考虑到标签的选择，这就要使用到Label Selector，即： Label用于给某个资源对象定义标识 Label Selector用于查询和筛选拥有某些标签的资源对象 当前有两种Label Selector： 基于等式的Label Selector name = slave: 选择所有包含Label中key=“name\"且value=“slave\"的对象 env != production: 选择所有包括Label中的key=“env\"且value不等于\"production\"的对象 基于集合的Label Selector name in (master, slave): 选择所有包含Label中的key=“name\"且value=“master\"或\"slave\"的对象 name not in (frontend): 选择所有包含Label中的key=“name\"且value不等于\"frontend\"的对象 标签的选择条件可以使用多个，此时将多个Label Selector进行组合，使用逗号”,“进行分隔即可。例如： name=slave，env!=production name not in (frontend)，env!=production 命令方式 # 为pod资源打标签 [root@master ~]# kubectl label pod nginx-pod version=1.0 -n dev pod/nginx-pod labeled # 为pod资源更新标签 [root@master ~]# kubectl label pod nginx-pod version=2.0 -n dev --overwrite pod/nginx-pod labeled # 查看标签 [root@master ~]# kubectl get pod nginx-pod -n dev --show-labels NAME READY STATUS RESTARTS AGE LABELS nginx-pod 1/1 Running 0 10m version=2.0 # 筛选标签 [root@master ~]# kubectl get pod -n dev -l version=2.0 --show-labels NAME READY STATUS RESTARTS AGE LABELS nginx-pod 1/1 Running 0 17m version=2.0 [root@master ~]# kubectl get pod -n dev -l version!=2.0 --show-labels No resources found in dev namespace. #删除标签 [root@master ~]# kubectl label pod nginx-pod version- -n dev pod/nginx-pod labeled 配置方式 apiVersion: v1 kind: Pod metadata: name: nginx namespace: dev labels: version: \"3.0\" env: \"test\" spec: containers: - image: nginx:latest name: pod ports: - name: nginx-port containerPort: 80 protocol: TCP 然后就可以执行对应的更新命令了：kubectl apply -f pod-nginx.yaml ","date":"2023-08-04","objectID":"/kubernetes/:18:0","tags":["devops","kubernetes"],"title":"Kubernetes使用手册 (资料来源于B站“冰糖没糖”的分享)","uri":"/kubernetes/"},{"categories":["笔记"],"content":"4.4 Deployment 在kubernetes中，Pod是最小的控制单元，但是kubernetes很少直接控制Pod，一般都是通过Pod控制器来完成的。Pod控制器用于pod的管理，确保pod资源符合预期的状态，当pod的资源出现故障时，会尝试进行重启或重建pod。 在kubernetes中Pod控制器的种类有很多，本章节只介绍一种：Deployment。 命令操作 # 命令格式: kubectl create deployment 名称 [参数] # --image 指定pod的镜像 # --port 指定端口 # --replicas 指定创建pod数量 # --namespace 指定namespace [root@master ~]# kubectl create deploy nginx --image=nginx:latest --port=80 --replicas=3 -n dev deployment.apps/nginx created # 查看创建的Pod [root@master ~]# kubectl get pods -n dev NAME READY STATUS RESTARTS AGE nginx-5ff7956ff6-6k8cb 1/1 Running 0 19s nginx-5ff7956ff6-jxfjt 1/1 Running 0 19s nginx-5ff7956ff6-v6jqw 1/1 Running 0 19s # 查看deployment的信息 [root@master ~]# kubectl get deploy -n dev NAME READY UP-TO-DATE AVAILABLE AGE nginx 3/3 3 3 2m42s # UP-TO-DATE：成功升级的副本数量 # AVAILABLE：可用副本的数量 [root@master ~]# kubectl get deploy -n dev -o wide NAME READY UP-TO-DATE AVAILABLE AGE CONTAINERS IMAGES SELECTOR nginx 3/3 3 3 2m51s nginx nginx:latest run=nginx # 查看deployment的详细信息 [root@master ~]# kubectl describe deploy nginx -n dev Name: nginx Namespace: dev CreationTimestamp: Wed, 08 May 2021 11:14:14 +0800 Labels: run=nginx Annotations: deployment.kubernetes.io/revision: 1 Selector: run=nginx Replicas: 3 desired | 3 updated | 3 total | 3 available | 0 unavailable StrategyType: RollingUpdate MinReadySeconds: 0 RollingUpdateStrategy: 25% max unavailable, 25% max surge Pod Template: Labels: run=nginx Containers: nginx: Image: nginx:latest Port: 80/TCP Host Port: 0/TCP Environment: \u003cnone\u003e Mounts: \u003cnone\u003e Volumes: \u003cnone\u003e Conditions: Type Status Reason ---- ------ ------ Available True MinimumReplicasAvailable Progressing True NewReplicaSetAvailable OldReplicaSets: \u003cnone\u003e NewReplicaSet: nginx-5ff7956ff6 (3/3 replicas created) Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal ScalingReplicaSet 5m43s deployment-controller Scaled up replicaset nginx-5ff7956ff6 to 3 # 删除 [root@master ~]# kubectl delete deploy nginx -n dev deployment.apps \"nginx\" deleted 配置操作 创建一个deploy-nginx.yaml，内容如下： apiVersion: apps/v1 kind: Deployment metadata: name: nginx namespace: dev spec: replicas: 3 selector: matchLabels: run: nginx template: metadata: labels: run: nginx spec: containers: - image: nginx:latest name: nginx ports: - containerPort: 80 protocol: TCP 然后就可以执行对应的创建和删除命令了： 创建：kubectl create -f deploy-nginx.yaml 删除：kubectl delete -f deploy-nginx.yaml ","date":"2023-08-04","objectID":"/kubernetes/:19:0","tags":["devops","kubernetes"],"title":"Kubernetes使用手册 (资料来源于B站“冰糖没糖”的分享)","uri":"/kubernetes/"},{"categories":["笔记"],"content":"4.5 Service 通过上节课的学习，已经能够利用Deployment来创建一组Pod来提供具有高可用性的服务。 虽然每个Pod都会分配一个单独的Pod IP，然而却存在如下两问题： Pod IP 会随着Pod的重建产生变化 Pod IP 仅仅是集群内可见的虚拟IP，外部无法访问 这样对于访问这个服务带来了难度。因此，kubernetes设计了Service来解决这个问题。 Service可以看作是一组同类Pod对外的访问接口。借助Service，应用可以方便地实现服务发现和负载均衡。 操作一：创建集群内部可访问的Service # 暴露Service [root@master ~]# kubectl expose deploy nginx --name=svc-nginx1 --type=ClusterIP --port=80 --target-port=80 -n dev service/svc-nginx1 exposed # 查看service [root@master ~]# kubectl get svc svc-nginx1 -n dev -o wide NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR svc-nginx1 ClusterIP 10.109.179.231 \u003cnone\u003e 80/TCP 3m51s run=nginx # 这里产生了一个CLUSTER-IP，这就是service的IP，在Service的生命周期中，这个地址是不会变动的 # 可以通过这个IP访问当前service对应的POD [root@master ~]# curl 10.109.179.231:80 \u003c!DOCTYPE html\u003e \u003chtml\u003e \u003chead\u003e \u003ctitle\u003eWelcome to nginx!\u003c/title\u003e \u003c/head\u003e \u003cbody\u003e \u003ch1\u003eWelcome to nginx!\u003c/h1\u003e ....... \u003c/body\u003e \u003c/html\u003e 操作二：创建集群外部也可访问的Service # 上面创建的Service的type类型为ClusterIP，这个ip地址只用集群内部可访问 # 如果需要创建外部也可以访问的Service，需要修改type为NodePort [root@master ~]# kubectl expose deploy nginx --name=svc-nginx2 --type=NodePort --port=80 --target-port=80 -n dev service/svc-nginx2 exposed # 此时查看，会发现出现了NodePort类型的Service，而且有一对Port（80:31928/TC） [root@master ~]# kubectl get svc svc-nginx2 -n dev -o wide NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR svc-nginx2 NodePort 10.100.94.0 \u003cnone\u003e 80:31928/TCP 9s run=nginx # 接下来就可以通过集群外的主机访问 节点IP:31928访问服务了 # 例如在的电脑主机上通过浏览器访问下面的地址 http://192.168.5.4:31928/ 删除Service [root@master ~]# kubectl delete svc svc-nginx-1 -n dev service \"svc-nginx-1\" deleted 配置方式 创建一个svc-nginx.yaml，内容如下： apiVersion: v1 kind: Service metadata: name: svc-nginx namespace: dev spec: clusterIP: 10.109.179.231 #固定svc的内网ip ports: - port: 80 protocol: TCP targetPort: 80 selector: run: nginx type: ClusterIP 然后就可以执行对应的创建和删除命令了： 创建：kubectl create -f svc-nginx.yaml 删除：kubectl delete -f svc-nginx.yaml 小结 至此，已经掌握了Namespace、Pod、Deployment、Service资源的基本操作，有了这些操作，就可以在kubernetes集群中实现一个服务的简单部署和访问了，但是如果想要更好的使用kubernetes，就需要深入学习这几种资源的细节和原理。 5. Pod详解 ","date":"2023-08-04","objectID":"/kubernetes/:20:0","tags":["devops","kubernetes"],"title":"Kubernetes使用手册 (资料来源于B站“冰糖没糖”的分享)","uri":"/kubernetes/"},{"categories":["笔记"],"content":"5.1 Pod介绍 ","date":"2023-08-04","objectID":"/kubernetes/:21:0","tags":["devops","kubernetes"],"title":"Kubernetes使用手册 (资料来源于B站“冰糖没糖”的分享)","uri":"/kubernetes/"},{"categories":["笔记"],"content":"5.1.1 Pod结构 每个Pod中都可以包含一个或者多个容器，这些容器可以分为两类： 用户程序所在的容器，数量可多可少 Pause容器，这是每个Pod都会有的一个根容器，它的作用有两个： 可以以它为依据，评估整个Pod的健康状态 可以在根容器上设置Ip地址，其它容器都此Ip（Pod IP），以实现Pod内部的网路通信 这里是Pod内部的通讯，Pod的之间的通讯采用虚拟二层网络技术来实现，我们当前环境用的是Flannel ","date":"2023-08-04","objectID":"/kubernetes/:21:1","tags":["devops","kubernetes"],"title":"Kubernetes使用手册 (资料来源于B站“冰糖没糖”的分享)","uri":"/kubernetes/"},{"categories":["笔记"],"content":"5.1.2 Pod定义 下面是Pod的资源清单： apiVersion: v1 #必选，版本号，例如v1 kind: Pod #必选，资源类型，例如 Pod metadata: #必选，元数据 name: string #必选，Pod名称 namespace: string #Pod所属的命名空间,默认为\"default\" labels: #自定义标签列表 - name: string spec: #必选，Pod中容器的详细定义 containers: #必选，Pod中容器列表 - name: string #必选，容器名称 image: string #必选，容器的镜像名称 imagePullPolicy: [ Always|Never|IfNotPresent ] #获取镜像的策略 command: [string] #容器的启动命令列表，如不指定，使用打包时使用的启动命令 args: [string] #容器的启动命令参数列表 workingDir: string #容器的工作目录 volumeMounts: #挂载到容器内部的存储卷配置 - name: string #引用pod定义的共享存储卷的名称，需用volumes[]部分定义的的卷名 mountPath: string #存储卷在容器内mount的绝对路径，应少于512字符 readOnly: boolean #是否为只读模式 ports: #需要暴露的端口库号列表 - name: string #端口的名称 containerPort: int #容器需要监听的端口号 hostPort: int #容器所在主机需要监听的端口号，默认与Container相同 protocol: string #端口协议，支持TCP和UDP，默认TCP env: #容器运行前需设置的环境变量列表 - name: string #环境变量名称 value: string #环境变量的值 resources: #资源限制和请求的设置 limits: #资源限制的设置 cpu: string #Cpu的限制，单位为core数，将用于docker run --cpu-shares参数 memory: string #内存限制，单位可以为Mib/Gib，将用于docker run --memory参数 requests: #资源请求的设置 cpu: string #Cpu请求，容器启动的初始可用数量 memory: string #内存请求,容器启动的初始可用数量 lifecycle: #生命周期钩子 postStart: #容器启动后立即执行此钩子,如果执行失败,会根据重启策略进行重启 preStop: #容器终止前执行此钩子,无论结果如何,容器都会终止 livenessProbe: #对Pod内各容器健康检查的设置，当探测无响应几次后将自动重启该容器 exec: #对Pod容器内检查方式设置为exec方式 command: [string] #exec方式需要制定的命令或脚本 httpGet: #对Pod内个容器健康检查方法设置为HttpGet，需要制定Path、port path: string port: number host: string scheme: string HttpHeaders: - name: string value: string tcpSocket: #对Pod内个容器健康检查方式设置为tcpSocket方式 port: number initialDelaySeconds: 0 #容器启动完成后首次探测的时间，单位为秒 timeoutSeconds: 0 #对容器健康检查探测等待响应的超时时间，单位秒，默认1秒 periodSeconds: 0 #对容器监控检查的定期探测时间设置，单位秒，默认10秒一次 successThreshold: 0 failureThreshold: 0 securityContext: privileged: false restartPolicy: [Always | Never | OnFailure] #Pod的重启策略 nodeName: \u003cstring\u003e #设置NodeName表示将该Pod调度到指定到名称的node节点上 nodeSelector: obeject #设置NodeSelector表示将该Pod调度到包含这个label的node上 imagePullSecrets: #Pull镜像时使用的secret名称，以key：secretkey格式指定 - name: string hostNetwork: false #是否使用主机网络模式，默认为false，如果设置为true，表示使用宿主机网络 volumes: #在该pod上定义共享存储卷列表 - name: string #共享存储卷名称 （volumes类型有很多种） emptyDir: {} #类型为emtyDir的存储卷，与Pod同生命周期的一个临时目录。为空值 hostPath: string #类型为hostPath的存储卷，表示挂载Pod所在宿主机的目录 path: string #Pod所在宿主机的目录，将被用于同期中mount的目录 secret: #类型为secret的存储卷，挂载集群与定义的secret对象到容器内部 scretname: string items: - key: string path: string configMap: #类型为configMap的存储卷，挂载预定义的configMap对象到容器内部 name: string items: - key: string path: string #小提示： # 在这里，可通过一个命令来查看每种资源的可配置项 # kubectl explain 资源类型 查看某种资源可以配置的一级属性 # kubectl explain 资源类型.属性 查看属性的子属性 [root@k8s-master01 ~]# kubectl explain pod KIND: Pod VERSION: v1 FIELDS: apiVersion \u003cstring\u003e kind \u003cstring\u003e metadata \u003cObject\u003e spec \u003cObject\u003e status \u003cObject\u003e [root@k8s-master01 ~]# kubectl explain pod.metadata KIND: Pod VERSION: v1 RESOURCE: metadata \u003cObject\u003e FIELDS: annotations \u003cmap[string]string\u003e clusterName \u003cstring\u003e creationTimestamp \u003cstring\u003e deletionGracePeriodSeconds \u003cinteger\u003e deletionTimestamp \u003cstring\u003e finalizers \u003c[]string\u003e generateName \u003cstring\u003e generation \u003cinteger\u003e labels \u003cmap[string]string\u003e managedFields \u003c[]Object\u003e name \u003cstring\u003e namespace \u003cstring\u003e ownerReferences \u003c[]Object\u003e resourceVersion \u003cstring\u003e selfLink \u003cstring\u003e uid \u003cstring\u003e 在kubernetes中基本所有资源的一级属性都是一样的，主要包含5部分： apiVersion \u003cstring\u003e 版本，由kubernetes内部定义，版本号必须可以用 kubectl api-versions 查询到 kind \u003cstring\u003e 类型，由kubernetes内部定义，版本号必须可以用 kubectl api-resources 查询到 metadata \u003cObject\u003e 元数据，主要是资源标识和说明，常用的有name、namespace、labels等 spec \u003cObject\u003e 描述，这是配置中最重要的一部分，里面是对各种资源配置的详细描述 status \u003cObject\u003e 状态信息，里面的内容不需要定义，由kubernetes自动生成 在上面的属性中，spec是接下来研究的重点，继续看下它的常见子属性: containers \u003c[]Object\u003e 容器列表，用于定义容器的详细信息 nodeName \u003cString\u003e 根据nodeName的值将pod调度到指定的Node节点上 nodeSelector \u003cmap[]\u003e 根据NodeSelector中定义的信息选择将该Pod调度到包含这些label的Node 上 hostNetwork \u003cboolean\u003e 是否使用主机网络模式，默认为false，如果设置为true，表示使用宿主机网络 volumes \u003c[]Object\u003e 存储卷，用于定义Pod上面挂在的存储信息 restartPolicy \u003cstring\u003e 重启策略，表示Pod在遇到故障的时候的处理策略 ","date":"2023-08-04","objectID":"/kubernetes/:21:2","tags":["devops","kubernetes"],"title":"Kubernetes使用手册 (资料来源于B站“冰糖没糖”的分享)","uri":"/kubernetes/"},{"categories":["笔记"],"content":"5.2 Pod配置 本小节主要来研究 pod.spec.containers属性，这也是pod配置中最为关键的一项配置。 [root@k8s-master01 ~]# kubectl explain pod.spec.containers KIND: Pod VERSION: v1 RESOURCE: containers \u003c[]Object\u003e # 数组，代表可以有多个容器 FIELDS: name \u003cstring\u003e # 容器名称 image \u003cstring\u003e # 容器需要的镜像地址 imagePullPolicy \u003cstring\u003e # 镜像拉取策略 command \u003c[]string\u003e # 容器的启动命令列表，如不指定，使用打包时使用的启动命令 args \u003c[]string\u003e # 容器的启动命令需要的参数列表 env \u003c[]Object\u003e # 容器环境变量的配置 ports \u003c[]Object\u003e # 容器需要暴露的端口号列表 resources \u003cObject\u003e # 资源限制和资源请求的设置 ","date":"2023-08-04","objectID":"/kubernetes/:22:0","tags":["devops","kubernetes"],"title":"Kubernetes使用手册 (资料来源于B站“冰糖没糖”的分享)","uri":"/kubernetes/"},{"categories":["笔记"],"content":"5.2.1 基本配置 创建pod-base.yaml文件，内容如下： apiVersion: v1 kind: Pod metadata: name: pod-base namespace: dev labels: user: heima spec: containers: - name: nginx image: nginx:1.17.1 - name: busybox image: busybox:1.30 上面定义了一个比较简单Pod的配置，里面有两个容器： nginx：用1.17.1版本的nginx镜像创建，（nginx是一个轻量级web容器） busybox：用1.30版本的busybox镜像创建，（busybox是一个小巧的linux命令集合） # 创建Pod [root@k8s-master01 pod]# kubectl apply -f pod-base.yaml pod/pod-base created # 查看Pod状况 # READY 1/2 : 表示当前Pod中有2个容器，其中1个准备就绪，1个未就绪 # RESTARTS : 重启次数，因为有1个容器故障了，Pod一直在重启试图恢复它 [root@k8s-master01 pod]# kubectl get pod -n dev NAME READY STATUS RESTARTS AGE pod-base 1/2 Running 4 95s # 可以通过describe查看内部的详情 # 此时已经运行起来了一个基本的Pod，虽然它暂时有问题 [root@k8s-master01 pod]# kubectl describe pod pod-base -n dev ","date":"2023-08-04","objectID":"/kubernetes/:22:1","tags":["devops","kubernetes"],"title":"Kubernetes使用手册 (资料来源于B站“冰糖没糖”的分享)","uri":"/kubernetes/"},{"categories":["笔记"],"content":"5.2.2 镜像拉取 创建pod-imagepullpolicy.yaml文件，内容如下： apiVersion: v1 kind: Pod metadata: name: pod-imagepullpolicy namespace: dev spec: containers: - name: nginx image: nginx:1.17.1 imagePullPolicy: Never # 用于设置镜像拉取策略 - name: busybox image: busybox:1.30 imagePullPolicy，用于设置镜像拉取策略，kubernetes支持配置三种拉取策略： Always：总是从远程仓库拉取镜像（一直远程下载） IfNotPresent：本地有则使用本地镜像，本地没有则从远程仓库拉取镜像（本地有就本地 本地没远程下载） Never：只使用本地镜像，从不去远程仓库拉取，本地没有就报错 （一直使用本地） 默认值说明： 如果镜像tag为具体版本号， 默认策略是：IfNotPresent 如果镜像tag为：latest（最终版本） ，默认策略是always # 创建Pod [root@k8s-master01 pod]# kubectl create -f pod-imagepullpolicy.yaml pod/pod-imagepullpolicy created # 查看Pod详情 # 此时明显可以看到nginx镜像有一步Pulling image \"nginx:1.17.1\"的过程 [root@k8s-master01 pod]# kubectl describe pod pod-imagepullpolicy -n dev ...... Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled \u003cunknown\u003e default-scheduler Successfully assigned dev/pod-imagePullPolicy to node1 Normal Pulling 32s kubelet, node1 Pulling image \"nginx:1.17.1\" Normal Pulled 26s kubelet, node1 Successfully pulled image \"nginx:1.17.1\" Normal Created 26s kubelet, node1 Created container nginx Normal Started 25s kubelet, node1 Started container nginx Normal Pulled 7s (x3 over 25s) kubelet, node1 Container image \"busybox:1.30\" already present on machine Normal Created 7s (x3 over 25s) kubelet, node1 Created container busybox Normal Started 7s (x3 over 25s) kubelet, node1 Started container busybox ","date":"2023-08-04","objectID":"/kubernetes/:22:2","tags":["devops","kubernetes"],"title":"Kubernetes使用手册 (资料来源于B站“冰糖没糖”的分享)","uri":"/kubernetes/"},{"categories":["笔记"],"content":"5.2.3 启动命令 在前面的案例中，一直有一个问题没有解决，就是的busybox容器一直没有成功运行，那么到底是什么原因导致这个容器的故障呢？ 原来busybox并不是一个程序，而是类似于一个工具类的集合，kubernetes集群启动管理后，它会自动关闭。解决方法就是让其一直在运行，这就用到了command配置。 创建pod-command.yaml文件，内容如下： apiVersion: v1 kind: Pod metadata: name: pod-command namespace: dev spec: containers: - name: nginx image: nginx:1.17.1 - name: busybox image: busybox:1.30 command: [\"/bin/sh\",\"-c\",\"touch /tmp/hello.txt;while true;do /bin/echo $(date +%T) \u003e\u003e /tmp/hello.txt; sleep 3; done;\"] command，用于在pod中的容器初始化完毕之后运行一个命令。 稍微解释下上面命令的意思： “/bin/sh”,\"-c”, 使用sh执行命令 touch /tmp/hello.txt; 创建一个/tmp/hello.txt 文件 while true;do /bin/echo $(date +%T) » /tmp/hello.txt; sleep 3; done; 每隔3秒向文件中写入当前时间 # 创建Pod [root@k8s-master01 pod]# kubectl create -f pod-command.yaml pod/pod-command created # 查看Pod状态 # 此时发现两个pod都正常运行了 [root@k8s-master01 pod]# kubectl get pods pod-command -n dev NAME READY STATUS RESTARTS AGE pod-command 2/2 Runing 0 2s # 进入pod中的busybox容器，查看文件内容 # 补充一个命令: kubectl exec pod名称 -n 命名空间 -it -c 容器名称 /bin/sh 在容器内部执行命令 # 使用这个命令就可以进入某个容器的内部，然后进行相关操作了 # 比如，可以查看txt文件的内容 [root@k8s-master01 pod]# kubectl exec pod-command -n dev -it -c busybox /bin/sh / # tail -f /tmp/hello.txt 14:44:19 14:44:22 14:44:25 特别说明： 通过上面发现command已经可以完成启动命令和传递参数的功能，为什么这里还要提供一个args选项，用于传递参数呢?这其实跟docker有点关系，kubernetes中的command、args两项其实是实现覆盖Dockerfile中ENTRYPOINT的功能。 1 如果command和args均没有写，那么用Dockerfile的配置。 2 如果command写了，但args没有写，那么Dockerfile默认的配置会被忽略，执行输入的command 3 如果command没写，但args写了，那么Dockerfile中配置的ENTRYPOINT的命令会被执行，使用当前args的参数 4 如果command和args都写了，那么Dockerfile的配置被忽略，执行command并追加上args参数 ","date":"2023-08-04","objectID":"/kubernetes/:22:3","tags":["devops","kubernetes"],"title":"Kubernetes使用手册 (资料来源于B站“冰糖没糖”的分享)","uri":"/kubernetes/"},{"categories":["笔记"],"content":"5.2.4 环境变量 创建pod-env.yaml文件，内容如下： apiVersion: v1 kind: Pod metadata: name: pod-env namespace: dev spec: containers: - name: busybox image: busybox:1.30 command: [\"/bin/sh\",\"-c\",\"while true;do /bin/echo $(date +%T);sleep 60; done;\"] env: # 设置环境变量列表 - name: \"username\" value: \"admin\" - name: \"password\" value: \"123456\" env，环境变量，用于在pod中的容器设置环境变量。 # 创建Pod [root@k8s-master01 ~]# kubectl create -f pod-env.yaml pod/pod-env created # 进入容器，输出环境变量 [root@k8s-master01 ~]# kubectl exec pod-env -n dev -c busybox -it /bin/sh / # echo $username admin / # echo $password 123456 这种方式不是很推荐，推荐将这些配置单独存储在配置文件中，这种方式将在后面介绍。 ","date":"2023-08-04","objectID":"/kubernetes/:22:4","tags":["devops","kubernetes"],"title":"Kubernetes使用手册 (资料来源于B站“冰糖没糖”的分享)","uri":"/kubernetes/"},{"categories":["笔记"],"content":"5.2.5 端口设置 本小节来介绍容器的端口设置，也就是containers的ports选项。 首先看下ports支持的子选项： [root@k8s-master01 ~]# kubectl explain pod.spec.containers.ports KIND: Pod VERSION: v1 RESOURCE: ports \u003c[]Object\u003e FIELDS: name \u003cstring\u003e # 端口名称，如果指定，必须保证name在pod中是唯一的 containerPort\u003cinteger\u003e # 容器要监听的端口(0\u003cx\u003c65536) hostPort \u003cinteger\u003e # 容器要在主机上公开的端口，如果设置，主机上只能运行容器的一个副本(一般省略) hostIP \u003cstring\u003e # 要将外部端口绑定到的主机IP(一般省略) protocol \u003cstring\u003e # 端口协议。必须是UDP、TCP或SCTP。默认为“TCP”。 接下来，编写一个测试案例，创建pod-ports.yaml apiVersion: v1 kind: Pod metadata: name: pod-ports namespace: dev spec: containers: - name: nginx image: nginx:1.17.1 ports: # 设置容器暴露的端口列表 - name: nginx-port containerPort: 80 protocol: TCP # 创建Pod [root@k8s-master01 ~]# kubectl create -f pod-ports.yaml pod/pod-ports created # 查看pod # 在下面可以明显看到配置信息 [root@k8s-master01 ~]# kubectl get pod pod-ports -n dev -o yaml ...... spec: containers: - image: nginx:1.17.1 imagePullPolicy: IfNotPresent name: nginx ports: - containerPort: 80 name: nginx-port protocol: TCP ...... 访问容器中的程序需要使用的是 Podip:containerPort ","date":"2023-08-04","objectID":"/kubernetes/:22:5","tags":["devops","kubernetes"],"title":"Kubernetes使用手册 (资料来源于B站“冰糖没糖”的分享)","uri":"/kubernetes/"},{"categories":["笔记"],"content":"5.2.6 资源配额 容器中的程序要运行，肯定是要占用一定资源的，比如cpu和内存等，如果不对某个容器的资源做限制，那么它就可能吃掉大量资源，导致其它容器无法运行。针对这种情况，kubernetes提供了对内存和cpu的资源进行配额的机制，这种机制主要通过resources选项实现，他有两个子选项： limits：用于限制运行时容器的最大占用资源，当容器占用资源超过limits时会被终止，并进行重启 requests ：用于设置容器需要的最小资源，如果环境资源不够，容器将无法启动 可以通过上面两个选项设置资源的上下限。 接下来，编写一个测试案例，创建pod-resources.yaml apiVersion: v1 kind: Pod metadata: name: pod-resources namespace: dev spec: containers: - name: nginx image: nginx:1.17.1 resources: # 资源配额 limits: # 限制资源（上限） cpu: \"2\" # CPU限制，单位是core数 memory: \"10Gi\" # 内存限制 requests: # 请求资源（下限） cpu: \"1\" # CPU限制，单位是core数 memory: \"10Mi\" # 内存限制 在这对cpu和memory的单位做一个说明： cpu：core数，可以为整数或小数 memory： 内存大小，可以使用Gi、Mi、G、M等形式 # 运行Pod [root@k8s-master01 ~]# kubectl create -f pod-resources.yaml pod/pod-resources created # 查看发现pod运行正常 [root@k8s-master01 ~]# kubectl get pod pod-resources -n dev NAME READY STATUS RESTARTS AGE pod-resources 1/1 Running 0 39s # 接下来，停止Pod [root@k8s-master01 ~]# kubectl delete -f pod-resources.yaml pod \"pod-resources\" deleted # 编辑pod，修改resources.requests.memory的值为10Gi [root@k8s-master01 ~]# vim pod-resources.yaml # 再次启动pod [root@k8s-master01 ~]# kubectl create -f pod-resources.yaml pod/pod-resources created # 查看Pod状态，发现Pod启动失败 [root@k8s-master01 ~]# kubectl get pod pod-resources -n dev -o wide NAME READY STATUS RESTARTS AGE pod-resources 0/1 Pending 0 20s # 查看pod详情会发现，如下提示 [root@k8s-master01 ~]# kubectl describe pod pod-resources -n dev ...... Warning FailedScheduling 35s default-scheduler 0/3 nodes are available: 1 node(s) had taint {node-role.kubernetes.io/master: }, that the pod didn't tolerate, 2 Insufficient memory.(内存不足) ","date":"2023-08-04","objectID":"/kubernetes/:22:6","tags":["devops","kubernetes"],"title":"Kubernetes使用手册 (资料来源于B站“冰糖没糖”的分享)","uri":"/kubernetes/"},{"categories":["笔记"],"content":"5.3 Pod生命周期 我们一般将pod对象从创建至终的这段时间范围称为pod的生命周期，它主要包含下面的过程： pod创建过程 运行初始化容器（init container）过程 运行主容器（main container） 容器启动后钩子（post start）、容器终止前钩子（pre stop） 容器的存活性探测（liveness probe）、就绪性探测（readiness probe） pod终止过程 在整个生命周期中，Pod会出现5种状态（相位），分别如下： 挂起（Pending）：apiserver已经创建了pod资源对象，但它尚未被调度完成或者仍处于下载镜像的过程中 运行中（Running）：pod已经被调度至某节点，并且所有容器都已经被kubelet创建完成 成功（Succeeded）：pod中的所有容器都已经成功终止并且不会被重启 失败（Failed）：所有容器都已经终止，但至少有一个容器终止失败，即容器返回了非0值的退出状态 未知（Unknown）：apiserver无法正常获取到pod对象的状态信息，通常由网络通信失败所导致 ","date":"2023-08-04","objectID":"/kubernetes/:23:0","tags":["devops","kubernetes"],"title":"Kubernetes使用手册 (资料来源于B站“冰糖没糖”的分享)","uri":"/kubernetes/"},{"categories":["笔记"],"content":"5.3.1 创建和终止 pod的创建过程 用户通过kubectl或其他api客户端提交需要创建的pod信息给apiServer apiServer开始生成pod对象的信息，并将信息存入etcd，然后返回确认信息至客户端 apiServer开始反映etcd中的pod对象的变化，其它组件使用watch机制来跟踪检查apiServer上的变动 scheduler发现有新的pod对象要创建，开始为Pod分配主机并将结果信息更新至apiServer node节点上的kubelet发现有pod调度过来，尝试调用docker启动容器，并将结果回送至apiServer apiServer将接收到的pod状态信息存入etcd中 pod的终止过程 用户向apiServer发送删除pod对象的命令 apiServcer中的pod对象信息会随着时间的推移而更新，在宽限期内（默认30s），pod被视为dead 将pod标记为terminating状态 kubelet在监控到pod对象转为terminating状态的同时启动pod关闭过程 端点控制器监控到pod对象的关闭行为时将其从所有匹配到此端点的service资源的端点列表中移除 如果当前pod对象定义了preStop钩子处理器，则在其标记为terminating后即会以同步的方式启动执行 pod对象中的容器进程收到停止信号 宽限期结束后，若pod中还存在仍在运行的进程，那么pod对象会收到立即终止的信号 kubelet请求apiServer将此pod资源的宽限期设置为0从而完成删除操作，此时pod对于用户已不可见 ","date":"2023-08-04","objectID":"/kubernetes/:23:1","tags":["devops","kubernetes"],"title":"Kubernetes使用手册 (资料来源于B站“冰糖没糖”的分享)","uri":"/kubernetes/"},{"categories":["笔记"],"content":"5.3.2 初始化容器 初始化容器是在pod的主容器启动之前要运行的容器，主要是做一些主容器的前置工作，它具有两大特征： 初始化容器必须运行完成直至结束，若某初始化容器运行失败，那么kubernetes需要重启它直到成功完成 初始化容器必须按照定义的顺序执行，当且仅当前一个成功之后，后面的一个才能运行 初始化容器有很多的应用场景，下面列出的是最常见的几个： 提供主容器镜像中不具备的工具程序或自定义代码 初始化容器要先于应用容器串行启动并运行完成，因此可用于延后应用容器的启动直至其依赖的条件得到满足 接下来做一个案例，模拟下面这个需求： 假设要以主容器来运行nginx，但是要求在运行nginx之前先要能够连接上mysql和redis所在服务器 为了简化测试，事先规定好mysql (192.168.5.4)和redis (192.168.5.5)服务器的地址 创建pod-initcontainer.yaml，内容如下： apiVersion: v1 kind: Pod metadata: name: pod-initcontainer namespace: dev spec: containers: - name: main-container image: nginx:1.17.1 ports: - name: nginx-port containerPort: 80 initContainers: - name: test-mysql image: busybox:1.30 command: ['sh', '-c', 'until ping 192.168.5.14 -c 1 ; do echo waiting for mysql...; sleep 2; done;'] - name: test-redis image: busybox:1.30 command: ['sh', '-c', 'until ping 192.168.5.15 -c 1 ; do echo waiting for reids...; sleep 2; done;'] # 创建pod [root@k8s-master01 ~]# kubectl create -f pod-initcontainer.yaml pod/pod-initcontainer created # 查看pod状态 # 发现pod卡在启动第一个初始化容器过程中，后面的容器不会运行 root@k8s-master01 ~]# kubectl describe pod pod-initcontainer -n dev ........ Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 49s default-scheduler Successfully assigned dev/pod-initcontainer to node1 Normal Pulled 48s kubelet, node1 Container image \"busybox:1.30\" already present on machine Normal Created 48s kubelet, node1 Created container test-mysql Normal Started 48s kubelet, node1 Started container test-mysql # 动态查看pod [root@k8s-master01 ~]# kubectl get pods pod-initcontainer -n dev -w NAME READY STATUS RESTARTS AGE pod-initcontainer 0/1 Init:0/2 0 15s pod-initcontainer 0/1 Init:1/2 0 52s pod-initcontainer 0/1 Init:1/2 0 53s pod-initcontainer 0/1 PodInitializing 0 89s pod-initcontainer 1/1 Running 0 90s # 接下来新开一个shell，为当前服务器新增两个ip，观察pod的变化 [root@k8s-master01 ~]# ifconfig ens33:1 192.168.5.14 netmask 255.255.255.0 up [root@k8s-master01 ~]# ifconfig ens33:2 192.168.5.15 netmask 255.255.255.0 up ","date":"2023-08-04","objectID":"/kubernetes/:23:2","tags":["devops","kubernetes"],"title":"Kubernetes使用手册 (资料来源于B站“冰糖没糖”的分享)","uri":"/kubernetes/"},{"categories":["笔记"],"content":"5.3.3 钩子函数 钩子函数能够感知自身生命周期中的事件，并在相应的时刻到来时运行用户指定的程序代码。 kubernetes在主容器的启动之后和停止之前提供了两个钩子函数： post start：容器创建之后执行，如果失败了会重启容器 pre stop ：容器终止之前执行，执行完成之后容器将成功终止，在其完成之前会阻塞删除容器的操作 钩子处理器支持使用下面三种方式定义动作： Exec命令：在容器内执行一次命令 …… lifecycle: postStart: exec: command: - cat - /tmp/healthy …… TCPSocket：在当前容器尝试访问指定的socket …… lifecycle: postStart: tcpSocket: port: 8080 …… HTTPGet：在当前容器中向某url发起http请求 …… lifecycle: postStart: httpGet: path: / #URI地址 port: 80 #端口号 host: 192.168.5.3 #主机地址 scheme: HTTP #支持的协议，http或者https …… 接下来，以exec方式为例，演示下钩子函数的使用，创建pod-hook-exec.yaml文件，内容如下： apiVersion: v1 kind: Pod metadata: name: pod-hook-exec namespace: dev spec: containers: - name: main-container image: nginx:1.17.1 ports: - name: nginx-port containerPort: 80 lifecycle: postStart: exec: # 在容器启动的时候执行一个命令，修改掉nginx的默认首页内容 command: [\"/bin/sh\", \"-c\", \"echo postStart... \u003e /usr/share/nginx/html/index.html\"] preStop: exec: # 在容器停止之前停止nginx服务 command: [\"/usr/sbin/nginx\",\"-s\",\"quit\"] # 创建pod [root@k8s-master01 ~]# kubectl create -f pod-hook-exec.yaml pod/pod-hook-exec created # 查看pod [root@k8s-master01 ~]# kubectl get pods pod-hook-exec -n dev -o wide NAME READY STATUS RESTARTS AGE IP NODE pod-hook-exec 1/1 Running 0 29s 10.244.2.48 node2 # 访问pod [root@k8s-master01 ~]# curl 10.244.2.48 postStart... ","date":"2023-08-04","objectID":"/kubernetes/:23:3","tags":["devops","kubernetes"],"title":"Kubernetes使用手册 (资料来源于B站“冰糖没糖”的分享)","uri":"/kubernetes/"},{"categories":["笔记"],"content":"5.3.4 容器探测 容器探测用于检测容器中的应用实例是否正常工作，是保障业务可用性的一种传统机制。如果经过探测，实例的状态不符合预期，那么kubernetes就会把该问题实例” 摘除 “，不承担业务流量。kubernetes提供了两种探针来实现容器探测，分别是： liveness probes：存活性探针，用于检测应用实例当前是否处于正常运行状态，如果不是，k8s会重启容器 readiness probes：就绪性探针，用于检测应用实例当前是否可以接收请求，如果不能，k8s不会转发流量 livenessProbe 决定是否重启容器，readinessProbe 决定是否将请求转发给容器。 上面两种探针目前均支持三种探测方式： Exec命令：在容器内执行一次命令，如果命令执行的退出码为0，则认为程序正常，否则不正常 …… livenessProbe: exec: command: - cat - /tmp/healthy …… TCPSocket：将会尝试访问一个用户容器的端口，如果能够建立这条连接，则认为程序正常，否则不正常 …… livenessProbe: tcpSocket: port: 8080 …… HTTPGet：调用容器内Web应用的URL，如果返回的状态码在200和399之间，则认为程序正常，否则不正常 …… livenessProbe: httpGet: path: / #URI地址 port: 80 #端口号 host: 127.0.0.1 #主机地址 scheme: HTTP #支持的协议，http或者https …… 下面以liveness probes为例，做几个演示： 方式一：Exec 创建pod-liveness-exec.yaml apiVersion: v1 kind: Pod metadata: name: pod-liveness-exec namespace: dev spec: containers: - name: nginx image: nginx:1.17.1 ports: - name: nginx-port containerPort: 80 livenessProbe: exec: command: [\"/bin/cat\",\"/tmp/hello.txt\"] # 执行一个查看文件的命令 创建pod，观察效果 # 创建Pod [root@k8s-master01 ~]# kubectl create -f pod-liveness-exec.yaml pod/pod-liveness-exec created # 查看Pod详情 [root@k8s-master01 ~]# kubectl describe pods pod-liveness-exec -n dev ...... Normal Created 20s (x2 over 50s) kubelet, node1 Created container nginx Normal Started 20s (x2 over 50s) kubelet, node1 Started container nginx Normal Killing 20s kubelet, node1 Container nginx failed liveness probe, will be restarted Warning Unhealthy 0s (x5 over 40s) kubelet, node1 Liveness probe failed: cat: can't open '/tmp/hello11.txt': No such file or directory # 观察上面的信息就会发现nginx容器启动之后就进行了健康检查 # 检查失败之后，容器被kill掉，然后尝试进行重启（这是重启策略的作用，后面讲解） # 稍等一会之后，再观察pod信息，就可以看到RESTARTS不再是0，而是一直增长 [root@k8s-master01 ~]# kubectl get pods pod-liveness-exec -n dev NAME READY STATUS RESTARTS AGE pod-liveness-exec 0/1 CrashLoopBackOff 2 3m19s # 当然接下来，可以修改成一个存在的文件，比如/tmp/hello.txt，再试，结果就正常了...... 方式二：TCPSocket 创建pod-liveness-tcpsocket.yaml apiVersion: v1 kind: Pod metadata: name: pod-liveness-tcpsocket namespace: dev spec: containers: - name: nginx image: nginx:1.17.1 ports: - name: nginx-port containerPort: 80 livenessProbe: tcpSocket: port: 8080 # 尝试访问8080端口 创建pod，观察效果 # 创建Pod [root@k8s-master01 ~]# kubectl create -f pod-liveness-tcpsocket.yaml pod/pod-liveness-tcpsocket created # 查看Pod详情 [root@k8s-master01 ~]# kubectl describe pods pod-liveness-tcpsocket -n dev ...... Normal Scheduled 31s default-scheduler Successfully assigned dev/pod-liveness-tcpsocket to node2 Normal Pulled \u003cinvalid\u003e kubelet, node2 Container image \"nginx:1.17.1\" already present on machine Normal Created \u003cinvalid\u003e kubelet, node2 Created container nginx Normal Started \u003cinvalid\u003e kubelet, node2 Started container nginx Warning Unhealthy \u003cinvalid\u003e (x2 over \u003cinvalid\u003e) kubelet, node2 Liveness probe failed: dial tcp 10.244.2.44:8080: connect: connection refused # 观察上面的信息，发现尝试访问8080端口,但是失败了 # 稍等一会之后，再观察pod信息，就可以看到RESTARTS不再是0，而是一直增长 [root@k8s-master01 ~]# kubectl get pods pod-liveness-tcpsocket -n dev NAME READY STATUS RESTARTS AGE pod-liveness-tcpsocket 0/1 CrashLoopBackOff 2 3m19s # 当然接下来，可以修改成一个可以访问的端口，比如80，再试，结果就正常了...... 方式三：HTTPGet 创建pod-liveness-httpget.yaml apiVersion: v1 kind: Pod metadata: name: pod-liveness-httpget namespace: dev spec: containers: - name: nginx image: nginx:1.17.1 ports: - name: nginx-port containerPort: 80 livenessProbe: httpGet: # 其实就是访问http://127.0.0.1:80/hello scheme: HTTP #支持的协议，http或者https port: 80 #端口号 path: /hello #URI地址 创建pod，观察效果 # 创建Pod [root@k8s-master01 ~]# kubectl create -f pod-liveness-httpget.yaml pod/pod-liveness-httpget created # 查看Pod详情 [root@k8s-master01 ~]# kubectl describe pod pod-liveness-httpget -n dev ....... Normal Pulled 6s (x3 over 64s) kubelet, node1 Container image \"nginx:1.17.1\" already present on machine Normal Created 6s (x3 over 64s) kubelet, node1 Created container nginx Normal Started 6s (x3 over 63s) kubelet, node1 Started container nginx Warning Unhealthy 6s (x6 over 56s) kubelet, node1 Liveness probe failed: HTTP probe failed with statuscode: 404 Norma","date":"2023-08-04","objectID":"/kubernetes/:23:4","tags":["devops","kubernetes"],"title":"Kubernetes使用手册 (资料来源于B站“冰糖没糖”的分享)","uri":"/kubernetes/"},{"categories":["笔记"],"content":"5.3.5 重启策略 在上一节中，一旦容器探测出现了问题，kubernetes就会对容器所在的Pod进行重启，其实这是由pod的重启策略决定的，pod的重启策略有 3 种，分别如下： Always ：容器失效时，自动重启该容器，这也是默认值。 OnFailure ： 容器终止运行且退出码不为0时重启 Never ： 不论状态为何，都不重启该容器 重启策略适用于pod对象中的所有容器，首次需要重启的容器，将在其需要时立即进行重启，随后再次需要重启的操作将由kubelet延迟一段时间后进行，且反复的重启操作的延迟时长以此为10s、20s、40s、80s、160s和300s，300s是最大延迟时长。 创建pod-restartpolicy.yaml： apiVersion: v1 kind: Pod metadata: name: pod-restartpolicy namespace: dev spec: containers: - name: nginx image: nginx:1.17.1 ports: - name: nginx-port containerPort: 80 livenessProbe: httpGet: scheme: HTTP port: 80 path: /hello restartPolicy: Never # 设置重启策略为Never 运行Pod测试 # 创建Pod [root@k8s-master01 ~]# kubectl create -f pod-restartpolicy.yaml pod/pod-restartpolicy created # 查看Pod详情，发现nginx容器失败 [root@k8s-master01 ~]# kubectl describe pods pod-restartpolicy -n dev ...... Warning Unhealthy 15s (x3 over 35s) kubelet, node1 Liveness probe failed: HTTP probe failed with statuscode: 404 Normal Killing 15s kubelet, node1 Container nginx failed liveness probe # 多等一会，再观察pod的重启次数，发现一直是0，并未重启 [root@k8s-master01 ~]# kubectl get pods pod-restartpolicy -n dev NAME READY STATUS RESTARTS AGE pod-restartpolicy 0/1 Running 0 5min42s ","date":"2023-08-04","objectID":"/kubernetes/:23:5","tags":["devops","kubernetes"],"title":"Kubernetes使用手册 (资料来源于B站“冰糖没糖”的分享)","uri":"/kubernetes/"},{"categories":["笔记"],"content":"5.4 Pod调度 在默认情况下，一个Pod在哪个Node节点上运行，是由Scheduler组件采用相应的算法计算出来的，这个过程是不受人工控制的。但是在实际使用中，这并不满足的需求，因为很多情况下，我们想控制某些Pod到达某些节点上，那么应该怎么做呢？这就要求了解kubernetes对Pod的调度规则，kubernetes提供了四大类调度方式： 自动调度：运行在哪个节点上完全由Scheduler经过一系列的算法计算得出 定向调度：NodeName、NodeSelector 亲和性调度：NodeAffinity、PodAffinity、PodAntiAffinity 污点（容忍）调度：Taints、Toleration ","date":"2023-08-04","objectID":"/kubernetes/:24:0","tags":["devops","kubernetes"],"title":"Kubernetes使用手册 (资料来源于B站“冰糖没糖”的分享)","uri":"/kubernetes/"},{"categories":["笔记"],"content":"5.4.1 定向调度 定向调度，指的是利用在pod上声明nodeName或者nodeSelector，以此将Pod调度到期望的node节点上。注意，这里的调度是强制的，这就意味着即使要调度的目标Node不存在，也会向上面进行调度，只不过pod运行失败而已。 NodeName NodeName用于强制约束将Pod调度到指定的Name的Node节点上。这种方式，其实是直接跳过Scheduler的调度逻辑，直接将Pod调度到指定名称的节点。 接下来，实验一下：创建一个pod-nodename.yaml文件 apiVersion: v1 kind: Pod metadata: name: pod-nodename namespace: dev spec: containers: - name: nginx image: nginx:1.17.1 nodeName: node1 # 指定调度到node1节点上 #创建Pod [root@k8s-master01 ~]# kubectl create -f pod-nodename.yaml pod/pod-nodename created #查看Pod调度到NODE属性，确实是调度到了node1节点上 [root@k8s-master01 ~]# kubectl get pods pod-nodename -n dev -o wide NAME READY STATUS RESTARTS AGE IP NODE ...... pod-nodename 1/1 Running 0 56s 10.244.1.87 node1 ...... # 接下来，删除pod，修改nodeName的值为node3（并没有node3节点） [root@k8s-master01 ~]# kubectl delete -f pod-nodename.yaml pod \"pod-nodename\" deleted [root@k8s-master01 ~]# vim pod-nodename.yaml [root@k8s-master01 ~]# kubectl create -f pod-nodename.yaml pod/pod-nodename created #再次查看，发现已经向Node3节点调度，但是由于不存在node3节点，所以pod无法正常运行 [root@k8s-master01 ~]# kubectl get pods pod-nodename -n dev -o wide NAME READY STATUS RESTARTS AGE IP NODE ...... pod-nodename 0/1 Pending 0 6s \u003cnone\u003e node3 ...... NodeSelector NodeSelector用于将pod调度到添加了指定标签的node节点上。它是通过kubernetes的label-selector机制实现的，也就是说，在pod创建之前，会由scheduler使用MatchNodeSelector调度策略进行label匹配，找出目标node，然后将pod调度到目标节点，该匹配规则是强制约束。 接下来，实验一下： 1 首先分别为node节点添加标签 [root@k8s-master01 ~]# kubectl label nodes node1 nodeenv=pro node/node2 labeled [root@k8s-master01 ~]# kubectl label nodes node2 nodeenv=test node/node2 labeled 2 创建一个pod-nodeselector.yaml文件，并使用它创建Pod apiVersion: v1 kind: Pod metadata: name: pod-nodeselector namespace: dev spec: containers: - name: nginx image: nginx:1.17.1 nodeSelector: nodeenv: pro # 指定调度到具有nodeenv=pro标签的节点上 #创建Pod [root@k8s-master01 ~]# kubectl create -f pod-nodeselector.yaml pod/pod-nodeselector created #查看Pod调度到NODE属性，确实是调度到了node1节点上 [root@k8s-master01 ~]# kubectl get pods pod-nodeselector -n dev -o wide NAME READY STATUS RESTARTS AGE IP NODE ...... pod-nodeselector 1/1 Running 0 47s 10.244.1.87 node1 ...... # 接下来，删除pod，修改nodeSelector的值为nodeenv: xxxx（不存在打有此标签的节点） [root@k8s-master01 ~]# kubectl delete -f pod-nodeselector.yaml pod \"pod-nodeselector\" deleted [root@k8s-master01 ~]# vim pod-nodeselector.yaml [root@k8s-master01 ~]# kubectl create -f pod-nodeselector.yaml pod/pod-nodeselector created #再次查看，发现pod无法正常运行,Node的值为none [root@k8s-master01 ~]# kubectl get pods -n dev -o wide NAME READY STATUS RESTARTS AGE IP NODE pod-nodeselector 0/1 Pending 0 2m20s \u003cnone\u003e \u003cnone\u003e # 查看详情,发现node selector匹配失败的提示 [root@k8s-master01 ~]# kubectl describe pods pod-nodeselector -n dev ....... Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning FailedScheduling \u003cunknown\u003e default-scheduler 0/3 nodes are available: 3 node(s) didn't match node selector. ","date":"2023-08-04","objectID":"/kubernetes/:24:1","tags":["devops","kubernetes"],"title":"Kubernetes使用手册 (资料来源于B站“冰糖没糖”的分享)","uri":"/kubernetes/"},{"categories":["笔记"],"content":"5.4.2 亲和性调度 上一节，介绍了两种定向调度的方式，使用起来非常方便，但是也有一定的问题，那就是如果没有满足条件的Node，那么Pod将不会被运行，即使在集群中还有可用Node列表也不行，这就限制了它的使用场景。 基于上面的问题，kubernetes还提供了一种亲和性调度（Affinity）。它在NodeSelector的基础之上的进行了扩展，可以通过配置的形式，实现优先选择满足条件的Node进行调度，如果没有，也可以调度到不满足条件的节点上，使调度更加灵活。 Affinity主要分为三类： nodeAffinity(node亲和性）: 以node为目标，解决pod可以调度到哪些node的问题 podAffinity(pod亲和性) : 以pod为目标，解决pod可以和哪些已存在的pod部署在同一个拓扑域中的问题 podAntiAffinity(pod反亲和性) : 以pod为目标，解决pod不能和哪些已存在pod部署在同一个拓扑域中的问题 关于亲和性(反亲和性)使用场景的说明： 亲和性：如果两个应用频繁交互，那就有必要利用亲和性让两个应用的尽可能的靠近，这样可以减少因网络通信而带来的性能损耗。 反亲和性：当应用的采用多副本部署时，有必要采用反亲和性让各个应用实例打散分布在各个node上，这样可以提高服务的高可用性。 NodeAffinity 首先来看一下 NodeAffinity的可配置项： pod.spec.affinity.nodeAffinity requiredDuringSchedulingIgnoredDuringExecution Node节点必须满足指定的所有规则才可以，相当于硬限制 nodeSelectorTerms 节点选择列表 matchFields 按节点字段列出的节点选择器要求列表 matchExpressions 按节点标签列出的节点选择器要求列表(推荐) key 键 values 值 operator 关系符 支持Exists, DoesNotExist, In, NotIn, Gt, Lt preferredDuringSchedulingIgnoredDuringExecution 优先调度到满足指定的规则的Node，相当于软限制 (倾向) preference 一个节点选择器项，与相应的权重相关联 matchFields 按节点字段列出的节点选择器要求列表 matchExpressions 按节点标签列出的节点选择器要求列表(推荐) key 键 values 值 operator 关系符 支持In, NotIn, Exists, DoesNotExist, Gt, Lt weight 倾向权重，在范围1-100。 关系符的使用说明: - matchExpressions: - key: nodeenv # 匹配存在标签的key为nodeenv的节点 operator: Exists - key: nodeenv # 匹配标签的key为nodeenv,且value是\"xxx\"或\"yyy\"的节点 operator: In values: [\"xxx\",\"yyy\"] - key: nodeenv # 匹配标签的key为nodeenv,且value大于\"xxx\"的节点 operator: Gt values: \"xxx\" 接下来首先演示一下 requiredDuringSchedulingIgnoredDuringExecution , 创建pod-nodeaffinity-required.yaml apiVersion: v1 kind: Pod metadata: name: pod-nodeaffinity-required namespace: dev spec: containers: - name: nginx image: nginx:1.17.1 affinity: #亲和性设置 nodeAffinity: #设置node亲和性 requiredDuringSchedulingIgnoredDuringExecution: # 硬限制 nodeSelectorTerms: - matchExpressions: # 匹配env的值在[\"xxx\",\"yyy\"]中的标签 - key: nodeenv operator: In values: [\"xxx\",\"yyy\"] # 创建pod [root@k8s-master01 ~]# kubectl create -f pod-nodeaffinity-required.yaml pod/pod-nodeaffinity-required created # 查看pod状态 （运行失败） [root@k8s-master01 ~]# kubectl get pods pod-nodeaffinity-required -n dev -o wide NAME READY STATUS RESTARTS AGE IP NODE ...... pod-nodeaffinity-required 0/1 Pending 0 16s \u003cnone\u003e \u003cnone\u003e ...... # 查看Pod的详情 # 发现调度失败，提示node选择失败 [root@k8s-master01 ~]# kubectl describe pod pod-nodeaffinity-required -n dev ...... Warning FailedScheduling \u003cunknown\u003e default-scheduler 0/3 nodes are available: 3 node(s) didn't match node selector. Warning FailedScheduling \u003cunknown\u003e default-scheduler 0/3 nodes are available: 3 node(s) didn't match node selector. #接下来，停止pod [root@k8s-master01 ~]# kubectl delete -f pod-nodeaffinity-required.yaml pod \"pod-nodeaffinity-required\" deleted # 修改文件，将values: [\"xxx\",\"yyy\"]------\u003e [\"pro\",\"yyy\"] [root@k8s-master01 ~]# vim pod-nodeaffinity-required.yaml # 再次启动 [root@k8s-master01 ~]# kubectl create -f pod-nodeaffinity-required.yaml pod/pod-nodeaffinity-required created # 此时查看，发现调度成功，已经将pod调度到了node1上 [root@k8s-master01 ~]# kubectl get pods pod-nodeaffinity-required -n dev -o wide NAME READY STATUS RESTARTS AGE IP NODE ...... pod-nodeaffinity-required 1/1 Running 0 11s 10.244.1.89 node1 ...... 接下来再演示一下 requiredDuringSchedulingIgnoredDuringExecution , 创建pod-nodeaffinity-preferred.yaml apiVersion: v1 kind: Pod metadata: name: pod-nodeaffinity-preferred namespace: dev spec: containers: - name: nginx image: nginx:1.17.1 affinity: #亲和性设置 nodeAffinity: #设置node亲和性 preferredDuringSchedulingIgnoredDuringExecution: # 软限制 - weight: 1 preference: matchExpressions: # 匹配env的值在[\"xxx\",\"yyy\"]中的标签(当前环境没有) - key: nodeenv operator: In values: [\"xxx\",\"yyy\"] # 创建pod [root@k8s-master01 ~]# kubectl create -f pod-nodeaffinity-preferred.yaml pod/pod-nodeaffinity-preferred created # 查看pod状态 （运行成功） [root@k8s-master01 ~]# kubectl get pod pod-nodeaffinity-preferred -n dev NAME READY STATUS RESTARTS AGE pod-nodeaffinity-preferred 1/1 Running 0 40s NodeAffinity规则设置的注意事项： 1 如果同时定义了nodeSelector和nodeAffinity，那么必须两个条件都得到满足，Pod才能运行在指定的Node上 2 如果nodeAffinity指定了多个nodeSelectorTerms，那么只需要其中一个能够匹配成功即可 3 如果一个nodeSelectorTerms","date":"2023-08-04","objectID":"/kubernetes/:24:2","tags":["devops","kubernetes"],"title":"Kubernetes使用手册 (资料来源于B站“冰糖没糖”的分享)","uri":"/kubernetes/"},{"categories":["笔记"],"content":"5.4.3 污点和容忍 污点（Taints） 前面的调度方式都是站在Pod的角度上，通过在Pod上添加属性，来确定Pod是否要调度到指定的Node上，其实我们也可以站在Node的角度上，通过在Node上添加污点属性，来决定是否允许Pod调度过来。 Node被设置上污点之后就和Pod之间存在了一种相斥的关系，进而拒绝Pod调度进来，甚至可以将已经存在的Pod驱逐出去。 污点的格式为：key=value:effect, key和value是污点的标签，effect描述污点的作用，支持如下三个选项： PreferNoSchedule：kubernetes将尽量避免把Pod调度到具有该污点的Node上，除非没有其他节点可调度 NoSchedule：kubernetes将不会把Pod调度到具有该污点的Node上，但不会影响当前Node上已存在的Pod NoExecute：kubernetes将不会把Pod调度到具有该污点的Node上，同时也会将Node上已存在的Pod驱离 使用kubectl设置和去除污点的命令示例如下： # 设置污点 kubectl taint nodes node1 key=value:effect # 去除污点 kubectl taint nodes node1 key:effect- # 去除所有污点 kubectl taint nodes node1 key- 接下来，演示下污点的效果： 准备节点node1（为了演示效果更加明显，暂时停止node2节点） 为node1节点设置一个污点: tag=heima:PreferNoSchedule；然后创建pod1( pod1 可以 ) 修改为node1节点设置一个污点: tag=heima:NoSchedule；然后创建pod2( pod1 正常 pod2 失败 ) 修改为node1节点设置一个污点: tag=heima:NoExecute；然后创建pod3 ( 3个pod都失败 ) # 为node1设置污点(PreferNoSchedule) [root@k8s-master01 ~]# kubectl taint nodes node1 tag=heima:PreferNoSchedule # 创建pod1 [root@k8s-master01 ~]# kubectl run taint1 --image=nginx:1.17.1 -n dev [root@k8s-master01 ~]# kubectl get pods -n dev -o wide NAME READY STATUS RESTARTS AGE IP NODE taint1-7665f7fd85-574h4 1/1 Running 0 2m24s 10.244.1.59 node1 # 为node1设置污点(取消PreferNoSchedule，设置NoSchedule) [root@k8s-master01 ~]# kubectl taint nodes node1 tag:PreferNoSchedule- [root@k8s-master01 ~]# kubectl taint nodes node1 tag=heima:NoSchedule # 创建pod2 [root@k8s-master01 ~]# kubectl run taint2 --image=nginx:1.17.1 -n dev [root@k8s-master01 ~]# kubectl get pods taint2 -n dev -o wide NAME READY STATUS RESTARTS AGE IP NODE taint1-7665f7fd85-574h4 1/1 Running 0 2m24s 10.244.1.59 node1 taint2-544694789-6zmlf 0/1 Pending 0 21s \u003cnone\u003e \u003cnone\u003e # 为node1设置污点(取消NoSchedule，设置NoExecute) [root@k8s-master01 ~]# kubectl taint nodes node1 tag:NoSchedule- [root@k8s-master01 ~]# kubectl taint nodes node1 tag=heima:NoExecute # 创建pod3 [root@k8s-master01 ~]# kubectl run taint3 --image=nginx:1.17.1 -n dev [root@k8s-master01 ~]# kubectl get pods -n dev -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED taint1-7665f7fd85-htkmp 0/1 Pending 0 35s \u003cnone\u003e \u003cnone\u003e \u003cnone\u003e taint2-544694789-bn7wb 0/1 Pending 0 35s \u003cnone\u003e \u003cnone\u003e \u003cnone\u003e taint3-6d78dbd749-tktkq 0/1 Pending 0 6s \u003cnone\u003e \u003cnone\u003e \u003cnone\u003e 小提示： 使用kubeadm搭建的集群，默认就会给master节点添加一个污点标记,所以pod就不会调度到master节点上. 容忍（Toleration） 上面介绍了污点的作用，我们可以在node上添加污点用于拒绝pod调度上来，但是如果就是想将一个pod调度到一个有污点的node上去，这时候应该怎么做呢？这就要使用到容忍。 污点就是拒绝，容忍就是忽略，Node通过污点拒绝pod调度上去，Pod通过容忍忽略拒绝 下面先通过一个案例看下效果： 上一小节，已经在node1节点上打上了 NoExecute的污点，此时pod是调度不上去的 本小节，可以通过给pod添加容忍，然后将其调度上去 创建pod-toleration.yaml,内容如下 apiVersion: v1 kind: Pod metadata: name: pod-toleration namespace: dev spec: containers: - name: nginx image: nginx:1.17.1 tolerations: # 添加容忍 - key: \"tag\" # 要容忍的污点的key operator: \"Equal\" # 操作符 value: \"heima\" # 容忍的污点的value effect: \"NoExecute\" # 添加容忍的规则，这里必须和标记的污点规则相同 # 添加容忍之前的pod [root@k8s-master01 ~]# kubectl get pods -n dev -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED pod-toleration 0/1 Pending 0 3s \u003cnone\u003e \u003cnone\u003e \u003cnone\u003e # 添加容忍之后的pod [root@k8s-master01 ~]# kubectl get pods -n dev -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED pod-toleration 1/1 Running 0 3s 10.244.1.62 node1 \u003cnone\u003e 下面看一下容忍的详细配置: [root@k8s-master01 ~]# kubectl explain pod.spec.tolerations ...... FIELDS: key # 对应着要容忍的污点的键，空意味着匹配所有的键 value # 对应着要容忍的污点的值 operator # key-value的运算符，支持Equal和Exists（默认） effect # 对应污点的effect，空意味着匹配所有影响 tolerationSeconds # 容忍时间, 当effect为NoExecute时生效，表示pod在Node上的停留时间 6. Pod控制器详解 ","date":"2023-08-04","objectID":"/kubernetes/:24:3","tags":["devops","kubernetes"],"title":"Kubernetes使用手册 (资料来源于B站“冰糖没糖”的分享)","uri":"/kubernetes/"},{"categories":["笔记"],"content":"6.1 Pod控制器介绍 Pod是kubernetes的最小管理单元，在kubernetes中，按照pod的创建方式可以将其分为两类： 自主式pod：kubernetes直接创建出来的Pod，这种pod删除后就没有了，也不会重建 控制器创建的pod：kubernetes通过控制器创建的pod，这种pod删除了之后还会自动重建 什么是Pod控制器 Pod控制器是管理pod的中间层，使用Pod控制器之后，只需要告诉Pod控制器，想要多少个什么样的Pod就可以了，它会创建出满足条件的Pod并确保每一个Pod资源处于用户期望的目标状态。如果Pod资源在运行中出现故障，它会基于指定策略重新编排Pod。 在kubernetes中，有很多类型的pod控制器，每种都有自己的适合的场景，常见的有下面这些： ReplicationController：比较原始的pod控制器，已经被废弃，由ReplicaSet替代 ReplicaSet：保证副本数量一直维持在期望值，并支持pod数量扩缩容，镜像版本升级 Deployment：通过控制ReplicaSet来控制Pod，并支持滚动升级、回退版本 Horizontal Pod Autoscaler：可以根据集群负载自动水平调整Pod的数量，实现削峰填谷 DaemonSet：在集群中的指定Node上运行且仅运行一个副本，一般用于守护进程类的任务 Job：它创建出来的pod只要完成任务就立即退出，不需要重启或重建，用于执行一次性任务 Cronjob：它创建的Pod负责周期性任务控制，不需要持续后台运行 StatefulSet：管理有状态应用 ","date":"2023-08-04","objectID":"/kubernetes/:25:0","tags":["devops","kubernetes"],"title":"Kubernetes使用手册 (资料来源于B站“冰糖没糖”的分享)","uri":"/kubernetes/"},{"categories":["笔记"],"content":"6.2 ReplicaSet(RS) ReplicaSet的主要作用是保证一定数量的pod正常运行，它会持续监听这些Pod的运行状态，一旦Pod发生故障，就会重启或重建。同时它还支持对pod数量的扩缩容和镜像版本的升降级。 ReplicaSet的资源清单文件： apiVersion: apps/v1 # 版本号 kind: ReplicaSet # 类型 metadata: # 元数据 name: # rs名称 namespace: # 所属命名空间 labels: #标签 controller: rs spec: # 详情描述 replicas: 3 # 副本数量 selector: # 选择器，通过它指定该控制器管理哪些pod matchLabels: # Labels匹配规则 app: nginx-pod matchExpressions: # Expressions匹配规则 - {key: app, operator: In, values: [nginx-pod]} template: # 模板，当副本数量不足时，会根据下面的模板创建pod副本 metadata: labels: app: nginx-pod spec: containers: - name: nginx image: nginx:1.17.1 ports: - containerPort: 80 在这里面，需要新了解的配置项就是 spec下面几个选项： replicas：指定副本数量，其实就是当前rs创建出来的pod的数量，默认为1 selector：选择器，它的作用是建立pod控制器和pod之间的关联关系，采用的Label Selector机制 在pod模板上定义label，在控制器上定义选择器，就可以表明当前控制器能管理哪些pod了 template：模板，就是当前控制器创建pod所使用的模板板，里面其实就是前一章学过的pod的定义 创建ReplicaSet 创建pc-replicaset.yaml文件，内容如下： apiVersion: apps/v1 kind: ReplicaSet metadata: name: pc-replicaset namespace: dev spec: replicas: 3 selector: matchLabels: app: nginx-pod template: metadata: labels: app: nginx-pod spec: containers: - name: nginx image: nginx:1.17.1 # 创建rs [root@k8s-master01 ~]# kubectl create -f pc-replicaset.yaml replicaset.apps/pc-replicaset created # 查看rs # DESIRED:期望副本数量 # CURRENT:当前副本数量 # READY:已经准备好提供服务的副本数量 [root@k8s-master01 ~]# kubectl get rs pc-replicaset -n dev -o wide NAME DESIRED CURRENT READY AGE CONTAINERS IMAGES SELECTOR pc-replicaset 3 3 3 22s nginx nginx:1.17.1 app=nginx-pod # 查看当前控制器创建出来的pod # 这里发现控制器创建出来的pod的名称是在控制器名称后面拼接了-xxxxx随机码 [root@k8s-master01 ~]# kubectl get pod -n dev NAME READY STATUS RESTARTS AGE pc-replicaset-6vmvt 1/1 Running 0 54s pc-replicaset-fmb8f 1/1 Running 0 54s pc-replicaset-snrk2 1/1 Running 0 54s 扩缩容 # 编辑rs的副本数量，修改spec:replicas: 6即可 [root@k8s-master01 ~]# kubectl edit rs pc-replicaset -n dev replicaset.apps/pc-replicaset edited # 查看pod [root@k8s-master01 ~]# kubectl get pods -n dev NAME READY STATUS RESTARTS AGE pc-replicaset-6vmvt 1/1 Running 0 114m pc-replicaset-cftnp 1/1 Running 0 10s pc-replicaset-fjlm6 1/1 Running 0 10s pc-replicaset-fmb8f 1/1 Running 0 114m pc-replicaset-s2whj 1/1 Running 0 10s pc-replicaset-snrk2 1/1 Running 0 114m # 当然也可以直接使用命令实现 # 使用scale命令实现扩缩容， 后面--replicas=n直接指定目标数量即可 [root@k8s-master01 ~]# kubectl scale rs pc-replicaset --replicas=2 -n dev replicaset.apps/pc-replicaset scaled # 命令运行完毕，立即查看，发现已经有4个开始准备退出了 [root@k8s-master01 ~]# kubectl get pods -n dev NAME READY STATUS RESTARTS AGE pc-replicaset-6vmvt 0/1 Terminating 0 118m pc-replicaset-cftnp 0/1 Terminating 0 4m17s pc-replicaset-fjlm6 0/1 Terminating 0 4m17s pc-replicaset-fmb8f 1/1 Running 0 118m pc-replicaset-s2whj 0/1 Terminating 0 4m17s pc-replicaset-snrk2 1/1 Running 0 118m #稍等片刻，就只剩下2个了 [root@k8s-master01 ~]# kubectl get pods -n dev NAME READY STATUS RESTARTS AGE pc-replicaset-fmb8f 1/1 Running 0 119m pc-replicaset-snrk2 1/1 Running 0 119m 镜像升级 # 编辑rs的容器镜像 - image: nginx:1.17.2 [root@k8s-master01 ~]# kubectl edit rs pc-replicaset -n dev replicaset.apps/pc-replicaset edited # 再次查看，发现镜像版本已经变更了 [root@k8s-master01 ~]# kubectl get rs -n dev -o wide NAME DESIRED CURRENT READY AGE CONTAINERS IMAGES ... pc-replicaset 2 2 2 140m nginx nginx:1.17.2 ... # 同样的道理，也可以使用命令完成这个工作 # kubectl set image rs rs名称 容器=镜像版本 -n namespace [root@k8s-master01 ~]# kubectl set image rs pc-replicaset nginx=nginx:1.17.1 -n dev replicaset.apps/pc-replicaset image updated # 再次查看，发现镜像版本已经变更了 [root@k8s-master01 ~]# kubectl get rs -n dev -o wide NAME DESIRED CURRENT READY AGE CONTAINERS IMAGES ... pc-replicaset 2 2 2 145m nginx nginx:1.17.1 ... 删除ReplicaSet # 使用kubectl delete命令会删除此RS以及它管理的Pod # 在kubernetes删除RS前，会将RS的replicasclear调整为0，等待所有的Pod被删除后，在执行RS对象的删除 [root@k8s-master01 ~]# kubectl delete rs pc-replicaset -n dev replicaset.apps \"pc-replicaset\" deleted [root@k8s-master01 ~]# kubectl get pod -n dev -o wide No resources found in dev namespace. # 如果希望仅仅删除RS对象（保留Pod），可以使用kubectl delete命令时添加--cascade=false选项（不推荐）。 [root@k8s-master01 ~]# kubectl delete rs pc-replicaset -n dev --cascade=false replicaset.app","date":"2023-08-04","objectID":"/kubernetes/:26:0","tags":["devops","kubernetes"],"title":"Kubernetes使用手册 (资料来源于B站“冰糖没糖”的分享)","uri":"/kubernetes/"},{"categories":["笔记"],"content":"6.3 Deployment(Deploy) 为了更好的解决服务编排的问题，kubernetes在V1.2版本开始，引入了Deployment控制器。值得一提的是，这种控制器并不直接管理pod，而是通过管理ReplicaSet来简介管理Pod，即：Deployment管理ReplicaSet，ReplicaSet管理Pod。所以Deployment比ReplicaSet功能更加强大。 Deployment主要功能有下面几个： 支持ReplicaSet的所有功能 支持发布的停止、继续 支持滚动升级和回滚版本 Deployment的资源清单文件： apiVersion: apps/v1 # 版本号 kind: Deployment # 类型 metadata: # 元数据 name: # rs名称 namespace: # 所属命名空间 labels: #标签 controller: deploy spec: # 详情描述 replicas: 3 # 副本数量 revisionHistoryLimit: 3 # 保留历史版本 paused: false # 暂停部署，默认是false progressDeadlineSeconds: 600 # 部署超时时间（s），默认是600 strategy: # 策略 type: RollingUpdate # 滚动更新策略 rollingUpdate: # 滚动更新 maxSurge: 30% # 最大额外可以存在的副本数，可以为百分比，也可以为整数 maxUnavailable: 30% # 最大不可用状态的 Pod 的最大值，可以为百分比，也可以为整数 selector: # 选择器，通过它指定该控制器管理哪些pod matchLabels: # Labels匹配规则 app: nginx-pod matchExpressions: # Expressions匹配规则 - {key: app, operator: In, values: [nginx-pod]} template: # 模板，当副本数量不足时，会根据下面的模板创建pod副本 metadata: labels: app: nginx-pod spec: containers: - name: nginx image: nginx:1.17.1 ports: - containerPort: 80 创建deployment 创建pc-deployment.yaml，内容如下： apiVersion: apps/v1 kind: Deployment metadata: name: pc-deployment namespace: dev spec: replicas: 3 selector: matchLabels: app: nginx-pod template: metadata: labels: app: nginx-pod spec: containers: - name: nginx image: nginx:1.17.1 # 创建deployment [root@k8s-master01 ~]# kubectl create -f pc-deployment.yaml --record=true deployment.apps/pc-deployment created # 查看deployment # UP-TO-DATE 最新版本的pod的数量 # AVAILABLE 当前可用的pod的数量 [root@k8s-master01 ~]# kubectl get deploy pc-deployment -n dev NAME READY UP-TO-DATE AVAILABLE AGE pc-deployment 3/3 3 3 15s # 查看rs # 发现rs的名称是在原来deployment的名字后面添加了一个10位数的随机串 [root@k8s-master01 ~]# kubectl get rs -n dev NAME DESIRED CURRENT READY AGE pc-deployment-6696798b78 3 3 3 23s # 查看pod [root@k8s-master01 ~]# kubectl get pods -n dev NAME READY STATUS RESTARTS AGE pc-deployment-6696798b78-d2c8n 1/1 Running 0 107s pc-deployment-6696798b78-smpvp 1/1 Running 0 107s pc-deployment-6696798b78-wvjd8 1/1 Running 0 107s 扩缩容 # 变更副本数量为5个 [root@k8s-master01 ~]# kubectl scale deploy pc-deployment --replicas=5 -n dev deployment.apps/pc-deployment scaled # 查看deployment [root@k8s-master01 ~]# kubectl get deploy pc-deployment -n dev NAME READY UP-TO-DATE AVAILABLE AGE pc-deployment 5/5 5 5 2m # 查看pod [root@k8s-master01 ~]# kubectl get pods -n dev NAME READY STATUS RESTARTS AGE pc-deployment-6696798b78-d2c8n 1/1 Running 0 4m19s pc-deployment-6696798b78-jxmdq 1/1 Running 0 94s pc-deployment-6696798b78-mktqv 1/1 Running 0 93s pc-deployment-6696798b78-smpvp 1/1 Running 0 4m19s pc-deployment-6696798b78-wvjd8 1/1 Running 0 4m19s # 编辑deployment的副本数量，修改spec:replicas: 4即可 [root@k8s-master01 ~]# kubectl edit deploy pc-deployment -n dev deployment.apps/pc-deployment edited # 查看pod [root@k8s-master01 ~]# kubectl get pods -n dev NAME READY STATUS RESTARTS AGE pc-deployment-6696798b78-d2c8n 1/1 Running 0 5m23s pc-deployment-6696798b78-jxmdq 1/1 Running 0 2m38s pc-deployment-6696798b78-smpvp 1/1 Running 0 5m23s pc-deployment-6696798b78-wvjd8 1/1 Running 0 5m23s 镜像更新 deployment支持两种更新策略:重建更新和 滚动更新,可以通过 strategy指定策略类型,支持两个属性: strategy：指定新的Pod替换旧的Pod的策略， 支持两个属性： type：指定策略类型，支持两种策略 Recreate：在创建出新的Pod之前会先杀掉所有已存在的Pod RollingUpdate：滚动更新，就是杀死一部分，就启动一部分，在更新过程中，存在两个版本Pod rollingUpdate：当type为RollingUpdate时生效，用于为RollingUpdate设置参数，支持两个属性： maxUnavailable：用来指定在升级过程中不可用Pod的最大数量，默认为25%。 maxSurge： 用来指定在升级过程中可以超过期望的Pod的最大数量，默认为25%。 重建更新 编辑pc-deployment.yaml,在spec节点下添加更新策略 spec: strategy: # 策略 type: Recreate # 重建更新 创建deploy进行验证 # 变更镜像 [root@k8s-master01 ~]# kubectl set image deployment pc-deployment nginx=nginx:1.17.2 -n dev deployment.apps/pc-deployment image updated # 观察升级过程 [root@k8s-master01 ~]# kubectl get pods -n dev -w NAME READY STATUS RESTARTS AGE pc-deployment-5d89bdfbf9-65qcw 1/1 Running 0 31s pc-deployment-5d89bdfbf9-w5nzv 1/1 Running 0 31s pc-deployment-5d89bdfbf9-xpt7w 1/1 Running 0 31s pc-deployment-5d89bdfbf9-xpt7w 1/1 Terminating 0 41s pc-deployment-5d89bdfbf9-65qcw 1/1 Terminating 0 41s pc-deployment-5d","date":"2023-08-04","objectID":"/kubernetes/:27:0","tags":["devops","kubernetes"],"title":"Kubernetes使用手册 (资料来源于B站“冰糖没糖”的分享)","uri":"/kubernetes/"},{"categories":["笔记"],"content":"6.4 Horizontal Pod Autoscaler(HPA) 在前面的课程中，我们已经可以实现通过手工执行 kubectl scale命令实现Pod扩容或缩容，但是这显然不符合Kubernetes的定位目标–自动化、智能化。 Kubernetes期望可以实现通过监测Pod的使用情况，实现pod数量的自动调整，于是就产生了Horizontal Pod Autoscaler（HPA）这种控制器。 HPA可以获取每个Pod利用率，然后和HPA中定义的指标进行对比，同时计算出需要伸缩的具体值，最后实现Pod的数量的调整。其实HPA与之前的Deployment一样，也属于一种Kubernetes资源对象，它通过追踪分析RC控制的所有目标Pod的负载变化情况，来确定是否需要针对性地调整目标Pod的副本数，这是HPA的实现原理。 接下来，我们来做一个实验 1 安装metrics-server metrics-server可以用来收集集群中的资源使用情况 # 安装git [root@k8s-master01 ~]# yum install git -y # 获取metrics-server, 注意使用的版本 [root@k8s-master01 ~]# git clone -b v0.3.6 https://github.com/kubernetes-incubator/metrics-server # 修改deployment, 注意修改的是镜像和初始化参数 [root@k8s-master01 ~]# cd /root/metrics-server/deploy/1.8+/ [root@k8s-master01 1.8+]# vim metrics-server-deployment.yaml 按图中添加下面选项 hostNetwork: true image: registry.cn-hangzhou.aliyuncs.com/google_containers/metrics-server-amd64:v0.3.6 args: - --kubelet-insecure-tls - --kubelet-preferred-address-types=InternalIP,Hostname,InternalDNS,ExternalDNS,ExternalIP # 安装metrics-server [root@k8s-master01 1.8+]# kubectl apply -f ./ # 查看pod运行情况 [root@k8s-master01 1.8+]# kubectl get pod -n kube-system metrics-server-6b976979db-2xwbj 1/1 Running 0 90s # 使用kubectl top node 查看资源使用情况 [root@k8s-master01 1.8+]# kubectl top node NAME CPU(cores) CPU% MEMORY(bytes) MEMORY% k8s-master01 289m 14% 1582Mi 54% k8s-node01 81m 4% 1195Mi 40% k8s-node02 72m 3% 1211Mi 41% [root@k8s-master01 1.8+]# kubectl top pod -n kube-system NAME CPU(cores) MEMORY(bytes) coredns-6955765f44-7ptsb 3m 9Mi coredns-6955765f44-vcwr5 3m 8Mi etcd-master 14m 145Mi ... # 至此,metrics-server安装完成 2 准备deployment和servie 创建pc-hpa-pod.yaml文件，内容如下： apiVersion: apps/v1 kind: Deployment metadata: name: nginx namespace: dev spec: strategy: # 策略 type: RollingUpdate # 滚动更新策略 replicas: 1 selector: matchLabels: app: nginx-pod template: metadata: labels: app: nginx-pod spec: containers: - name: nginx image: nginx:1.17.1 resources: # 资源配额 limits: # 限制资源（上限） cpu: \"1\" # CPU限制，单位是core数 requests: # 请求资源（下限） cpu: \"100m\" # CPU限制，单位是core数 # 创建service [root@k8s-master01 1.8+]# kubectl expose deployment nginx --type=NodePort --port=80 -n dev # 查看 [root@k8s-master01 1.8+]# kubectl get deployment,pod,svc -n dev NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/nginx 1/1 1 1 47s NAME READY STATUS RESTARTS AGE pod/nginx-7df9756ccc-bh8dr 1/1 Running 0 47s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/nginx NodePort 10.101.18.29 \u003cnone\u003e 80:31830/TCP 35s 3 部署HPA 创建pc-hpa.yaml文件，内容如下： apiVersion: autoscaling/v1 kind: HorizontalPodAutoscaler metadata: name: pc-hpa namespace: dev spec: minReplicas: 1 #最小pod数量 maxReplicas: 10 #最大pod数量 targetCPUUtilizationPercentage: 3 # CPU使用率指标 scaleTargetRef: # 指定要控制的nginx信息 apiVersion: apps/v1 kind: Deployment name: nginx # 创建hpa [root@k8s-master01 1.8+]# kubectl create -f pc-hpa.yaml horizontalpodautoscaler.autoscaling/pc-hpa created # 查看hpa [root@k8s-master01 1.8+]# kubectl get hpa -n dev NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE pc-hpa Deployment/nginx 0%/3% 1 10 1 62s 4 测试 使用压测工具对service地址 192.168.5.4:31830进行压测，然后通过控制台查看hpa和pod的变化 hpa变化 [root@k8s-master01 ~]# kubectl get hpa -n dev -w NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE pc-hpa Deployment/nginx 0%/3% 1 10 1 4m11s pc-hpa Deployment/nginx 0%/3% 1 10 1 5m19s pc-hpa Deployment/nginx 22%/3% 1 10 1 6m50s pc-hpa Deployment/nginx 22%/3% 1 10 4 7m5s pc-hpa Deployment/nginx 22%/3% 1 10 8 7m21s pc-hpa Deployment/nginx 6%/3% 1 10 8 7m51s pc-hpa Deployment/nginx 0%/3% 1 10 8 9m6s pc-hpa Deployment/nginx 0%/3% 1 10 8 13m pc-hpa Deployment/nginx 0%/3% 1 10 1 14m deployment变化 [root@k8s-master01 ~]# kubectl get deployment -n dev -w NAME READY UP-TO-DATE AVAILABLE AGE nginx 1/1 1 1 11m nginx 1/4 1 1 13m nginx 1/4 1 1 13m nginx 1/4 1 1 13m nginx 1/4 4 1 13m nginx 1/8 4 1 14m nginx 1/8 4 1 14m nginx 1/8 4 1 14m nginx 1/8 8 1 14m nginx 2/8 8 2 14m nginx 3/8 8 3 14m nginx 4/8 8 4 14m nginx 5/8 8 5 14m nginx 6/8 8 6 14m nginx 7/8 8 7 14m nginx 8/8 8 8 15m nginx 8/1 8 8 20m nginx 8/1 8 8 20m nginx","date":"2023-08-04","objectID":"/kubernetes/:28:0","tags":["devops","kubernetes"],"title":"Kubernetes使用手册 (资料来源于B站“冰糖没糖”的分享)","uri":"/kubernetes/"},{"categories":["笔记"],"content":"6.5 DaemonSet(DS) DaemonSet类型的控制器可以保证在集群中的每一台（或指定）节点上都运行一个副本。一般适用于日志收集、节点监控等场景。也就是说，如果一个Pod提供的功能是节点级别的（每个节点都需要且只需要一个），那么这类Pod就适合使用DaemonSet类型的控制器创建。 DaemonSet控制器的特点： 每当向集群中添加一个节点时，指定的 Pod 副本也将添加到该节点上 当节点从集群中移除时，Pod 也就被垃圾回收了 下面先来看下DaemonSet的资源清单文件 apiVersion: apps/v1 # 版本号 kind: DaemonSet # 类型 metadata: # 元数据 name: # rs名称 namespace: # 所属命名空间 labels: #标签 controller: daemonset spec: # 详情描述 revisionHistoryLimit: 3 # 保留历史版本 updateStrategy: # 更新策略 type: RollingUpdate # 滚动更新策略 rollingUpdate: # 滚动更新 maxUnavailable: 1 # 最大不可用状态的 Pod 的最大值，可以为百分比，也可以为整数 selector: # 选择器，通过它指定该控制器管理哪些pod matchLabels: # Labels匹配规则 app: nginx-pod matchExpressions: # Expressions匹配规则 - {key: app, operator: In, values: [nginx-pod]} template: # 模板，当副本数量不足时，会根据下面的模板创建pod副本 metadata: labels: app: nginx-pod spec: containers: - name: nginx image: nginx:1.17.1 ports: - containerPort: 80 创建pc-daemonset.yaml，内容如下： apiVersion: apps/v1 kind: DaemonSet metadata: name: pc-daemonset namespace: dev spec: selector: matchLabels: app: nginx-pod template: metadata: labels: app: nginx-pod spec: containers: - name: nginx image: nginx:1.17.1 # 创建daemonset [root@k8s-master01 ~]# kubectl create -f pc-daemonset.yaml daemonset.apps/pc-daemonset created # 查看daemonset [root@k8s-master01 ~]# kubectl get ds -n dev -o wide NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE AGE CONTAINERS IMAGES pc-daemonset 2 2 2 2 2 24s nginx nginx:1.17.1 # 查看pod,发现在每个Node上都运行一个pod [root@k8s-master01 ~]# kubectl get pods -n dev -o wide NAME READY STATUS RESTARTS AGE IP NODE pc-daemonset-9bck8 1/1 Running 0 37s 10.244.1.43 node1 pc-daemonset-k224w 1/1 Running 0 37s 10.244.2.74 node2 # 删除daemonset [root@k8s-master01 ~]# kubectl delete -f pc-daemonset.yaml daemonset.apps \"pc-daemonset\" deleted ","date":"2023-08-04","objectID":"/kubernetes/:29:0","tags":["devops","kubernetes"],"title":"Kubernetes使用手册 (资料来源于B站“冰糖没糖”的分享)","uri":"/kubernetes/"},{"categories":["笔记"],"content":"6.6 Job Job，主要用于负责**批量处理(一次要处理指定数量任务)短暂的一次性(每个任务仅运行一次就结束)**任务。Job特点如下： 当Job创建的pod执行成功结束时，Job将记录成功结束的pod数量 当成功结束的pod达到指定的数量时，Job将完成执行 Job的资源清单文件： apiVersion: batch/v1 # 版本号 kind: Job # 类型 metadata: # 元数据 name: # rs名称 namespace: # 所属命名空间 labels: #标签 controller: job spec: # 详情描述 completions: 1 # 指定job需要成功运行Pods的次数。默认值: 1 parallelism: 1 # 指定job在任一时刻应该并发运行Pods的数量。默认值: 1 activeDeadlineSeconds: 30 # 指定job可运行的时间期限，超过时间还未结束，系统将会尝试进行终止。 backoffLimit: 6 # 指定job失败后进行重试的次数。默认是6 manualSelector: true # 是否可以使用selector选择器选择pod，默认是false selector: # 选择器，通过它指定该控制器管理哪些pod matchLabels: # Labels匹配规则 app: counter-pod matchExpressions: # Expressions匹配规则 - {key: app, operator: In, values: [counter-pod]} template: # 模板，当副本数量不足时，会根据下面的模板创建pod副本 metadata: labels: app: counter-pod spec: restartPolicy: Never # 重启策略只能设置为Never或者OnFailure containers: - name: counter image: busybox:1.30 command: [\"bin/sh\",\"-c\",\"for i in 9 8 7 6 5 4 3 2 1; do echo $i;sleep 2;done\"] 关于重启策略设置的说明： 如果指定为OnFailure，则job会在pod出现故障时重启容器，而不是创建pod，failed次数不变 如果指定为Never，则job会在pod出现故障时创建新的pod，并且故障pod不会消失，也不会重启，failed次数加1 如果指定为Always的话，就意味着一直重启，意味着job任务会重复去执行了，当然不对，所以不能设置为Always 创建pc-job.yaml，内容如下： apiVersion: batch/v1 kind: Job metadata: name: pc-job namespace: dev spec: manualSelector: true selector: matchLabels: app: counter-pod template: metadata: labels: app: counter-pod spec: restartPolicy: Never containers: - name: counter image: busybox:1.30 command: [\"bin/sh\",\"-c\",\"for i in 9 8 7 6 5 4 3 2 1; do echo $i;sleep 3;done\"] # 创建job [root@k8s-master01 ~]# kubectl create -f pc-job.yaml job.batch/pc-job created # 查看job [root@k8s-master01 ~]# kubectl get job -n dev -o wide -w NAME COMPLETIONS DURATION AGE CONTAINERS IMAGES SELECTOR pc-job 0/1 21s 21s counter busybox:1.30 app=counter-pod pc-job 1/1 31s 79s counter busybox:1.30 app=counter-pod # 通过观察pod状态可以看到，pod在运行完毕任务后，就会变成Completed状态 [root@k8s-master01 ~]# kubectl get pods -n dev -w NAME READY STATUS RESTARTS AGE pc-job-rxg96 1/1 Running 0 29s pc-job-rxg96 0/1 Completed 0 33s # 接下来，调整下pod运行的总数量和并行数量 即：在spec下设置下面两个选项 # completions: 6 # 指定job需要成功运行Pods的次数为6 # parallelism: 3 # 指定job并发运行Pods的数量为3 # 然后重新运行job，观察效果，此时会发现，job会每次运行3个pod，总共执行了6个pod [root@k8s-master01 ~]# kubectl get pods -n dev -w NAME READY STATUS RESTARTS AGE pc-job-684ft 1/1 Running 0 5s pc-job-jhj49 1/1 Running 0 5s pc-job-pfcvh 1/1 Running 0 5s pc-job-684ft 0/1 Completed 0 11s pc-job-v7rhr 0/1 Pending 0 0s pc-job-v7rhr 0/1 Pending 0 0s pc-job-v7rhr 0/1 ContainerCreating 0 0s pc-job-jhj49 0/1 Completed 0 11s pc-job-fhwf7 0/1 Pending 0 0s pc-job-fhwf7 0/1 Pending 0 0s pc-job-pfcvh 0/1 Completed 0 11s pc-job-5vg2j 0/1 Pending 0 0s pc-job-fhwf7 0/1 ContainerCreating 0 0s pc-job-5vg2j 0/1 Pending 0 0s pc-job-5vg2j 0/1 ContainerCreating 0 0s pc-job-fhwf7 1/1 Running 0 2s pc-job-v7rhr 1/1 Running 0 2s pc-job-5vg2j 1/1 Running 0 3s pc-job-fhwf7 0/1 Completed 0 12s pc-job-v7rhr 0/1 Completed 0 12s pc-job-5vg2j 0/1 Completed 0 12s # 删除job [root@k8s-master01 ~]# kubectl delete -f pc-job.yaml job.batch \"pc-job\" deleted ","date":"2023-08-04","objectID":"/kubernetes/:30:0","tags":["devops","kubernetes"],"title":"Kubernetes使用手册 (资料来源于B站“冰糖没糖”的分享)","uri":"/kubernetes/"},{"categories":["笔记"],"content":"6.7 CronJob(CJ) CronJob控制器以Job控制器资源为其管控对象，并借助它管理pod资源对象，Job控制器定义的作业任务在其控制器资源创建之后便会立即执行，但CronJob可以以类似于Linux操作系统的周期性任务作业计划的方式控制其运行时间点及重复运行的方式。也就是说，CronJob可以在特定的时间点(反复的)去运行job任务。 CronJob的资源清单文件： apiVersion: batch/v1beta1 # 版本号 kind: CronJob # 类型 metadata: # 元数据 name: # rs名称 namespace: # 所属命名空间 labels: #标签 controller: cronjob spec: # 详情描述 schedule: # cron格式的作业调度运行时间点,用于控制任务在什么时间执行 concurrencyPolicy: # 并发执行策略，用于定义前一次作业运行尚未完成时是否以及如何运行后一次的作业 failedJobHistoryLimit: # 为失败的任务执行保留的历史记录数，默认为1 successfulJobHistoryLimit: # 为成功的任务执行保留的历史记录数，默认为3 startingDeadlineSeconds: # 启动作业错误的超时时长 jobTemplate: # job控制器模板，用于为cronjob控制器生成job对象;下面其实就是job的定义 metadata: spec: completions: 1 parallelism: 1 activeDeadlineSeconds: 30 backoffLimit: 6 manualSelector: true selector: matchLabels: app: counter-pod matchExpressions: 规则 - {key: app, operator: In, values: [counter-pod]} template: metadata: labels: app: counter-pod spec: restartPolicy: Never containers: - name: counter image: busybox:1.30 command: [\"bin/sh\",\"-c\",\"for i in 9 8 7 6 5 4 3 2 1; do echo $i;sleep 20;done\"] 需要重点解释的几个选项： schedule: cron表达式，用于指定任务的执行时间 */1 * * * * \u003c分钟\u003e \u003c小时\u003e \u003c日\u003e \u003c月份\u003e \u003c星期\u003e 分钟 值从 0 到 59. 小时 值从 0 到 23. 日 值从 1 到 31. 月 值从 1 到 12. 星期 值从 0 到 6, 0 代表星期日 多个时间可以用逗号隔开； 范围可以用连字符给出；*可以作为通配符； /表示每... concurrencyPolicy: Allow: 允许Jobs并发运行(默认) Forbid: 禁止并发运行，如果上一次运行尚未完成，则跳过下一次运行 Replace: 替换，取消当前正在运行的作业并用新作业替换它 创建pc-cronjob.yaml，内容如下： apiVersion: batch/v1beta1 kind: CronJob metadata: name: pc-cronjob namespace: dev labels: controller: cronjob spec: schedule: \"*/1 * * * *\" jobTemplate: metadata: spec: template: spec: restartPolicy: Never containers: - name: counter image: busybox:1.30 command: [\"bin/sh\",\"-c\",\"for i in 9 8 7 6 5 4 3 2 1; do echo $i;sleep 3;done\"] # 创建cronjob [root@k8s-master01 ~]# kubectl create -f pc-cronjob.yaml cronjob.batch/pc-cronjob created # 查看cronjob [root@k8s-master01 ~]# kubectl get cronjobs -n dev NAME SCHEDULE SUSPEND ACTIVE LAST SCHEDULE AGE pc-cronjob */1 * * * * False 0 \u003cnone\u003e 6s # 查看job [root@k8s-master01 ~]# kubectl get jobs -n dev NAME COMPLETIONS DURATION AGE pc-cronjob-1592587800 1/1 28s 3m26s pc-cronjob-1592587860 1/1 28s 2m26s pc-cronjob-1592587920 1/1 28s 86s # 查看pod [root@k8s-master01 ~]# kubectl get pods -n dev pc-cronjob-1592587800-x4tsm 0/1 Completed 0 2m24s pc-cronjob-1592587860-r5gv4 0/1 Completed 0 84s pc-cronjob-1592587920-9dxxq 1/1 Running 0 24s # 删除cronjob [root@k8s-master01 ~]# kubectl delete -f pc-cronjob.yaml cronjob.batch \"pc-cronjob\" deleted 7. Service详解 ","date":"2023-08-04","objectID":"/kubernetes/:31:0","tags":["devops","kubernetes"],"title":"Kubernetes使用手册 (资料来源于B站“冰糖没糖”的分享)","uri":"/kubernetes/"},{"categories":["笔记"],"content":"7.1 Service介绍 在kubernetes中，pod是应用程序的载体，我们可以通过pod的ip来访问应用程序，但是pod的ip地址不是固定的，这也就意味着不方便直接采用pod的ip对服务进行访问。 为了解决这个问题，kubernetes提供了Service资源，Service会对提供同一个服务的多个pod进行聚合，并且提供一个统一的入口地址。通过访问Service的入口地址就能访问到后面的pod服务。 Service在很多情况下只是一个概念，真正起作用的其实是kube-proxy服务进程，每个Node节点上都运行着一个kube-proxy服务进程。当创建Service的时候会通过api-server向etcd写入创建的service的信息，而kube-proxy会基于监听的机制发现这种Service的变动，然后它会将最新的Service信息转换成对应的访问规则。 # 10.97.97.97:80 是service提供的访问入口 # 当访问这个入口的时候，可以发现后面有三个pod的服务在等待调用， # kube-proxy会基于rr（轮询）的策略，将请求分发到其中一个pod上去 # 这个规则会同时在集群内的所有节点上都生成，所以在任何一个节点上访问都可以。 [root@node1 ~]# ipvsadm -Ln IP Virtual Server version 1.2.1 (size=4096) Prot LocalAddress:Port Scheduler Flags -\u003e RemoteAddress:Port Forward Weight ActiveConn InActConn TCP 10.97.97.97:80 rr -\u003e 10.244.1.39:80 Masq 1 0 0 -\u003e 10.244.1.40:80 Masq 1 0 0 -\u003e 10.244.2.33:80 Masq 1 0 0 kube-proxy目前支持三种工作模式: userspace 模式 userspace模式下，kube-proxy会为每一个Service创建一个监听端口，发向Cluster IP的请求被Iptables规则重定向到kube-proxy监听的端口上，kube-proxy根据LB算法选择一个提供服务的Pod并和其建立链接，以将请求转发到Pod上。 该模式下，kube-proxy充当了一个四层负责均衡器的角色。由于kube-proxy运行在userspace中，在进行转发处理时会增加内核和用户空间之间的数据拷贝，虽然比较稳定，但是效率比较低。 iptables 模式 iptables模式下，kube-proxy为service后端的每个Pod创建对应的iptables规则，直接将发向Cluster IP的请求重定向到一个Pod IP。 该模式下kube-proxy不承担四层负责均衡器的角色，只负责创建iptables规则。该模式的优点是较userspace模式效率更高，但不能提供灵活的LB策略，当后端Pod不可用时也无法进行重试。 ipvs 模式 ipvs模式和iptables类似，kube-proxy监控Pod的变化并创建相应的ipvs规则。ipvs相对iptables转发效率更高。除此以外，ipvs支持更多的LB算法。 # 此模式必须安装ipvs内核模块，否则会降级为iptables # 开启ipvs [root@k8s-master01 ~]# kubectl edit cm kube-proxy -n kube-system # 修改mode: \"ipvs\" [root@k8s-master01 ~]# kubectl delete pod -l k8s-app=kube-proxy -n kube-system [root@node1 ~]# ipvsadm -Ln IP Virtual Server version 1.2.1 (size=4096) Prot LocalAddress:Port Scheduler Flags -\u003e RemoteAddress:Port Forward Weight ActiveConn InActConn TCP 10.97.97.97:80 rr -\u003e 10.244.1.39:80 Masq 1 0 0 -\u003e 10.244.1.40:80 Masq 1 0 0 -\u003e 10.244.2.33:80 Masq 1 0 0 ","date":"2023-08-04","objectID":"/kubernetes/:32:0","tags":["devops","kubernetes"],"title":"Kubernetes使用手册 (资料来源于B站“冰糖没糖”的分享)","uri":"/kubernetes/"},{"categories":["笔记"],"content":"7.2 Service类型 Service的资源清单文件： kind: Service # 资源类型 apiVersion: v1 # 资源版本 metadata: # 元数据 name: service # 资源名称 namespace: dev # 命名空间 spec: # 描述 selector: # 标签选择器，用于确定当前service代理哪些pod app: nginx type: # Service类型，指定service的访问方式 clusterIP: # 虚拟服务的ip地址 sessionAffinity: # session亲和性，支持ClientIP、None两个选项 ports: # 端口信息 - protocol: TCP port: 3017 # service端口 targetPort: 5003 # pod端口 nodePort: 31122 # 主机端口 ClusterIP：默认值，它是Kubernetes系统自动分配的虚拟IP，只能在集群内部访问 NodePort：将Service通过指定的Node上的端口暴露给外部，通过此方法，就可以在集群外部访问服务 LoadBalancer：使用外接负载均衡器完成到服务的负载分发，注意此模式需要外部云环境支持 ExternalName： 把集群外部的服务引入集群内部，直接使用 ","date":"2023-08-04","objectID":"/kubernetes/:33:0","tags":["devops","kubernetes"],"title":"Kubernetes使用手册 (资料来源于B站“冰糖没糖”的分享)","uri":"/kubernetes/"},{"categories":["笔记"],"content":"7.3 Service使用 ","date":"2023-08-04","objectID":"/kubernetes/:34:0","tags":["devops","kubernetes"],"title":"Kubernetes使用手册 (资料来源于B站“冰糖没糖”的分享)","uri":"/kubernetes/"},{"categories":["笔记"],"content":"7.3.1 实验环境准备 在使用service之前，首先利用Deployment创建出3个pod，注意要为pod设置 app=nginx-pod的标签 创建deployment.yaml，内容如下： apiVersion: apps/v1 kind: Deployment metadata: name: pc-deployment namespace: dev spec: replicas: 3 selector: matchLabels: app: nginx-pod template: metadata: labels: app: nginx-pod spec: containers: - name: nginx image: nginx:1.17.1 ports: - containerPort: 80 [root@k8s-master01 ~]# kubectl create -f deployment.yaml deployment.apps/pc-deployment created # 查看pod详情 [root@k8s-master01 ~]# kubectl get pods -n dev -o wide --show-labels NAME READY STATUS IP NODE LABELS pc-deployment-66cb59b984-8p84h 1/1 Running 10.244.1.39 node1 app=nginx-pod pc-deployment-66cb59b984-vx8vx 1/1 Running 10.244.2.33 node2 app=nginx-pod pc-deployment-66cb59b984-wnncx 1/1 Running 10.244.1.40 node1 app=nginx-pod # 为了方便后面的测试，修改下三台nginx的index.html页面（三台修改的IP地址不一致） # kubectl exec -it pc-deployment-66cb59b984-8p84h -n dev /bin/sh # echo \"10.244.1.39\" \u003e /usr/share/nginx/html/index.html #修改完毕之后，访问测试 [root@k8s-master01 ~]# curl 10.244.1.39 10.244.1.39 [root@k8s-master01 ~]# curl 10.244.2.33 10.244.2.33 [root@k8s-master01 ~]# curl 10.244.1.40 10.244.1.40 ","date":"2023-08-04","objectID":"/kubernetes/:34:1","tags":["devops","kubernetes"],"title":"Kubernetes使用手册 (资料来源于B站“冰糖没糖”的分享)","uri":"/kubernetes/"},{"categories":["笔记"],"content":"7.3.2 ClusterIP类型的Service 创建service-clusterip.yaml文件 apiVersion: v1 kind: Service metadata: name: service-clusterip namespace: dev spec: selector: app: nginx-pod clusterIP: 10.97.97.97 # service的ip地址，如果不写，默认会生成一个 type: ClusterIP ports: - port: 80 # Service端口 targetPort: 80 # pod端口 # 创建service [root@k8s-master01 ~]# kubectl create -f service-clusterip.yaml service/service-clusterip created # 查看service [root@k8s-master01 ~]# kubectl get svc -n dev -o wide NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR service-clusterip ClusterIP 10.97.97.97 \u003cnone\u003e 80/TCP 13s app=nginx-pod # 查看service的详细信息 # 在这里有一个Endpoints列表，里面就是当前service可以负载到的服务入口 [root@k8s-master01 ~]# kubectl describe svc service-clusterip -n dev Name: service-clusterip Namespace: dev Labels: \u003cnone\u003e Annotations: \u003cnone\u003e Selector: app=nginx-pod Type: ClusterIP IP: 10.97.97.97 Port: \u003cunset\u003e 80/TCP TargetPort: 80/TCP Endpoints: 10.244.1.39:80,10.244.1.40:80,10.244.2.33:80 Session Affinity: None Events: \u003cnone\u003e # 查看ipvs的映射规则 [root@k8s-master01 ~]# ipvsadm -Ln TCP 10.97.97.97:80 rr -\u003e 10.244.1.39:80 Masq 1 0 0 -\u003e 10.244.1.40:80 Masq 1 0 0 -\u003e 10.244.2.33:80 Masq 1 0 0 # 访问10.97.97.97:80观察效果 [root@k8s-master01 ~]# curl 10.97.97.97:80 10.244.2.33 Endpoint Endpoint是kubernetes中的一个资源对象，存储在etcd中，用来记录一个service对应的所有pod的访问地址，它是根据service配置文件中selector描述产生的。 一个Service由一组Pod组成，这些Pod通过Endpoints暴露出来，Endpoints是实现实际服务的端点集合。换句话说，service和pod之间的联系是通过endpoints实现的。 负载分发策略 对Service的访问被分发到了后端的Pod上去，目前kubernetes提供了两种负载分发策略： 如果不定义，默认使用kube-proxy的策略，比如随机、轮询 基于客户端地址的会话保持模式，即来自同一个客户端发起的所有请求都会转发到固定的一个Pod上 此模式可以使在spec中添加 sessionAffinity:ClientIP选项 # 查看ipvs的映射规则【rr 轮询】 [root@k8s-master01 ~]# ipvsadm -Ln TCP 10.97.97.97:80 rr -\u003e 10.244.1.39:80 Masq 1 0 0 -\u003e 10.244.1.40:80 Masq 1 0 0 -\u003e 10.244.2.33:80 Masq 1 0 0 # 循环访问测试 [root@k8s-master01 ~]# while true;do curl 10.97.97.97:80; sleep 5; done; 10.244.1.40 10.244.1.39 10.244.2.33 10.244.1.40 10.244.1.39 10.244.2.33 # 修改分发策略----sessionAffinity:ClientIP # 查看ipvs规则【persistent 代表持久】 [root@k8s-master01 ~]# ipvsadm -Ln TCP 10.97.97.97:80 rr persistent 10800 -\u003e 10.244.1.39:80 Masq 1 0 0 -\u003e 10.244.1.40:80 Masq 1 0 0 -\u003e 10.244.2.33:80 Masq 1 0 0 # 循环访问测试 [root@k8s-master01 ~]# while true;do curl 10.97.97.97; sleep 5; done; 10.244.2.33 10.244.2.33 10.244.2.33 # 删除service [root@k8s-master01 ~]# kubectl delete -f service-clusterip.yaml service \"service-clusterip\" deleted ","date":"2023-08-04","objectID":"/kubernetes/:34:2","tags":["devops","kubernetes"],"title":"Kubernetes使用手册 (资料来源于B站“冰糖没糖”的分享)","uri":"/kubernetes/"},{"categories":["笔记"],"content":"7.3.3 HeadLiness类型的Service 在某些场景中，开发人员可能不想使用Service提供的负载均衡功能，而希望自己来控制负载均衡策略，针对这种情况，kubernetes提供了HeadLiness Service，这类Service不会分配Cluster IP，如果想要访问service，只能通过service的域名进行查询。 创建service-headliness.yaml apiVersion: v1 kind: Service metadata: name: service-headliness namespace: dev spec: selector: app: nginx-pod clusterIP: None # 将clusterIP设置为None，即可创建headliness Service type: ClusterIP ports: - port: 80 targetPort: 80 # 创建service [root@k8s-master01 ~]# kubectl create -f service-headliness.yaml service/service-headliness created # 获取service， 发现CLUSTER-IP未分配 [root@k8s-master01 ~]# kubectl get svc service-headliness -n dev -o wide NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR service-headliness ClusterIP None \u003cnone\u003e 80/TCP 11s app=nginx-pod # 查看service详情 [root@k8s-master01 ~]# kubectl describe svc service-headliness -n dev Name: service-headliness Namespace: dev Labels: \u003cnone\u003e Annotations: \u003cnone\u003e Selector: app=nginx-pod Type: ClusterIP IP: None Port: \u003cunset\u003e 80/TCP TargetPort: 80/TCP Endpoints: 10.244.1.39:80,10.244.1.40:80,10.244.2.33:80 Session Affinity: None Events: \u003cnone\u003e # 查看域名的解析情况 [root@k8s-master01 ~]# kubectl exec -it pc-deployment-66cb59b984-8p84h -n dev /bin/sh / # cat /etc/resolv.conf nameserver 10.96.0.10 search dev.svc.cluster.local svc.cluster.local cluster.local [root@k8s-master01 ~]# dig @10.96.0.10 service-headliness.dev.svc.cluster.local service-headliness.dev.svc.cluster.local. 30 IN A 10.244.1.40 service-headliness.dev.svc.cluster.local. 30 IN A 10.244.1.39 service-headliness.dev.svc.cluster.local. 30 IN A 10.244.2.33 ","date":"2023-08-04","objectID":"/kubernetes/:34:3","tags":["devops","kubernetes"],"title":"Kubernetes使用手册 (资料来源于B站“冰糖没糖”的分享)","uri":"/kubernetes/"},{"categories":["笔记"],"content":"7.3.4 NodePort类型的Service 在之前的样例中，创建的Service的ip地址只有集群内部才可以访问，如果希望将Service暴露给集群外部使用，那么就要使用到另外一种类型的Service，称为NodePort类型。NodePort的工作原理其实就是将service的端口映射到Node的一个端口上，然后就可以通过 NodeIp:NodePort来访问service了。 创建service-nodeport.yaml apiVersion: v1 kind: Service metadata: name: service-nodeport namespace: dev spec: selector: app: nginx-pod type: NodePort # service类型 ports: - port: 80 nodePort: 30002 # 指定绑定的node的端口(默认的取值范围是：30000-32767), 如果不指定，会默认分配 targetPort: 80 # 创建service [root@k8s-master01 ~]# kubectl create -f service-nodeport.yaml service/service-nodeport created # 查看service [root@k8s-master01 ~]# kubectl get svc -n dev -o wide NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) SELECTOR service-nodeport NodePort 10.105.64.191 \u003cnone\u003e 80:30002/TCP app=nginx-pod # 接下来可以通过电脑主机的浏览器去访问集群中任意一个nodeip的30002端口，即可访问到pod ","date":"2023-08-04","objectID":"/kubernetes/:34:4","tags":["devops","kubernetes"],"title":"Kubernetes使用手册 (资料来源于B站“冰糖没糖”的分享)","uri":"/kubernetes/"},{"categories":["笔记"],"content":"7.3.5 LoadBalancer类型的Service LoadBalancer和NodePort很相似，目的都是向外部暴露一个端口，区别在于LoadBalancer会在集群的外部再来做一个负载均衡设备，而这个设备需要外部环境支持的，外部服务发送到这个设备上的请求，会被设备负载之后转发到集群中。 ","date":"2023-08-04","objectID":"/kubernetes/:34:5","tags":["devops","kubernetes"],"title":"Kubernetes使用手册 (资料来源于B站“冰糖没糖”的分享)","uri":"/kubernetes/"},{"categories":["笔记"],"content":"7.3.6 ExternalName类型的Service ExternalName类型的Service用于引入集群外部的服务，它通过 externalName属性指定外部一个服务的地址，然后在集群内部访问此service就可以访问到外部的服务了。 apiVersion: v1 kind: Service metadata: name: service-externalname namespace: dev spec: type: ExternalName # service类型 externalName: www.baidu.com #改成ip地址也可以 # 创建service [root@k8s-master01 ~]# kubectl create -f service-externalname.yaml service/service-externalname created # 域名解析 [root@k8s-master01 ~]# dig @10.96.0.10 service-externalname.dev.svc.cluster.local service-externalname.dev.svc.cluster.local. 30 IN CNAME www.baidu.com. www.baidu.com. 30 IN CNAME www.a.shifen.com. www.a.shifen.com. 30 IN A 39.156.66.18 www.a.shifen.com. 30 IN A 39.156.66.14 ","date":"2023-08-04","objectID":"/kubernetes/:34:6","tags":["devops","kubernetes"],"title":"Kubernetes使用手册 (资料来源于B站“冰糖没糖”的分享)","uri":"/kubernetes/"},{"categories":["笔记"],"content":"7.4 Ingress介绍 在前面课程中已经提到，Service对集群之外暴露服务的主要方式有两种：NotePort和LoadBalancer，但是这两种方式，都有一定的缺点： NodePort方式的缺点是会占用很多集群机器的端口，那么当集群服务变多的时候，这个缺点就愈发明显 LB方式的缺点是每个service需要一个LB，浪费、麻烦，并且需要kubernetes之外设备的支持 基于这种现状，kubernetes提供了Ingress资源对象，Ingress只需要一个NodePort或者一个LB就可以满足暴露多个Service的需求。工作机制大致如下图表示： 实际上，Ingress相当于一个7层的负载均衡器，是kubernetes对反向代理的一个抽象，它的工作原理类似于Nginx，可以理解成在Ingress里建立诸多映射规则，Ingress Controller通过监听这些配置规则并转化成Nginx的反向代理配置 , 然后对外部提供服务。在这里有两个核心概念： ingress：kubernetes中的一个对象，作用是定义请求如何转发到service的规则 ingress controller：具体实现反向代理及负载均衡的程序，对ingress定义的规则进行解析，根据配置的规则来实现请求转发，实现方式有很多，比如Nginx, Contour, Haproxy等等 Ingress（以Nginx为例）的工作原理如下： 用户编写Ingress规则，说明哪个域名对应kubernetes集群中的哪个Service Ingress控制器动态感知Ingress服务规则的变化，然后生成一段对应的Nginx反向代理配置 Ingress控制器会将生成的Nginx配置写入到一个运行着的Nginx服务中，并动态更新 到此为止，其实真正在工作的就是一个Nginx了，内部配置了用户定义的请求转发规则 ","date":"2023-08-04","objectID":"/kubernetes/:35:0","tags":["devops","kubernetes"],"title":"Kubernetes使用手册 (资料来源于B站“冰糖没糖”的分享)","uri":"/kubernetes/"},{"categories":["笔记"],"content":"7.5 Ingress使用 ","date":"2023-08-04","objectID":"/kubernetes/:36:0","tags":["devops","kubernetes"],"title":"Kubernetes使用手册 (资料来源于B站“冰糖没糖”的分享)","uri":"/kubernetes/"},{"categories":["笔记"],"content":"7.5.1 环境准备 搭建ingress环境 # 创建文件夹 [root@k8s-master01 ~]# mkdir ingress-controller [root@k8s-master01 ~]# cd ingress-controller/ # 获取ingress-nginx，本次案例使用的是0.30版本 [root@k8s-master01 ingress-controller]# wget https://raw.githubusercontent.com/kubernetes/ingress-nginx/nginx-0.30.0/deploy/static/mandatory.yaml [root@k8s-master01 ingress-controller]# wget https://raw.githubusercontent.com/kubernetes/ingress-nginx/nginx-0.30.0/deploy/static/provider/baremetal/service-nodeport.yaml # 修改mandatory.yaml文件中的仓库 # 修改quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.30.0 # 为quay-mirror.qiniu.com/kubernetes-ingress-controller/nginx-ingress-controller:0.30.0 # 创建ingress-nginx [root@k8s-master01 ingress-controller]# kubectl apply -f ./ # 查看ingress-nginx [root@k8s-master01 ingress-controller]# kubectl get pod -n ingress-nginx NAME READY STATUS RESTARTS AGE pod/nginx-ingress-controller-fbf967dd5-4qpbp 1/1 Running 0 12h # 查看service [root@k8s-master01 ingress-controller]# kubectl get svc -n ingress-nginx NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE ingress-nginx NodePort 10.98.75.163 \u003cnone\u003e 80:32240/TCP,443:31335/TCP 11h 准备service和pod 为了后面的实验比较方便，创建如下图所示的模型 创建tomcat-nginx.yaml apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment namespace: dev spec: replicas: 3 selector: matchLabels: app: nginx-pod template: metadata: labels: app: nginx-pod spec: containers: - name: nginx image: nginx:1.17.1 ports: - containerPort: 80 --- apiVersion: apps/v1 kind: Deployment metadata: name: tomcat-deployment namespace: dev spec: replicas: 3 selector: matchLabels: app: tomcat-pod template: metadata: labels: app: tomcat-pod spec: containers: - name: tomcat image: tomcat:8.5-jre10-slim ports: - containerPort: 8080 --- apiVersion: v1 kind: Service metadata: name: nginx-service namespace: dev spec: selector: app: nginx-pod clusterIP: None type: ClusterIP ports: - port: 80 targetPort: 80 --- apiVersion: v1 kind: Service metadata: name: tomcat-service namespace: dev spec: selector: app: tomcat-pod clusterIP: None type: ClusterIP ports: - port: 8080 targetPort: 8080 # 创建 [root@k8s-master01 ~]# kubectl create -f tomcat-nginx.yaml # 查看 [root@k8s-master01 ~]# kubectl get svc -n dev NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE nginx-service ClusterIP None \u003cnone\u003e 80/TCP 48s tomcat-service ClusterIP None \u003cnone\u003e 8080/TCP 48s ","date":"2023-08-04","objectID":"/kubernetes/:36:1","tags":["devops","kubernetes"],"title":"Kubernetes使用手册 (资料来源于B站“冰糖没糖”的分享)","uri":"/kubernetes/"},{"categories":["笔记"],"content":"7.5.2 Http代理 创建ingress-http.yaml apiVersion: extensions/v1beta1 kind: Ingress metadata: name: ingress-http namespace: dev spec: rules: - host: nginx.itheima.com http: paths: - path: / backend: serviceName: nginx-service servicePort: 80 - host: tomcat.itheima.com http: paths: - path: / backend: serviceName: tomcat-service servicePort: 8080 # 创建 [root@k8s-master01 ~]# kubectl create -f ingress-http.yaml ingress.extensions/ingress-http created # 查看 [root@k8s-master01 ~]# kubectl get ing ingress-http -n dev NAME HOSTS ADDRESS PORTS AGE ingress-http nginx.itheima.com,tomcat.itheima.com 80 22s # 查看详情 [root@k8s-master01 ~]# kubectl describe ing ingress-http -n dev ... Rules: Host Path Backends ---- ---- -------- nginx.itheima.com / nginx-service:80 (10.244.1.96:80,10.244.1.97:80,10.244.2.112:80) tomcat.itheima.com / tomcat-service:8080(10.244.1.94:8080,10.244.1.95:8080,10.244.2.111:8080) ... # 接下来,在本地电脑上配置host文件,解析上面的两个域名到192.168.109.100(master)上 # 然后,就可以分别访问tomcat.itheima.com:32240 和 nginx.itheima.com:32240 查看效果了 ","date":"2023-08-04","objectID":"/kubernetes/:36:2","tags":["devops","kubernetes"],"title":"Kubernetes使用手册 (资料来源于B站“冰糖没糖”的分享)","uri":"/kubernetes/"},{"categories":["笔记"],"content":"7.5.3 Https代理 创建证书 # 生成证书 openssl req -x509 -sha256 -nodes -days 365 -newkey rsa:2048 -keyout tls.key -out tls.crt -subj \"/C=CN/ST=BJ/L=BJ/O=nginx/CN=itheima.com\" # 创建密钥 kubectl create secret tls tls-secret --key tls.key --cert tls.crt 创建ingress-https.yaml apiVersion: extensions/v1beta1 kind: Ingress metadata: name: ingress-https namespace: dev spec: tls: - hosts: - nginx.itheima.com - tomcat.itheima.com secretName: tls-secret # 指定秘钥 rules: - host: nginx.itheima.com http: paths: - path: / backend: serviceName: nginx-service servicePort: 80 - host: tomcat.itheima.com http: paths: - path: / backend: serviceName: tomcat-service servicePort: 8080 # 创建 [root@k8s-master01 ~]# kubectl create -f ingress-https.yaml ingress.extensions/ingress-https created # 查看 [root@k8s-master01 ~]# kubectl get ing ingress-https -n dev NAME HOSTS ADDRESS PORTS AGE ingress-https nginx.itheima.com,tomcat.itheima.com 10.104.184.38 80, 443 2m42s # 查看详情 [root@k8s-master01 ~]# kubectl describe ing ingress-https -n dev ... TLS: tls-secret terminates nginx.itheima.com,tomcat.itheima.com Rules: Host Path Backends ---- ---- -------- nginx.itheima.com / nginx-service:80 (10.244.1.97:80,10.244.1.98:80,10.244.2.119:80) tomcat.itheima.com / tomcat-service:8080(10.244.1.99:8080,10.244.2.117:8080,10.244.2.120:8080) ... # 下面可以通过浏览器访问https://nginx.itheima.com:31335 和 https://tomcat.itheima.com:31335来查看了 8. 数据存储 在前面已经提到，容器的生命周期可能很短，会被频繁地创建和销毁。那么容器在销毁时，保存在容器中的数据也会被清除。这种结果对用户来说，在某些情况下是不乐意看到的。为了持久化保存容器的数据，kubernetes引入了Volume的概念。 Volume是Pod中能够被多个容器访问的共享目录，它被定义在Pod上，然后被一个Pod里的多个容器挂载到具体的文件目录下，kubernetes通过Volume实现同一个Pod中不同容器之间的数据共享以及数据的持久化存储。Volume的生命容器不与Pod中单个容器的生命周期相关，当容器终止或者重启时，Volume中的数据也不会丢失。 kubernetes的Volume支持多种类型，比较常见的有下面几个： 简单存储：EmptyDir、HostPath、NFS 高级存储：PV、PVC 配置存储：ConfigMap、Secret ","date":"2023-08-04","objectID":"/kubernetes/:36:3","tags":["devops","kubernetes"],"title":"Kubernetes使用手册 (资料来源于B站“冰糖没糖”的分享)","uri":"/kubernetes/"},{"categories":["笔记"],"content":"8.1 基本存储 ","date":"2023-08-04","objectID":"/kubernetes/:37:0","tags":["devops","kubernetes"],"title":"Kubernetes使用手册 (资料来源于B站“冰糖没糖”的分享)","uri":"/kubernetes/"},{"categories":["笔记"],"content":"8.1.1 EmptyDir EmptyDir是最基础的Volume类型，一个EmptyDir就是Host上的一个空目录。 EmptyDir是在Pod被分配到Node时创建的，它的初始内容为空，并且无须指定宿主机上对应的目录文件，因为kubernetes会自动分配一个目录，当Pod销毁时， EmptyDir中的数据也会被永久删除。 EmptyDir用途如下： 临时空间，例如用于某些应用程序运行时所需的临时目录，且无须永久保留 一个容器需要从另一个容器中获取数据的目录（多容器共享目录） 接下来，通过一个容器之间文件共享的案例来使用一下EmptyDir。 在一个Pod中准备两个容器nginx和busybox，然后声明一个Volume分别挂在到两个容器的目录中，然后nginx容器负责向Volume中写日志，busybox中通过命令将日志内容读到控制台。 创建一个volume-emptydir.yaml apiVersion: v1 kind: Pod metadata: name: volume-emptydir namespace: dev spec: containers: - name: nginx image: nginx:1.17.1 ports: - containerPort: 80 volumeMounts: # 将logs-volume挂在到nginx容器中，对应的目录为 /var/log/nginx - name: logs-volume mountPath: /var/log/nginx - name: busybox image: busybox:1.30 command: [\"/bin/sh\",\"-c\",\"tail -f /logs/access.log\"] # 初始命令，动态读取指定文件中内容 volumeMounts: # 将logs-volume 挂在到busybox容器中，对应的目录为 /logs - name: logs-volume mountPath: /logs volumes: # 声明volume， name为logs-volume，类型为emptyDir - name: logs-volume emptyDir: {} # 创建Pod [root@k8s-master01 ~]# kubectl create -f volume-emptydir.yaml pod/volume-emptydir created # 查看pod [root@k8s-master01 ~]# kubectl get pods volume-emptydir -n dev -o wide NAME READY STATUS RESTARTS AGE IP NODE ...... volume-emptydir 2/2 Running 0 97s 10.42.2.9 node1 ...... # 通过podIp访问nginx [root@k8s-master01 ~]# curl 10.42.2.9 ...... # 通过kubectl logs命令查看指定容器的标准输出 [root@k8s-master01 ~]# kubectl logs -f volume-emptydir -n dev -c busybox 10.42.1.0 - - [27/Jun/2021:15:08:54 +0000] \"GET / HTTP/1.1\" 200 612 \"-\" \"curl/7.29.0\" \"-\" ","date":"2023-08-04","objectID":"/kubernetes/:37:1","tags":["devops","kubernetes"],"title":"Kubernetes使用手册 (资料来源于B站“冰糖没糖”的分享)","uri":"/kubernetes/"},{"categories":["笔记"],"content":"8.1.2 HostPath 上节课提到，EmptyDir中数据不会被持久化，它会随着Pod的结束而销毁，如果想简单的将数据持久化到主机中，可以选择HostPath。 HostPath就是将Node主机中一个实际目录挂在到Pod中，以供容器使用，这样的设计就可以保证Pod销毁了，但是数据依据可以存在于Node主机上。 创建一个volume-hostpath.yaml： apiVersion: v1 kind: Pod metadata: name: volume-hostpath namespace: dev spec: containers: - name: nginx image: nginx:1.17.1 ports: - containerPort: 80 volumeMounts: - name: logs-volume mountPath: /var/log/nginx - name: busybox image: busybox:1.30 command: [\"/bin/sh\",\"-c\",\"tail -f /logs/access.log\"] volumeMounts: - name: logs-volume mountPath: /logs volumes: - name: logs-volume hostPath: path: /root/logs type: DirectoryOrCreate # 目录存在就使用，不存在就先创建后使用 关于type的值的一点说明： DirectoryOrCreate 目录存在就使用，不存在就先创建后使用 Directory 目录必须存在 FileOrCreate 文件存在就使用，不存在就先创建后使用 File 文件必须存在 Socket unix套接字必须存在 CharDevice 字符设备必须存在 BlockDevice 块设备必须存在 # 创建Pod [root@k8s-master01 ~]# kubectl create -f volume-hostpath.yaml pod/volume-hostpath created # 查看Pod [root@k8s-master01 ~]# kubectl get pods volume-hostpath -n dev -o wide NAME READY STATUS RESTARTS AGE IP NODE ...... pod-volume-hostpath 2/2 Running 0 16s 10.42.2.10 node1 ...... #访问nginx [root@k8s-master01 ~]# curl 10.42.2.10 # 接下来就可以去host的/root/logs目录下查看存储的文件了 ### 注意: 下面的操作需要到Pod所在的节点运行（案例中是node1） [root@node1 ~]# ls /root/logs/ access.log error.log # 同样的道理，如果在此目录下创建一个文件，到容器中也是可以看到的 ","date":"2023-08-04","objectID":"/kubernetes/:37:2","tags":["devops","kubernetes"],"title":"Kubernetes使用手册 (资料来源于B站“冰糖没糖”的分享)","uri":"/kubernetes/"},{"categories":["笔记"],"content":"8.1.3 NFS HostPath可以解决数据持久化的问题，但是一旦Node节点故障了，Pod如果转移到了别的节点，又会出现问题了，此时需要准备单独的网络存储系统，比较常用的用NFS、CIFS。 NFS是一个网络文件存储系统，可以搭建一台NFS服务器，然后将Pod中的存储直接连接到NFS系统上，这样的话，无论Pod在节点上怎么转移，只要Node跟NFS的对接没问题，数据就可以成功访问。 1）首先要准备nfs的服务器，这里为了简单，直接是master节点做nfs服务器 # 在nfs上安装nfs服务 [root@nfs ~]# yum install nfs-utils -y # 准备一个共享目录 [root@nfs ~]# mkdir /root/data/nfs -pv # 将共享目录以读写权限暴露给192.168.5.0/24网段中的所有主机 [root@nfs ~]# vim /etc/exports [root@nfs ~]# more /etc/exports /root/data/nfs 192.168.5.0/24(rw,no_root_squash) # 启动nfs服务 [root@nfs ~]# systemctl restart nfs 2）接下来，要在的每个node节点上都安装下nfs，这样的目的是为了node节点可以驱动nfs设备 # 在node上安装nfs服务，注意不需要启动 [root@k8s-master01 ~]# yum install nfs-utils -y 3）接下来，就可以编写pod的配置文件了，创建volume-nfs.yaml apiVersion: v1 kind: Pod metadata: name: volume-nfs namespace: dev spec: containers: - name: nginx image: nginx:1.17.1 ports: - containerPort: 80 volumeMounts: - name: logs-volume mountPath: /var/log/nginx - name: busybox image: busybox:1.30 command: [\"/bin/sh\",\"-c\",\"tail -f /logs/access.log\"] volumeMounts: - name: logs-volume mountPath: /logs volumes: - name: logs-volume nfs: server: 192.168.5.6 #nfs服务器地址 path: /root/data/nfs #共享文件路径 4）最后，运行下pod，观察结果 # 创建pod [root@k8s-master01 ~]# kubectl create -f volume-nfs.yaml pod/volume-nfs created # 查看pod [root@k8s-master01 ~]# kubectl get pods volume-nfs -n dev NAME READY STATUS RESTARTS AGE volume-nfs 2/2 Running 0 2m9s # 查看nfs服务器上的共享目录，发现已经有文件了 [root@k8s-master01 ~]# ls /root/data/ access.log error.log ","date":"2023-08-04","objectID":"/kubernetes/:37:3","tags":["devops","kubernetes"],"title":"Kubernetes使用手册 (资料来源于B站“冰糖没糖”的分享)","uri":"/kubernetes/"},{"categories":["笔记"],"content":"8.2 高级存储 前面已经学习了使用NFS提供存储，此时就要求用户会搭建NFS系统，并且会在yaml配置nfs。由于kubernetes支持的存储系统有很多，要求客户全都掌握，显然不现实。为了能够屏蔽底层存储实现的细节，方便用户使用， kubernetes引入PV和PVC两种资源对象。 PV（Persistent Volume）是持久化卷的意思，是对底层的共享存储的一种抽象。一般情况下PV由kubernetes管理员进行创建和配置，它与底层具体的共享存储技术有关，并通过插件完成与共享存储的对接。 PVC（Persistent Volume Claim）是持久卷声明的意思，是用户对于存储需求的一种声明。换句话说，PVC其实就是用户向kubernetes系统发出的一种资源需求申请。 使用了PV和PVC之后，工作可以得到进一步的细分： 存储：存储工程师维护 PV： kubernetes管理员维护 PVC：kubernetes用户维护 ","date":"2023-08-04","objectID":"/kubernetes/:38:0","tags":["devops","kubernetes"],"title":"Kubernetes使用手册 (资料来源于B站“冰糖没糖”的分享)","uri":"/kubernetes/"},{"categories":["笔记"],"content":"8.2.1 PV PV是存储资源的抽象，下面是资源清单文件: apiVersion: v1 kind: PersistentVolume metadata: name: pv2 spec: nfs: # 存储类型，与底层真正存储对应 capacity: # 存储能力，目前只支持存储空间的设置 storage: 2Gi accessModes: # 访问模式 storageClassName: # 存储类别 persistentVolumeReclaimPolicy: # 回收策略 PV 的关键配置参数说明： 存储类型 底层实际存储的类型，kubernetes支持多种存储类型，每种存储类型的配置都有所差异 存储能力（capacity） 目前只支持存储空间的设置( storage=1Gi )，不过未来可能会加入IOPS、吞吐量等指标的配置 访问模式（accessModes） 用于描述用户应用对存储资源的访问权限，访问权限包括下面几种方式： ReadWriteOnce（RWO）：读写权限，但是只能被单个节点挂载 ReadOnlyMany（ROX）： 只读权限，可以被多个节点挂载 ReadWriteMany（RWX）：读写权限，可以被多个节点挂载 需要注意的是，底层不同的存储类型可能支持的访问模式不同 回收策略（persistentVolumeReclaimPolicy） 当PV不再被使用了之后，对其的处理方式。目前支持三种策略： Retain （保留） 保留数据，需要管理员手工清理数据 Recycle（回收） 清除 PV 中的数据，效果相当于执行 rm -rf /thevolume/* Delete （删除） 与 PV 相连的后端存储完成 volume 的删除操作，当然这常见于云服务商的存储服务 需要注意的是，底层不同的存储类型可能支持的回收策略不同 存储类别 PV可以通过storageClassName参数指定一个存储类别 具有特定类别的PV只能与请求了该类别的PVC进行绑定 未设定类别的PV则只能与不请求任何类别的PVC进行绑定 状态（status） 一个 PV 的生命周期中，可能会处于4中不同的阶段： Available（可用）： 表示可用状态，还未被任何 PVC 绑定 Bound（已绑定）： 表示 PV 已经被 PVC 绑定 Released（已释放）： 表示 PVC 被删除，但是资源还未被集群重新声明 Failed（失败）： 表示该 PV 的自动回收失败 实验 使用NFS作为存储，来演示PV的使用，创建3个PV，对应NFS中的3个暴露的路径。 准备NFS环境 # 创建目录 [root@nfs ~]# mkdir /root/data/{pv1,pv2,pv3} -pv # 暴露服务 [root@nfs ~]# more /etc/exports /root/data/pv1 192.168.5.0/24(rw,no_root_squash) /root/data/pv2 192.168.5.0/24(rw,no_root_squash) /root/data/pv3 192.168.5.0/24(rw,no_root_squash) # 重启服务 [root@nfs ~]# systemctl restart nfs 创建pv.yaml apiVersion: v1 kind: PersistentVolume metadata: name: pv1 spec: capacity: storage: 1Gi accessModes: - ReadWriteMany persistentVolumeReclaimPolicy: Retain nfs: path: /root/data/pv1 server: 192.168.5.6 --- apiVersion: v1 kind: PersistentVolume metadata: name: pv2 spec: capacity: storage: 2Gi accessModes: - ReadWriteMany persistentVolumeReclaimPolicy: Retain nfs: path: /root/data/pv2 server: 192.168.5.6 --- apiVersion: v1 kind: PersistentVolume metadata: name: pv3 spec: capacity: storage: 3Gi accessModes: - ReadWriteMany persistentVolumeReclaimPolicy: Retain nfs: path: /root/data/pv3 server: 192.168.5.6 # 创建 pv [root@k8s-master01 ~]# kubectl create -f pv.yaml persistentvolume/pv1 created persistentvolume/pv2 created persistentvolume/pv3 created # 查看pv [root@k8s-master01 ~]# kubectl get pv -o wide NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS AGE VOLUMEMODE pv1 1Gi RWX Retain Available 10s Filesystem pv2 2Gi RWX Retain Available 10s Filesystem pv3 3Gi RWX Retain Available 9s Filesystem ","date":"2023-08-04","objectID":"/kubernetes/:38:1","tags":["devops","kubernetes"],"title":"Kubernetes使用手册 (资料来源于B站“冰糖没糖”的分享)","uri":"/kubernetes/"},{"categories":["笔记"],"content":"8.2.2 PVC PVC是资源的申请，用来声明对存储空间、访问模式、存储类别需求信息。下面是资源清单文件: apiVersion: v1 kind: PersistentVolumeClaim metadata: name: pvc namespace: dev spec: accessModes: # 访问模式 selector: # 采用标签对PV选择 storageClassName: # 存储类别 resources: # 请求空间 requests: storage: 5Gi PVC 的关键配置参数说明： 访问模式（accessModes） 用于描述用户应用对存储资源的访问权限 选择条件（selector） 通过Label Selector的设置，可使PVC对于系统中己存在的PV进行筛选 存储类别（storageClassName） PVC在定义时可以设定需要的后端存储的类别，只有设置了该class的pv才能被系统选出 资源请求（Resources ） 描述对存储资源的请求 实验 创建pvc.yaml，申请pv apiVersion: v1 kind: PersistentVolumeClaim metadata: name: pvc1 namespace: dev spec: accessModes: - ReadWriteMany resources: requests: storage: 1Gi --- apiVersion: v1 kind: PersistentVolumeClaim metadata: name: pvc2 namespace: dev spec: accessModes: - ReadWriteMany resources: requests: storage: 1Gi --- apiVersion: v1 kind: PersistentVolumeClaim metadata: name: pvc3 namespace: dev spec: accessModes: - ReadWriteMany resources: requests: storage: 1Gi # 创建pvc [root@k8s-master01 ~]# kubectl create -f pvc.yaml persistentvolumeclaim/pvc1 created persistentvolumeclaim/pvc2 created persistentvolumeclaim/pvc3 created # 查看pvc [root@k8s-master01 ~]# kubectl get pvc -n dev -o wide NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE VOLUMEMODE pvc1 Bound pv1 1Gi RWX 15s Filesystem pvc2 Bound pv2 2Gi RWX 15s Filesystem pvc3 Bound pv3 3Gi RWX 15s Filesystem # 查看pv [root@k8s-master01 ~]# kubectl get pv -o wide NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM AGE VOLUMEMODE pv1 1Gi RWx Retain Bound dev/pvc1 3h37m Filesystem pv2 2Gi RWX Retain Bound dev/pvc2 3h37m Filesystem pv3 3Gi RWX Retain Bound dev/pvc3 3h37m Filesystem 创建pods.yaml, 使用pv apiVersion: v1 kind: Pod metadata: name: pod1 namespace: dev spec: containers: - name: busybox image: busybox:1.30 command: [\"/bin/sh\",\"-c\",\"while true;do echo pod1 \u003e\u003e /root/out.txt; sleep 10; done;\"] volumeMounts: - name: volume mountPath: /root/ volumes: - name: volume persistentVolumeClaim: claimName: pvc1 readOnly: false --- apiVersion: v1 kind: Pod metadata: name: pod2 namespace: dev spec: containers: - name: busybox image: busybox:1.30 command: [\"/bin/sh\",\"-c\",\"while true;do echo pod2 \u003e\u003e /root/out.txt; sleep 10; done;\"] volumeMounts: - name: volume mountPath: /root/ volumes: - name: volume persistentVolumeClaim: claimName: pvc2 readOnly: false # 创建pod [root@k8s-master01 ~]# kubectl create -f pods.yaml pod/pod1 created pod/pod2 created # 查看pod [root@k8s-master01 ~]# kubectl get pods -n dev -o wide NAME READY STATUS RESTARTS AGE IP NODE pod1 1/1 Running 0 14s 10.244.1.69 node1 pod2 1/1 Running 0 14s 10.244.1.70 node1 # 查看pvc [root@k8s-master01 ~]# kubectl get pvc -n dev -o wide NAME STATUS VOLUME CAPACITY ACCESS MODES AGE VOLUMEMODE pvc1 Bound pv1 1Gi RWX 94m Filesystem pvc2 Bound pv2 2Gi RWX 94m Filesystem pvc3 Bound pv3 3Gi RWX 94m Filesystem # 查看pv [root@k8s-master01 ~]# kubectl get pv -n dev -o wide NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM AGE VOLUMEMODE pv1 1Gi RWX Retain Bound dev/pvc1 5h11m Filesystem pv2 2Gi RWX Retain Bound dev/pvc2 5h11m Filesystem pv3 3Gi RWX Retain Bound dev/pvc3 5h11m Filesystem # 查看nfs中的文件存储 [root@nfs ~]# more /root/data/pv1/out.txt node1 node1 [root@nfs ~]# more /root/data/pv2/out.txt node2 node2 ","date":"2023-08-04","objectID":"/kubernetes/:38:2","tags":["devops","kubernetes"],"title":"Kubernetes使用手册 (资料来源于B站“冰糖没糖”的分享)","uri":"/kubernetes/"},{"categories":["笔记"],"content":"8.2.3 生命周期 PVC和PV是一一对应的，PV和PVC之间的相互作用遵循以下生命周期： 资源供应：管理员手动创建底层存储和PV 资源绑定：用户创建PVC，kubernetes负责根据PVC的声明去寻找PV，并绑定 在用户定义好PVC之后，系统将根据PVC对存储资源的请求在已存在的PV中选择一个满足条件的 一旦找到，就将该PV与用户定义的PVC进行绑定，用户的应用就可以使用这个PVC了 如果找不到，PVC则会无限期处于Pending状态，直到等到系统管理员创建了一个符合其要求的PV PV一旦绑定到某个PVC上，就会被这个PVC独占，不能再与其他PVC进行绑定了 资源使用：用户可在pod中像volume一样使用pvc Pod使用Volume的定义，将PVC挂载到容器内的某个路径进行使用。 资源释放：用户删除pvc来释放pv 当存储资源使用完毕后，用户可以删除PVC，与该PVC绑定的PV将会被标记为“已释放”，但还不能立刻与其他PVC进行绑定。通过之前PVC写入的数据可能还被留在存储设备上，只有在清除之后该PV才能再次使用。 资源回收：kubernetes根据pv设置的回收策略进行资源的回收 对于PV，管理员可以设定回收策略，用于设置与之绑定的PVC释放资源之后如何处理遗留数据的问题。只有PV的存储空间完成回收，才能供新的PVC绑定和使用 ","date":"2023-08-04","objectID":"/kubernetes/:38:3","tags":["devops","kubernetes"],"title":"Kubernetes使用手册 (资料来源于B站“冰糖没糖”的分享)","uri":"/kubernetes/"},{"categories":["笔记"],"content":"8.3 配置存储 ","date":"2023-08-04","objectID":"/kubernetes/:39:0","tags":["devops","kubernetes"],"title":"Kubernetes使用手册 (资料来源于B站“冰糖没糖”的分享)","uri":"/kubernetes/"},{"categories":["笔记"],"content":"8.3.1 ConfigMap ConfigMap是一种比较特殊的存储卷，它的主要作用是用来存储配置信息的。 创建configmap.yaml，内容如下： apiVersion: v1 kind: ConfigMap metadata: name: configmap namespace: dev data: info: | username:admin password:123456 接下来，使用此配置文件创建configmap # 创建configmap [root@k8s-master01 ~]# kubectl create -f configmap.yaml configmap/configmap created # 查看configmap详情 [root@k8s-master01 ~]# kubectl describe cm configmap -n dev Name: configmap Namespace: dev Labels: \u003cnone\u003e Annotations: \u003cnone\u003e Data ==== info: ---- username:admin password:123456 Events: \u003cnone\u003e 接下来创建一个pod-configmap.yaml，将上面创建的configmap挂载进去 apiVersion: v1 kind: Pod metadata: name: pod-configmap namespace: dev spec: containers: - name: nginx image: nginx:1.17.1 volumeMounts: # 将configmap挂载到目录 - name: config mountPath: /configmap/config volumes: # 引用configmap - name: config configMap: name: configmap # 创建pod [root@k8s-master01 ~]# kubectl create -f pod-configmap.yaml pod/pod-configmap created # 查看pod [root@k8s-master01 ~]# kubectl get pod pod-configmap -n dev NAME READY STATUS RESTARTS AGE pod-configmap 1/1 Running 0 6s #进入容器 [root@k8s-master01 ~]# kubectl exec -it pod-configmap -n dev /bin/sh # cd /configmap/config/ # ls info # more info username:admin password:123456 # 可以看到映射已经成功，每个configmap都映射成了一个目录 # key---\u003e文件 value----\u003e文件中的内容 # 此时如果更新configmap的内容, 容器中的值也会动态更新 ","date":"2023-08-04","objectID":"/kubernetes/:39:1","tags":["devops","kubernetes"],"title":"Kubernetes使用手册 (资料来源于B站“冰糖没糖”的分享)","uri":"/kubernetes/"},{"categories":["笔记"],"content":"8.3.2 Secret 在kubernetes中，还存在一种和ConfigMap非常类似的对象，称为Secret对象。它主要用于存储敏感信息，例如密码、秘钥、证书等等。 首先使用base64对数据进行编码 [root@k8s-master01 ~]# echo -n 'admin' | base64 #准备username YWRtaW4= [root@k8s-master01 ~]# echo -n '123456' | base64 #准备password MTIzNDU2 接下来编写secret.yaml，并创建Secret apiVersion: v1 kind: Secret metadata: name: secret namespace: dev type: Opaque data: username: YWRtaW4= password: MTIzNDU2 # 创建secret [root@k8s-master01 ~]# kubectl create -f secret.yaml secret/secret created # 查看secret详情 [root@k8s-master01 ~]# kubectl describe secret secret -n dev Name: secret Namespace: dev Labels: \u003cnone\u003e Annotations: \u003cnone\u003e Type: Opaque Data ==== password: 6 bytes username: 5 bytes 创建pod-secret.yaml，将上面创建的secret挂载进去： apiVersion: v1 kind: Pod metadata: name: pod-secret namespace: dev spec: containers: - name: nginx image: nginx:1.17.1 volumeMounts: # 将secret挂载到目录 - name: config mountPath: /secret/config volumes: - name: config secret: secretName: secret # 创建pod [root@k8s-master01 ~]# kubectl create -f pod-secret.yaml pod/pod-secret created # 查看pod [root@k8s-master01 ~]# kubectl get pod pod-secret -n dev NAME READY STATUS RESTARTS AGE pod-secret 1/1 Running 0 2m28s # 进入容器，查看secret信息，发现已经自动解码了 [root@k8s-master01 ~]# kubectl exec -it pod-secret /bin/sh -n dev / # ls /secret/config/ password username / # more /secret/config/username admin / # more /secret/config/password 123456 至此，已经实现了利用secret实现了信息的编码。 9. 安全认证 ","date":"2023-08-04","objectID":"/kubernetes/:39:2","tags":["devops","kubernetes"],"title":"Kubernetes使用手册 (资料来源于B站“冰糖没糖”的分享)","uri":"/kubernetes/"},{"categories":["笔记"],"content":"9.1 访问控制概述 Kubernetes作为一个分布式集群的管理工具，保证集群的安全性是其一个重要的任务。所谓的安全性其实就是保证对Kubernetes的各种客户端进行认证和鉴权操作。 客户端 在Kubernetes集群中，客户端通常有两类： User Account：一般是独立于kubernetes之外的其他服务管理的用户账号。 Service Account：kubernetes管理的账号，用于为Pod中的服务进程在访问Kubernetes时提供身份标识。 认证、授权与准入控制 ApiServer是访问及管理资源对象的唯一入口。任何一个请求访问ApiServer，都要经过下面三个流程： Authentication（认证）：身份鉴别，只有正确的账号才能够通过认证 Authorization（授权）： 判断用户是否有权限对访问的资源执行特定的动作 Admission Control（准入控制）：用于补充授权机制以实现更加精细的访问控制功能。 ","date":"2023-08-04","objectID":"/kubernetes/:40:0","tags":["devops","kubernetes"],"title":"Kubernetes使用手册 (资料来源于B站“冰糖没糖”的分享)","uri":"/kubernetes/"},{"categories":["笔记"],"content":"9.2 认证管理 Kubernetes集群安全的最关键点在于如何识别并认证客户端身份，它提供了3种客户端身份认证方式： HTTP Base认证：通过用户名+密码的方式认证 这种认证方式是把“用户名:密码”用BASE64算法进行编码后的字符串放在HTTP请求中的Header Authorization域里发送给服务端。服务端收到后进行解码，获取用户名及密码，然后进行用户身份认证的过程。 HTTP Token认证：通过一个Token来识别合法用户 这种认证方式是用一个很长的难以被模仿的字符串--Token来表明客户身份的一种方式。每个Token对应一个用户名，当客户端发起API调用请求时，需要在HTTP Header里放入Token，API Server接到Token后会跟服务器中保存的token进行比对，然后进行用户身份认证的过程。 HTTPS证书认证：基于CA根证书签名的双向数字证书认证方式 这种认证方式是安全性最高的一种方式，但是同时也是操作起来最麻烦的一种方式。 HTTPS认证大体分为3个过程： 证书申请和下发 HTTPS通信双方的服务器向CA机构申请证书，CA机构下发根证书、服务端证书及私钥给申请者 客户端和服务端的双向认证 1\u003e 客户端向服务器端发起请求，服务端下发自己的证书给客户端， 客户端接收到证书后，通过私钥解密证书，在证书中获得服务端的公钥， 客户端利用服务器端的公钥认证证书中的信息，如果一致，则认可这个服务器 2\u003e 客户端发送自己的证书给服务器端，服务端接收到证书后，通过私钥解密证书， 在证书中获得客户端的公钥，并用该公钥认证证书信息，确认客户端是否合法 服务器端和客户端进行通信 服务器端和客户端协商好加密方案后，客户端会产生一个随机的秘钥并加密，然后发送到服务器端。 服务器端接收这个秘钥后，双方接下来通信的所有内容都通过该随机秘钥加密 注意: Kubernetes允许同时配置多种认证方式，只要其中任意一个方式认证通过即可 ","date":"2023-08-04","objectID":"/kubernetes/:41:0","tags":["devops","kubernetes"],"title":"Kubernetes使用手册 (资料来源于B站“冰糖没糖”的分享)","uri":"/kubernetes/"},{"categories":["笔记"],"content":"9.3 授权管理 授权发生在认证成功之后，通过认证就可以知道请求用户是谁， 然后Kubernetes会根据事先定义的授权策略来决定用户是否有权限访问，这个过程就称为授权。 每个发送到ApiServer的请求都带上了用户和资源的信息：比如发送请求的用户、请求的路径、请求的动作等，授权就是根据这些信息和授权策略进行比较，如果符合策略，则认为授权通过，否则会返回错误。 API Server目前支持以下几种授权策略： AlwaysDeny：表示拒绝所有请求，一般用于测试 AlwaysAllow：允许接收所有请求，相当于集群不需要授权流程（Kubernetes默认的策略） ABAC：基于属性的访问控制，表示使用用户配置的授权规则对用户请求进行匹配和控制 Webhook：通过调用外部REST服务对用户进行授权 Node：是一种专用模式，用于对kubelet发出的请求进行访问控制 RBAC：基于角色的访问控制（kubeadm安装方式下的默认选项） RBAC(Role-Based Access Control) 基于角色的访问控制，主要是在描述一件事情：给哪些对象授予了哪些权限 其中涉及到了下面几个概念： 对象：User、Groups、ServiceAccount 角色：代表着一组定义在资源上的可操作动作(权限)的集合 绑定：将定义好的角色跟用户绑定在一起 RBAC引入了4个顶级资源对象： Role、ClusterRole：角色，用于指定一组权限 RoleBinding、ClusterRoleBinding：角色绑定，用于将角色（权限）赋予给对象 Role、ClusterRole 一个角色就是一组权限的集合，这里的权限都是许可形式的（白名单）。 # Role只能对命名空间内的资源进行授权，需要指定nameapce kind: Role apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: namespace: dev name: authorization-role rules: - apiGroups: [\"\"] # 支持的API组列表,\"\" 空字符串，表示核心API群 resources: [\"pods\"] # 支持的资源对象列表 verbs: [\"get\", \"watch\", \"list\"] # 允许的对资源对象的操作方法列表 # ClusterRole可以对集群范围内资源、跨namespaces的范围资源、非资源类型进行授权 kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: authorization-clusterrole rules: - apiGroups: [\"\"] resources: [\"pods\"] verbs: [\"get\", \"watch\", \"list\"] 需要详细说明的是，rules中的参数： apiGroups: 支持的API组列表 \"\",\"apps\", \"autoscaling\", \"batch\" resources：支持的资源对象列表 \"services\", \"endpoints\", \"pods\",\"secrets\",\"configmaps\",\"crontabs\",\"deployments\",\"jobs\", \"nodes\",\"rolebindings\",\"clusterroles\",\"daemonsets\",\"replicasets\",\"statefulsets\", \"horizontalpodautoscalers\",\"replicationcontrollers\",\"cronjobs\" verbs：对资源对象的操作方法列表 \"get\", \"list\", \"watch\", \"create\", \"update\", \"patch\", \"delete\", \"exec\" RoleBinding、ClusterRoleBinding 角色绑定用来把一个角色绑定到一个目标对象上，绑定目标可以是User、Group或者ServiceAccount。 # RoleBinding可以将同一namespace中的subject绑定到某个Role下，则此subject即具有该Role定义的权限 kind: RoleBinding apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: authorization-role-binding namespace: dev subjects: - kind: User name: heima apiGroup: rbac.authorization.k8s.io roleRef: kind: Role name: authorization-role apiGroup: rbac.authorization.k8s.io # ClusterRoleBinding在整个集群级别和所有namespaces将特定的subject与ClusterRole绑定，授予权限 kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: authorization-clusterrole-binding subjects: - kind: User name: heima apiGroup: rbac.authorization.k8s.io roleRef: kind: ClusterRole name: authorization-clusterrole apiGroup: rbac.authorization.k8s.io RoleBinding引用ClusterRole进行授权 RoleBinding可以引用ClusterRole，对属于同一命名空间内ClusterRole定义的资源主体进行授权。 一种很常用的做法就是，集群管理员为集群范围预定义好一组角色（ClusterRole），然后在多个命名空间中重复使用这些ClusterRole。这样可以大幅提高授权管理工作效率，也使得各个命名空间下的基础性授权规则与使用体验保持一致。 # 虽然authorization-clusterrole是一个集群角色，但是因为使用了RoleBinding # 所以heima只能读取dev命名空间中的资源 kind: RoleBinding apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: authorization-role-binding-ns namespace: dev subjects: - kind: User name: heima apiGroup: rbac.authorization.k8s.io roleRef: kind: ClusterRole name: authorization-clusterrole apiGroup: rbac.authorization.k8s.io 实战：创建一个只能管理dev空间下Pods资源的账号 创建账号 # 1) 创建证书 [root@k8s-master01 pki]# cd /etc/kubernetes/pki/ [root@k8s-master01 pki]# (umask 077;openssl genrsa -out devman.key 2048) # 2) 用apiserver的证书去签署 # 2-1) 签名申请，申请的用户是devman,组是devgroup [root@k8s-master01 pki]# openssl req -new -key devman.key -out devman.csr -subj \"/CN=devman/O=devgroup\" # 2-2) 签署证书 [root@k8s-master01 pki]# openssl x509 -req -in devman.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out devman.crt -days 3650 # 3) 设置集群、用户、上下文信息 [root@k8s-master01 pki]# kubectl config set-cluster kubernetes --embed-certs=true --certificate-authority=/etc/kubernetes/pki/ca.crt --server=https://192.168.109.100:6443 [root@k8s-master01 pki]# kubectl config set-credentials devman --embed-certs=true --client-certificate=/etc/kubernetes/pki/devman.crt --client-key=/etc/kubernetes/pki/devman.key [root@k8s-master01 pki]# kubectl config set-context devman@kubernetes --cluster=kubernetes --user=devman # 切换账户到devman [root@k8s-master01 pki]# ","date":"2023-08-04","objectID":"/kubernetes/:42:0","tags":["devops","kubernetes"],"title":"Kubernetes使用手册 (资料来源于B站“冰糖没糖”的分享)","uri":"/kubernetes/"},{"categories":["笔记"],"content":"9.4 准入控制 通过了前面的认证和授权之后，还需要经过准入控制处理通过之后，apiserver才会处理这个请求。 准入控制是一个可配置的控制器列表，可以通过在Api-Server上通过命令行设置选择执行哪些准入控制器： --admission-control=NamespaceLifecycle,LimitRanger,ServiceAccount,PersistentVolumeLabel, DefaultStorageClass,ResourceQuota,DefaultTolerationSeconds 只有当所有的准入控制器都检查通过之后，apiserver才执行该请求，否则返回拒绝。 当前可配置的Admission Control准入控制如下： AlwaysAdmit：允许所有请求 AlwaysDeny：禁止所有请求，一般用于测试 AlwaysPullImages：在启动容器之前总去下载镜像 DenyExecOnPrivileged：它会拦截所有想在Privileged Container上执行命令的请求 ImagePolicyWebhook：这个插件将允许后端的一个Webhook程序来完成admission controller的功能。 Service Account：实现ServiceAccount实现了自动化 SecurityContextDeny：这个插件将使用SecurityContext的Pod中的定义全部失效 ResourceQuota：用于资源配额管理目的，观察所有请求，确保在namespace上的配额不会超标 LimitRanger：用于资源限制管理，作用于namespace上，确保对Pod进行资源限制 InitialResources：为未设置资源请求与限制的Pod，根据其镜像的历史资源的使用情况进行设置 NamespaceLifecycle：如果尝试在一个不存在的namespace中创建资源对象，则该创建请求将被拒绝。当删除一个namespace时，系统将会删除该namespace中所有对象。 DefaultStorageClass：为了实现共享存储的动态供应，为未指定StorageClass或PV的PVC尝试匹配默认的StorageClass，尽可能减少用户在申请PVC时所需了解的后端存储细节 DefaultTolerationSeconds：这个插件为那些没有设置forgiveness tolerations并具有notready:NoExecute和unreachable:NoExecute两种taints的Pod设置默认的“容忍”时间，为5min PodSecurityPolicy：这个插件用于在创建或修改Pod时决定是否根据Pod的security context和可用的PodSecurityPolicy对Pod的安全策略进行控制 10. DashBoard 之前在kubernetes中完成的所有操作都是通过命令行工具kubectl完成的。其实，为了提供更丰富的用户体验，kubernetes还开发了一个基于web的用户界面（Dashboard）。用户可以使用Dashboard部署容器化的应用，还可以监控应用的状态，执行故障排查以及管理kubernetes中各种资源。 ","date":"2023-08-04","objectID":"/kubernetes/:43:0","tags":["devops","kubernetes"],"title":"Kubernetes使用手册 (资料来源于B站“冰糖没糖”的分享)","uri":"/kubernetes/"},{"categories":["笔记"],"content":"10.1 部署Dashboard 下载yaml，并运行Dashboard # 下载yaml [root@k8s-master01 ~]# wget https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0/aio/deploy/recommended.yaml # 修改kubernetes-dashboard的Service类型 kind: Service apiVersion: v1 metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard namespace: kubernetes-dashboard spec: type: NodePort # 新增 ports: - port: 443 targetPort: 8443 nodePort: 30009 # 新增 selector: k8s-app: kubernetes-dashboard # 部署 [root@k8s-master01 ~]# kubectl create -f recommended.yaml # 查看namespace下的kubernetes-dashboard下的资源 [root@k8s-master01 ~]# kubectl get pod,svc -n kubernetes-dashboard NAME READY STATUS RESTARTS AGE pod/dashboard-metrics-scraper-c79c65bb7-zwfvw 1/1 Running 0 111s pod/kubernetes-dashboard-56484d4c5-z95z5 1/1 Running 0 111s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/dashboard-metrics-scraper ClusterIP 10.96.89.218 \u003cnone\u003e 8000/TCP 111s service/kubernetes-dashboard NodePort 10.104.178.171 \u003cnone\u003e 443:30009/TCP 111s 2）创建访问账户，获取token # 创建账号 [root@k8s-master01-1 ~]# kubectl create serviceaccount dashboard-admin -n kubernetes-dashboard # 授权 [root@k8s-master01-1 ~]# kubectl create clusterrolebinding dashboard-admin-rb --clusterrole=cluster-admin --serviceaccount=kubernetes-dashboard:dashboard-admin # 获取账号token [root@k8s-master01 ~]# kubectl get secrets -n kubernetes-dashboard | grep dashboard-admin dashboard-admin-token-xbqhh kubernetes.io/service-account-token 3 2m35s [root@k8s-master01 ~]# kubectl describe secrets dashboard-admin-token-xbqhh -n kubernetes-dashboard Name: dashboard-admin-token-xbqhh Namespace: kubernetes-dashboard Labels: \u003cnone\u003e Annotations: kubernetes.io/service-account.name: dashboard-admin kubernetes.io/service-account.uid: 95d84d80-be7a-4d10-a2e0-68f90222d039 Type: kubernetes.io/service-account-token Data ==== namespace: 20 bytes token: eyJhbGciOiJSUzI1NiIsImtpZCI6ImJrYkF4bW5XcDhWcmNGUGJtek5NODFuSXl1aWptMmU2M3o4LTY5a2FKS2cifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlcm5ldGVzLWRhc2hib2FyZCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJkYXNoYm9hcmQtYWRtaW4tdG9rZW4teGJxaGgiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoiZGFzaGJvYXJkLWFkbWluIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiOTVkODRkODAtYmU3YS00ZDEwLWEyZTAtNjhmOTAyMjJkMDM5Iiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50Omt1YmVybmV0ZXMtZGFzaGJvYXJkOmRhc2hib2FyZC1hZG1pbiJ9.NAl7e8ZfWWdDoPxkqzJzTB46sK9E8iuJYnUI9vnBaY3Jts7T1g1msjsBnbxzQSYgAG--cV0WYxjndzJY_UWCwaGPrQrt_GunxmOK9AUnzURqm55GR2RXIZtjsWVP2EBatsDgHRmuUbQvTFOvdJB4x3nXcYLN2opAaMqg3rnU2rr-A8zCrIuX_eca12wIp_QiuP3SF-tzpdLpsyRfegTJZl6YnSGyaVkC9id-cxZRb307qdCfXPfCHR_2rt5FVfxARgg_C0e3eFHaaYQO7CitxsnIoIXpOFNAR8aUrmopJyODQIPqBWUehb7FhlU1DCduHnIIXVC_UICZ-MKYewBDLw ca.crt: 1025 bytes # 创建并获取账号token（新版，上述旧版执行失效无法获取到token，kind安装的wsl2设置的用户名是 admin-user） kubectl create token admin-user -n kubernetes-dashboard 3）通过浏览器访问Dashboard的UI 在登录页面上输入上面的token 出现下面的页面代表成功 ","date":"2023-08-04","objectID":"/kubernetes/:44:0","tags":["devops","kubernetes"],"title":"Kubernetes使用手册 (资料来源于B站“冰糖没糖”的分享)","uri":"/kubernetes/"},{"categories":["笔记"],"content":"10.2 使用DashBoard 本章节以Deployment为例演示DashBoard的使用 查看 选择指定的命名空间 dev，然后点击 Deployments，查看dev空间下的所有deployment 扩缩容 在 Deployment上点击 规模，然后指定 目标副本数量，点击确定 编辑 在 Deployment上点击 编辑，然后修改 yaml文件，点击确定 查看Pod 点击 Pods, 查看pods列表 操作Pod 选中某个Pod，可以对其执行日志（logs）、进入执行（exec）、编辑、删除操作 Dashboard提供了kubectl的绝大部分功能，这里不再一一演示 ","date":"2023-08-04","objectID":"/kubernetes/:45:0","tags":["devops","kubernetes"],"title":"Kubernetes使用手册 (资料来源于B站“冰糖没糖”的分享)","uri":"/kubernetes/"},{"categories":["笔记"],"content":"WSL2 中遇到的问题 wsl2 使用用kind安装的kubernates环境：参考链接： https://kubernetes.io/blog/2020/05/21/wsl-docker-kubernetes-on-the-windows-desktop/ kubernates 部署无法在浏览器中打开的问题 : kubectl port-forward service/gateway -n bookstore-microservices 30080:8080 istio部署无法在浏览器中打开的问题 : https://stackoverflow.com/questions/67869462/istio-ingress-gateway-on-wsl2-docker-desktop kubectl port-forward service/istio-ingressgateway -n istio-system :80 ","date":"2023-08-04","objectID":"/kubernetes/:46:0","tags":["devops","kubernetes"],"title":"Kubernetes使用手册 (资料来源于B站“冰糖没糖”的分享)","uri":"/kubernetes/"},{"categories":["学习方法"],"content":" kubernetes官方网站 深入架构原理与落地实践-GitHub地址 深入架构原理与落地实践-在线阅读 谷歌云微服务demo 基于Kubernetes实现的微服务架构示例工程。《凤凰架构》的配套组件 k8s 这个东西真的内容太多了，没有啥系统性的资料，里面各种知识点真的太多了，最好的就是看官方文档，并且结合工作当中的实践慢慢积累，才能由浅入深，只是看文档想掌握深点，个人感觉很困难。 如果不是专做这行，只是把它当作应用部署的底层平台的话可以按以下的简单的流程做 1 、云服务商买一套，或者自己搭建个 k8s 环境 2 、运行起来一套简单的前后端分离服务，这里主要练习的是多个镜像启动多个不同的 workload ，然后怎么能互相访问对接，这个环节能掌握清楚 workload service ingress 都是干嘛的，该怎么用，怎么关联 3 、你会发现当你测试环境发生重启，或者 pod 重建后，数据库数据都没了，这会你就应该研究数据持久化了，pv pvc 的概念就出来了 5 、然后你又创建了个前端服务，想修改个前端页面的配置文件参数，比如网页的 title ，其他都一模一样，但是每个服务一个镜像，太麻烦了，容器里直接修改，重建就没了，配置文件放 pv pvc ，太小题大做，这会 configmap 出现了。连接数据库的配置文件，密钥明文，太 low 了，secret 出现了 6 、前端页面镜像有 bug ，必须要重打镜像了，cicd 出现了，你是选择 docker build 还是 jenkins ，新的知识又增加了 7 、更新 workload 的镜像，问题又来了，服务会不会受损，多副本就不会受损吗？如何优雅终止，健康检查，无损更新? 8 、服务高峰期怎么应对，手动扩副本数太傻，hpa 来了 如果想深点，做些 k8s 运维或者技术支持的，那么除了上面的必须熟悉，下面的东西必知必会 1 、清楚 k8s 的工作逻辑，master 的三大件是干嘛的，kubelet ，kube-proxy ，coredns 都是干嘛的，比如执行个创建或查询一个 workload ，系统组件之间怎么通讯的，创建一个 pod 后，容器网络和外界是怎么打通的，k8s 资源调度和分配逻辑是什么 2 、自己搭建一套 k8s 集群，多 master 的最好，master 和 worker 节点分开（一定是自己搭建，不要购买云服务商现成的容器服务，自己搭建过程会收获不少东西） 3 、熟练使用 kubectl 命令进行各种查询分析 4 、清楚 rbac 的功能和使用 等你发现 k8s 了解差不多了，发现总得有个监控吧，prometheus 出现了 流量治理，服务分析也得有 ==\u003e istio 日志持久化保存 ==》els 来了 漂亮帅气的监控 ==》得研究 grafana 了 服务都上 k8s 了，集群越来越重要，etcd 崩了怎么办：etcd 原理和备份恢复得研究下 想自建镜像仓库了 ==\u003e harbor 研究下。 这个云平台太贵了，业务想换个云平台，集群要迁移，velero 走起 想给 pod 限速了，想给集群加审计了 ","date":"2023-08-01","objectID":"/how-to-learn-kubernets/:0:0","tags":["devops","kubernetes"],"title":"如何学习kubernetes","uri":"/how-to-learn-kubernets/"},{"categories":["笔记"],"content":"背景 java默认是通过/proc/meminfo来取服务器内存数据的，而docker容器中/proc/meminfo文件记录的是宿主机的内存参数，jvm堆最大值默认是服务器的1/4，这样就可能会导致docker容器的限制内存最大值可能小于jvm堆内存上限。 现象就是在jvm触发fullGC或者majorGC之前堆内存就已经快达到容器最大内存限制，导致容器将java进程杀死。 oracle在JDK 1.8u131开始对Docker容器进行支持，以解决上述问题，并且在JDK1.8u191版本进行了完善，因此升级JDK版本到1.8u191之后的版本即可解决问题(备注：不仅仅是内存，CPU同样如此，比如：Java应用容器化后GC的问题以及解决方案） 参考资料：oracle官网 Java Improvements for Docker Containers 章节，可能需要梯子 解决方案 ","date":"2023-06-23","objectID":"/set-jvm-in-docker/:0:0","tags":["docker","jvm"],"title":"Docker容器中设置jvm参数","uri":"/set-jvm-in-docker/"},{"categories":["笔记"],"content":"1、设置-Xmx最大内存 缺点就是每次更改容器最大限制内存都要更改一下对应-Xmx参数的值，显然这种是不合适的。 ","date":"2023-06-23","objectID":"/set-jvm-in-docker/:1:0","tags":["docker","jvm"],"title":"Docker容器中设置jvm参数","uri":"/set-jvm-in-docker/"},{"categories":["笔记"],"content":"2、通过设置堆占内存比例的方式设置堆的最大内存值 Docker是通过cgroup来实现最大内存设置的，添加识别cgroup配置的jvm参数即可读取到docker容器的内存最大值 JDK 8u131+、JDK 9版本可以设置如下参数指定堆最大内存如下所示，其中MaxRamFraction默认值是4，堆内存的利用率比较低，可以适当调高该占比 -XX:+UnlockExperimentalVMOptions #（开启实验性参数） -XX:+UseCGroupMemoryLimitForHeap # （指定JVM从主机读取cgroup限制） -XX:MaxRAMFraction=int #（服务器内存/jvm最大内存,默认是4,只能设置为整数，一般都是2） 不使用MaxRamFraction设置最大内存占比，而是将容器最大内存限制设置到环境变量中，在容器内通过环境变量读取到容器最大内存限制后通过-Xmx来设置堆最大内存为读取到的环境变量的80%，这样的好处是可以更灵活的配置堆的内存参数 JDK 8u191+、JDK 10及以上版本可以使用如下参数来动态指定jvm堆内存最大值（推荐） -XX:+UseContainerSupport #（使用容器内存） -XX:MaxRAMPercentage =75.0 #（使用容器内存百分比，表示jvm占容器内存75%） ","date":"2023-06-23","objectID":"/set-jvm-in-docker/:2:0","tags":["docker","jvm"],"title":"Docker容器中设置jvm参数","uri":"/set-jvm-in-docker/"},{"categories":["技术"],"content":"为什么不是充血模型？ 现有的业务系统领域模型未拆分。 系统调用错综复杂，排查问题难度较大。 代码结构错综复杂，排查问题难度较大。 充血模型对开发人员技术要求较高。 因此充血模型不符合现在的实际情况。 ","date":"2022-06-13","objectID":"/java-seven-level-architecture/:0:1","tags":["java","架构"],"title":"7层项目结构和maven创建项目模板","uri":"/java-seven-level-architecture/"},{"categories":["技术"],"content":"为什么是贫血模型？ 1 对领域模型的依赖不大，上手较快。 2 系统调用，通过module隔离开排查问题，方便 3 代码结构层级清晰，排查问题方便。 4 通过modular的依赖避免了一些技术上的陷阱，如多表操作是不是失效等问题， 5 支持脚手架一键生成项目骨架六完善的技术组件支持。 6 完善的技术组件支持 因此统一使用贫血模型 ","date":"2022-06-13","objectID":"/java-seven-level-architecture/:0:2","tags":["java","架构"],"title":"7层项目结构和maven创建项目模板","uri":"/java-seven-level-architecture/"},{"categories":["技术"],"content":"项目分层名词解释 API层： 提供标准的接口、常量和枚举。提供同一对外的接口，可以单独达成价保各外部系统使用。 Service层： API层接口的实现层，DTO转BO Manage层： 外部系统的衔接层，获取外部接口的返回结果。1、对接DAO：一个manage对应一个DAO，包含对一个表的各种操作，2 对接Integratio：一个manage对应一个integration，获取外部接口的结果。如果把DB，HTTP， rpc点都看成一个接口，management就是为了这些外部系统的出入参做准备的，类似于对外的一个service层。 DAO层: 数据库交互层，与redis，DB等数据库进行交互。DO对象。 Integration层: 外部系统集成层。HTTP，RPC等外部调用从这里开始，获取外围系统的数据。 WEB层： 对外暴露Http服务层 TASK层: 暴露定时任务 Common层： 封装常量、枚举Service，Mange、Integration层调用 各个Module内部必须按照业务功能进行划分。如出库、入库、调拨等分成不同的package。 ","date":"2022-06-13","objectID":"/java-seven-level-architecture/:0:3","tags":["java","架构"],"title":"7层项目结构和maven创建项目模板","uri":"/java-seven-level-architecture/"},{"categories":["技术"],"content":"IDEA对象转化插件 Codemarker easyCode GenerateO2O ","date":"2022-06-13","objectID":"/java-seven-level-architecture/:0:4","tags":["java","架构"],"title":"7层项目结构和maven创建项目模板","uri":"/java-seven-level-architecture/"},{"categories":["技术"],"content":"如何通过mvn archetype 命令创建模板 1 打包项目 ，在已准备好的项目模板最外层执行打包命令： mvn clean install 2 从项目中创建archetype ：在已准备好的项目模板最外层执行命令： mvn archetype:create-from-project 3 进入target\\generated-sources\\archetype,安装archetype到maven仓库中:mvn clean install 或者 mvn clean deploy 4 生成archetype-catalog.xml:进入target\\generated-sources\\archetype,执行 mvn archetype:crawl 5 查看archetype-catalog.xml ","date":"2022-06-13","objectID":"/java-seven-level-architecture/:0:5","tags":["java","架构"],"title":"7层项目结构和maven创建项目模板","uri":"/java-seven-level-architecture/"},{"categories":["技术"],"content":"通过maven的acrchetype创建项目 1 通过idea创建：new project选择maven，选择 create from archetype …… 2 输入命令“mvn archetype:generate” 创建项目骨架,根据提示一步步往下…… ","date":"2022-06-13","objectID":"/java-seven-level-architecture/:0:6","tags":["java","架构"],"title":"7层项目结构和maven创建项目模板","uri":"/java-seven-level-architecture/"},{"categories":["技术"],"content":"问题发现 【问题背景】 业务中心建设-库存中心压测时，发版失败，失败原因：数据库连接数超过MySQL最大连接数。 【问题排查】 1、由于在压测环境做库存中心压测，所以第一反应时觉得压测的TPS过高造成的数据库连接不够。故找DBA将库存中心的数据库最大连接数从2048调整到3072。再次发版，数据库连接数依旧被打满。 2、当时Spring配置的连接池大小：最大最小连接数都是300，尝试将数据库连接数降低到50，再次发版，数据库连接数飙升到6000 3、重新调整springboot数据源配置，将最小连接降低到1，最终在压测环境发版成功。 ","date":"2022-03-04","objectID":"/unit-test-start-multi-spring-container-problem/:0:1","tags":["spring","unit-test"],"title":"单元测试重复启动spring容器问题填坑","uri":"/unit-test-start-multi-spring-container-problem/"},{"categories":["技术"],"content":"问题处理 【干掉@MockBean】 @MockBean的作用：创建一个虚拟的对象替代那些不易构造会不易获取的对象。在实际开发中，我们自己的Controller，Service很可能去调用其他同事的接口或数据库，对方可能只写了一个接口，还没来得及写实现，这样时没办法进行联调测试的。此时可以通过@MockBean注入一个虚拟的Bean对象用于完成本地的单元测试。 @MockBean 实例 @MockBean public RocketMqTempalte rocketMqTempalte; @MockBean public UserIntegration userIntegration; //... 以上写法会造成SpringBoot启动多次，每次启动都会连接数据源，即有N个单元测试就启动了N个spring容器，就开启了300*N个数据库连接，因此导致数据库连接资源耗尽，无法发版。 优化单元测试 通过一个公共的单元测试配置类如TestConfiguration管理需要Mock的Bean，使用Mockito.mock()方法生成Mock对象，在需要使用Mock对象的地方通过Spring依赖注入的方式注入即可。 @Configuration public class TestConfiguratin @Bean private InventoryOperationDetailDao inventoryOperationDetailDao(){ return Mockito.mock(InventoryOperationDetailDao.class); } @Bean private InventoryOperationDetailManage inventoryOperationDetailManage (){ return Mockito.mock(InventoryOperationDetailManage.class); } // ... ","date":"2022-03-04","objectID":"/unit-test-start-multi-spring-container-problem/:0:2","tags":["spring","unit-test"],"title":"单元测试重复启动spring容器问题填坑","uri":"/unit-test-start-multi-spring-container-problem/"},{"categories":["技术"],"content":"问题分析 [参考文档] https://docs.spring.io/spring-framework/reference/testing/testcontext-framework/ctx-management/caching.html Context Caching Once the TestContext framework loads an ApplicationContext (or WebApplicationContext) for a test, that context is cached and reused for all subsequent tests that declare the same unique context configuration within the same test suite. To understand how caching works, it is important to understand what is meant by “unique” and “test suite.” An ApplicationContext can be uniquely identified by the combination of configuration parameters that is used to load it. Consequently, the unique combination of configuration parameters is used to generate a key under which the context is cached. The TestContext framework uses the following configuration parameters to build the context cache key: locations (from @ContextConfiguration) classes (from @ContextConfiguration) contextInitializerClasses (from @ContextConfiguration) contextCustomizers (from ContextCustomizerFactory) – this includes @DynamicPropertySource methods as well as various features from Spring Boot’s testing support such as @MockBean and @SpyBean. contextLoader (from @ContextConfiguration) parent (from @ContextHierarchy) activeProfiles (from @ActiveProfiles) propertySourceLocations (from @TestPropertySource) propertySourceProperties (from @TestPropertySource) resourceBasePath (from @WebAppConfiguration) For example, if TestClassA specifies {\"app-config.xml\", \"test-config.xml\"} for the locations (or value) attribute of @ContextConfiguration, the TestContext framework loads the corresponding ApplicationContext and stores it in a static context cache under a key that is based solely on those locations. So, if TestClassB also defines {\"app-config.xml\", \"test-config.xml\"} for its locations (either explicitly or implicitly through inheritance) but does not define @WebAppConfiguration, a different ContextLoader, different active profiles, different context initializers, different test property sources, or a different parent context, then the same ApplicationContext is shared by both test classes. This means that the setup cost for loading an application context is incurred only once (per test suite), and subsequent test execution is much faster. 根据文档的描述，不难知道 application context是通过key：value方式进行缓存的，\u003c/br\u003e唯一键为组合键，包含：locations、classes、contextInitiallizerClasses、contextCustomizers、contextLoader、parent、activeProfiles、propertySourceLocations、propertySourceProperties、resourceBasePath。 而@MockBean的使用会导致每个application context中的contextCustomizer的不同，从而导致存储在context cache中的application context 的unique key 不同，\u003c/br\u003e最终导致application context 在测试类之间不能共享。虽然没有官方文档说明这一点，不过在org.springframework.boot.test.mock.mockito.MockitoContextCustomizerFactory源码中可以找到一些痕迹。 MockitoContextCustomizerFactory源码 // ... class MockitoContextCustomizerFactory implements ContextCustomizerFactory{ @overried public MockitoContextCustomizerFactory createContextCustomizer(Class\u003c?\u003e testClass,List\u003cContextConfigurationAttributes\u003e configAttributes){ // we gather the explicit mock definitions here since they from part of the // MergedContextConfiguration key. Different mocks need to hava a fifferent key . !!! 看这里 ！！！ DefinitionsParser parser = new DefinitionsParser(); parser.parse(testClass); return new MockitoContextCustomizer(parser.getDefinitions()); } } 上面代码中注释所说的MergedContextConfiguration 就是 application context caching 的unique key。 ","date":"2022-03-04","objectID":"/unit-test-start-multi-spring-container-problem/:0:3","tags":["spring","unit-test"],"title":"单元测试重复启动spring容器问题填坑","uri":"/unit-test-start-multi-spring-container-problem/"},{"categories":["笔记"],"content":"一、背景 1、库存与串码再领域模型上分离。库存属于逻辑概念，串码与实物一一对应，属于实体概念。 2、串码数据量较大，需要做分库/分表，优化串码查询性能 3、在线串码表需要定期备份，将历史数据迁移至历史库，优化串码查询性能。 注： 所有非在线交易类的串码问题参考：串码详情ES大宽表方案 ","date":"2021-10-22","objectID":"/inventory-core-sharding-design/:0:1","tags":["架构","分库分表"],"title":"库存中心分库分表设计","uri":"/inventory-core-sharding-design/"},{"categories":["笔记"],"content":"二、数据分片方案 串码中心数据分片方案的出发点： 【问题1】解决大量串码信息存储问题 【问题2】解决数据分片后在线交易串码精准查找问题（海外VRS出库场景，指定imei1货imei2）。 【问题3】解决数据分片后在线交易串码模糊查找问题（如v-work扫码出库场景，给定任意一个数字（可能是imei1,imei2,itemCode,boxNo等任意一种属性）查找对应的串码信息）. 2.1 方案一 – 只分表不分库 【优点】 所有数据均在一个库中，不存在跨库的分布式事务，不存在多数据源处理。 代码逻辑较为简单，只需要维护分表的逻辑即可。 【缺点】 串码库发生抖动，所有代理的所有串码相关业务都受到影响。 单库的性能瓶颈，数据量增大后无法水平扩展数据库的性能。 不能解决问题2和问题3 —- imei1，imei2，meid，boxNo，itemCode通过同一个接口扫码出库实现不了，调拨场景也无法准确定位到这批串码（imei1……）所在的分库。 2.2 方案2 – 按代理分库 【优点】 每一个代理的数据在一个库中，串码库发生抖动不会影响其他的代理。 分多个库的性能比方案一中单库的性能高很多。 【缺点】 代理间的调拨，涉及多个数据源的分布式事务问题。 不能解决问题2和问题3 —- imei1，imei2，meid，boxNo，itemCode通过同一个接口扫码出库实现不了，调拨场景也无法准确定位到这批串码（imei1……）所在的分表。 2.3 方案3 – 按UUID分库且分表 2.3.1 【前提】 MES 一代串码同步到v-work库存中心时，有库存中心为每条串码生成全局唯一的UUID（和平台组沟通，可以较高的效率生成全局唯一的UUID） 现有的业务数据规则： 1. IMEI1、IMEI2固定是15位数字（8位tac+6位流水+1位校验码）例如：“865224038614541” 1. MEID固定是14位字符（8位头+6位16进制的流水）例如：“A00000754A0AEB” 1. BOX_NO手机：18位字符；IOT ：BOX_NO 长度不固定 2.3.2 【串码映射表】 串码信息表 (1) –\u003e 串码映射表（N） –\u003e 串码详情表 串码映射表, uuid value 备注 123456780123456 865224038614541 imei1 123456780123456 145416830422568 imei2 123456780123456 A00000754A0AEB meid 123456780123456 210616830422562 boxNo 123456780123456 264224038614542 itemCode 库存串码表 uuid first_agent_code ime1 ime2 meid boxNo itemCode arehouse_code account_id sku_code 123123123 5454545454 6363636363 73737373 838383 321123 93939393 654321 100010001 456789 【优点】 分多个库的性能比方案一中单库的性能高很多，可以解决大量串码存储问题 可以解决问题2和问题3 【缺点】 串码映射表冗余了很多数据，存储空间增加 串码映射表大量的数据存储将会带来性能瓶颈。（1亿串码*5个映射维度 = 5亿的数据量） 2.3.3 【串码映射表、库存串码表分库分表】 在2.3.1【前提】中，可以发现业务长度的数据是有一些规律的，可以利用业务数据的长度特性针对2.3.2【串码映射表】做分库分表处理。 根据业务数据的长度，将串码映射表拆分为3个库（每个库按uuid分为512张表）： 1）长度为15的imei1和imei2与uuid的映射库 2）长度14的meid与uuid的映射库 3）长度不为14且不为15的boxNo，itemCode与uuid的映射库 2.3.4 【扫码出库场景分析】 @startuml 交付中心 -\u003e 库存中心: 1、调用库存中心扫码出库接口 库存中心 -\u003e 库存中心: 2、库存中心扣减库存 库存中心 -\u003e 串码中心: 3、库存中心\\n携带串码调用串码中心 串码中心 -\u003e 串码中心映射表: 4、根据串码长度找到对应的分库 \\n 假设分别扫码 \\n 1 imei1=100 2 boxNo=200 串码中心映射表 -\u003e 串码中心映射表: 5.1根据imei1长度\\n找到对应的分库serialCode_15 \\n 根据100%512得到这条数据所在分表，\\n 并得到100对饮的UUID 串码中心映射表 -\u003e 库存串码映射表: 5.2根据uuid%5找到对应的分库\\n 根据uud%64得到这条数据所在分表，\\n 并得到100对饮的UUID 串码中心映射表 -\u003e 串码中心映射表: 6.1根据boxNo长度找到对应的分库 \\n serialCode_not_15_14 \\n 根据200%512得到这条数据所在分表，\\n 并得到200对饮的UUID 串码中心映射表 -\u003e 库存串码映射表: 6.2根据uuid%5找到对应的分库\\n 根据uud%64得到这条数据所在分表，\\n 并得到200对饮的UUID 库存串码映射表 -\u003e 库存串码映射表: 5.3 在iemi1=100的分表中\\n 进行对串码行的出库操作 库存串码映射表 -\u003e 库存串码映射表: 6.3 在boxNo=200的分表中\\n 进行对串码行的出库操作 交付中心 -\u003e 库存中心: 1、调用库存你中心扫码出库接口 @enduml 以上步骤5和步骤6涉及跨串码中心2个库的分布式事务，spring支持跨多数据源事务 2.3.5 【串码数据规模预估】 【串码映射表】 （1） 串码映射表容量预估： 在2.3.3【串码映射表、库存串码表分库分表】中串码映射表的分库分表规则： 3个DB * 512张表/DB * 500~1000万数据/表（为确保MySql性能最优） = 76.8亿~153.6亿，即串码映射表最多可容纳153.6亿条串码与UUID的映射数据 （2） 业务所需映射表容量预估： 按现有手机业务规模（1亿销售/年），同比预估iot未来业务规模（一亿销量/年）： 【1亿手机+1亿iot】*5 （imei1，imei2,meid,boxNo,itemCode会对应5条映射记录） =10 亿，即未来业务规模对应的串码映射表数量为10亿 【结论1】2.3.3【串码映射表、库存串码表分库分表】中将串码映射表分为3个库，每个库分512张表，完全可以满足未来业务需求 因串码映射表是很少的2列（UUID和业务码值），因此每条串码映射的数据量都很小。 假设UUID为64位（最大位数），业务码值也为64位，即一条串码映射的数据量为：128位的字符串，即128字节（java字符串中，英文，数字，符号各占1字节），即0.1KB 为了应对未来的变化，假设串码映射表单条数据增加到0.5kb 串码映射表容量最大：76.8亿~153.6亿 * 0.5KB = 38.4 ~76.8亿KB=3.8T~7.6T 业务所需映射表容量：10亿*0.5KB = 500GB 假设一个Redis/ES节点内存容量 = 100GB，即支撑手机1亿/年，iot1亿/年，需要10个Redis/ES组成的cluster集群（每个Redis/Es节点一主一从，单节点就要5个） 【结论2】为了提升串码映射表的性能，支撑手机1亿/年，iot 1亿/年的销量，可以将串码映射表数据存储在redis中，缓存时间为1年。如果业务数据持续增大，可以缩小缓存时间范围（如缓存半年，超过半年的查DB） 【库存串码表】 （1）容量预估 在2.3.3【串码映射表、库存串码表分库分表】中串码库存串码表的分库分表规则： 5个DB * 64张表/DB * 500~1000万数据/表（为确保MySql性能最优） = 16亿~32亿，即库存串码表最多可容纳32亿条串码与UUID的映射数据 【结论3】库存串码表最多容纳32亿条数据，以手机1亿/年，iot1亿/年的销量欸参考值，预计库存串码表可以容纳16年内的实时交易数据（根据业务需求，历史库存串码信息数据可以清理到历史库） ","date":"2021-10-22","objectID":"/inventory-core-sharding-design/:0:2","tags":["架构","分库分表"],"title":"库存中心分库分表设计","uri":"/inventory-core-sharding-design/"},{"categories":["笔记"],"content":"三、总结 1）串码映射表MySql: 最大业务容量-2亿/年（手机，IOT各1亿）.最大容量 – 在MySql分为3个库，每个库512张表，预计可以容纳76.8亿~153.6亿条串码记录，预计可以容纳30年内的串码映射记录 2）串码映射表缓存：优化SQL查询路径，将串码映射表做缓存，最大业务容量（手机1亿/年，iot1亿/年）需要500GB内存，约10个Redis/ES集群，含主备 3）串码映射表缓存容量增长：【方案1】增加Redis/ES 集群中的节点数量。【方案2】增加新的K-V型中间件（如淘宝tiar），纯内存与磁盘顺序写并存。 4）库存串码表：最大容量可以容纳32亿条串码数据库，按最大业务容纳（手机1年/年，IOT1年/年）计算，预计可以容纳16年内的实时交易数据。 ","date":"2021-10-22","objectID":"/inventory-core-sharding-design/:0:3","tags":["架构","分库分表"],"title":"库存中心分库分表设计","uri":"/inventory-core-sharding-design/"},{"categories":[],"content":" 深入架构原理与落地实践-GitHub地址 深入架构原理与落地实践-在线阅读 凤凰架构：构建可靠的大型分布式系统 凤凰架构对应github仓库 ","date":"2021-03-03","objectID":"/books/:0:0","tags":["book"],"title":"架构类书籍资料","uri":"/books/"}]